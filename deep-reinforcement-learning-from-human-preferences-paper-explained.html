<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>&quot;Deep Reinforcement Learning From Human Preferences&quot; Paper Explained - The NLP Student&#x27;s Blog</title><meta name="description" content="This paper is the work of a collaboration with Deep Mind and Open AI, improving the field of Deep Reinforcement Learning. The ideas discussed in this paper are also a key component in the training of GPT-4. RL agents need good reward functions to learn complex tasks. However, it&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://thenlpstudent.github.io/deep-reinforcement-learning-from-human-preferences-paper-explained.html"><link rel="alternate" type="application/atom+xml" href="https://thenlpstudent.github.io/feed.xml"><link rel="alternate" type="application/json" href="https://thenlpstudent.github.io/feed.json"><meta property="og:title" content="'Deep Reinforcement Learning From Human Preferences' Paper Explained"><meta property="og:image" content="https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400.jpg"><meta property="og:image:width" content="314"><meta property="og:image:height" content="314"><meta property="og:site_name" content="The NLP Student's Blog"><meta property="og:description" content="This paper is the work of a collaboration with Deep Mind and Open AI, improving the field of Deep Reinforcement Learning. The ideas discussed in this paper are also a key component in the training of GPT-4. RL agents need good reward functions to learn complex tasks. However, it&hellip;"><meta property="og:url" content="https://thenlpstudent.github.io/deep-reinforcement-learning-from-human-preferences-paper-explained.html"><meta property="og:type" content="article"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@studentnlp"><meta name="twitter:title" content="'Deep Reinforcement Learning From Human Preferences' Paper Explained"><meta name="twitter:description" content="This paper is the work of a collaboration with Deep Mind and Open AI, improving the field of Deep Reinforcement Learning. The ideas discussed in this paper are also a key component in the training of GPT-4. RL agents need good reward functions to learn complex tasks. However, it&hellip;"><meta name="twitter:image" content="https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400.jpg"><link rel="shortcut icon" href="https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400-2.jpg" type="image/png"><style>:root{--body-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--heading-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--logo-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--menu-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol"}</style><link rel="stylesheet" href="https://thenlpstudent.github.io/assets/css/style.css?v=8ac49514f3b5a54ab40b9772cb61e8d3"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://thenlpstudent.github.io/deep-reinforcement-learning-from-human-preferences-paper-explained.html"},"headline":"\"Deep Reinforcement Learning From Human Preferences\" Paper Explained","datePublished":"2023-04-09T07:07","dateModified":"2023-04-12T22:12","image":{"@type":"ImageObject","url":"https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400.jpg","height":314,"width":314},"description":"This paper is the work of a collaboration with Deep Mind and Open AI, improving the field of Deep Reinforcement Learning. The ideas discussed in this paper are also a key component in the training of GPT-4. RL agents need good reward functions to learn complex tasks. However, it&hellip;","author":{"@type":"Person","name":"The NLP Student","url":"https://thenlpstudent.github.io/authors/chirath-nissanka/"},"publisher":{"@type":"Organization","name":"The NLP Student","logo":{"@type":"ImageObject","url":"https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400.jpg","height":314,"width":314}}}</script><script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script></head><body><div class="site-container"><header class="top" id="js-header"><a class="logo" href="https://thenlpstudent.github.io/"><img src="https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400.jpg" alt="The NLP Student&#x27;s Blog" width="314" height="314"></a></header><main><article class="post"><div class="hero"><header class="hero__content"><div class="wrapper"><div class="post__meta"><time datetime="2023-04-09T07:07">April 9, 2023</time></div><h1>&quot;Deep Reinforcement Learning From Human Preferences&quot; Paper Explained</h1><div class="post__meta post__meta--author"><a href="https://thenlpstudent.github.io/authors/chirath-nissanka/" class="feed__author invert">The NLP Student</a></div></div></header></div><div class="wrapper post__entry"><p><span data-preserver-spaces="true"><a href="https://arxiv.org/abs/1706.03741">This paper</a> is the work of a collaboration with </span><strong><span data-preserver-spaces="true">Deep Mind </span></strong><span data-preserver-spaces="true">and </span><strong><span data-preserver-spaces="true">Open AI</span></strong><span data-preserver-spaces="true">, improving the field of</span><strong><span data-preserver-spaces="true"> Deep Reinforcement Learning. </span></strong></p><p><strong><span data-preserver-spaces="true">The ideas discussed in this paper are also a key component in the training of GPT-4</span></strong><span data-preserver-spaces="true">. </span></p><p><span data-preserver-spaces="true">RL agents need <strong>good reward functions</strong> to<strong> learn complex tasks</strong>. However, it takes work to design reward functions from scratch. </span></p><p>Learning a reward function using a neural network is also proposed. Another idea is why not let a human reward the agent altogether.</p><p><span data-preserver-spaces="true">This paper attempts to <strong>combine both of those two ideas</strong>. </span></p><p><strong><span data-preserver-spaces="true">The method discussed in this paper is a crucial component that Open AI used in training GPT-4</span></strong><span data-preserver-spaces="true">. </span></p><div class="post__toc"><h3>Table of Contents</h3><ul><li><a href="#mcetoc_1gthqshlp2h">Introduction </a></li><li><a href="#mcetoc_1gthqshlp2i">Related Work </a></li><li><a href="#mcetoc_1gthqshlp2j">The methodology </a><ul><li><a href="#mcetoc_1gtr5h2ssi9">A breakdown of the process </a></li><li><a href="#mcetoc_1gtr5h2ssia">The Reward Function Estimator</a></li><li><a href="#mcetoc_1gtr5h2ssib">Selecting queries and asking for human feedback</a></li><li><a href="#mcetoc_1gtr5h2ssic">How feedback is processed </a></li><li><a href="#mcetoc_1gtr5h2ssid">Fitting the Reward Function </a></li></ul></li><li><a href="#mcetoc_1gthqshlp2k">Summary of results </a></li></ul></div><h2 id="mcetoc_1gthqshlp2f"></h2><h2 id="mcetoc_1gthqshlp2h">Introduction </h2><p>Reinforcement learning is a learning paradigm in which an <strong>agent observes the environment, takes an action, and receives a reward. </strong></p><p>This process happens continuously in a loop. <br><br>The agent's objective is to maximize the reward it accumulates by taking proper actions at the appropriate timestep t. </p><figure class="r48jcc pT0Scc iPVvYb"><img loading="lazy" src="https://miro.medium.com/v2/resize:fit:1400/1*7cuAqjQ97x1H_sBIeAVVZg.png" alt="Reinforcement Learning 101. Learn the essentials of Reinforcement… | by  Shweta Bhatt | Towards Data Science" data-is-external-image="true"></figure><p>Notice that in traditional reinforcement learning, the environment gives the agent its reward.  <br><br>That means we need to design a reward function by looking at the environment at that timestep and see what amount of reward should be given to the agent. <br><br>Designing a simple reward function that approximately captures the intended behavior is problematic in complex tasks. In most cases, this leads to the agent optimizing for the reward function without satisfying the intended preferences. </p><p>Instead of designing a reward function, we can use a human to give out a reward by observing the environment. However, this is very expensive and time-consuming. <br><br>Therefore we need to devise a way that uses human feedback very sparingly. </p><p>The approach mentioned in the paper is to <strong>learn a reward function from human feedback and then optimize that reward function</strong>. This reward function is responsible for giving out rewards to the agent. <strong>The agent then optimizes its behavior to the learned reward function. </strong></p><p>The method proposed in the paper is beneficial in tasks where humans can recognize the desired behavior, but one that you can only demonstrate to the agent what the desired behavior is. <br><br>For example, consider an agent doing a black flip. You know when the agent does a backflip correctly and when an agent doesn't.  <br>But it would be easier to do a backflip if you are an expert at doing a backflip. </p><figure class="r48jcc pT0Scc iPVvYb"><img loading="lazy" src="https://s.yimg.com/ny/api/res/1.2/VoXpdGw1yV8YCS9OiBBxSQ--/YXBwaWQ9aGlnaGxhbmRlcjt3PTY0MDtoPTM2MA--/https://o.aolcdn.com/hss/storage/midas/f2aa87f4f1dc29d0919055d960d5f6cf/205870720/ezgif.com-optimize+%284%29.gif" alt="Watch Boston Dynamics' Atlas robot nail a backflip | Engadget" data-is-external-image="true"></figure><br><br>This method allows agents to be trained by non-experts. <br>This method also scales to significant complex problems and claims to be economical with human feedback. <p></p><p>The algorithm proposed in the paper fits a reward function to given human feedback while simultaneously training a policy to optimize the currently predicted reward function. </p><figure class="post__image"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/9/sss.PNG" alt="" width="672" height="291" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/9/responsive/sss-xs.PNG 300w, https://thenlpstudent.github.io/media/posts/9/responsive/sss-sm.PNG 480w, https://thenlpstudent.github.io/media/posts/9/responsive/sss-md.PNG 768w, https://thenlpstudent.github.io/media/posts/9/responsive/sss-lg.PNG 1024w, https://thenlpstudent.github.io/media/posts/9/responsive/sss-xl.PNG 1360w, https://thenlpstudent.github.io/media/posts/9/responsive/sss-2xl.PNG 1600w"></figure><br><br>The human feedback is taken from a subject (volunteer, nonexpert), where they compare short video clips of the agent's behavior. <p></p><p>The subject picks one of the video clips with the correct behavior, or if both video clips are equally bad or equally good, they can choose "cannot be decided." <br><br>Since human feedback is taken in real-time, and the reward function changes in real-time, the <strong>agent cannot exploit the reward function </strong>since it constantly modifies (volatile). <strong>This method leads to the agent working in line with human preferences. </strong></p><h2 id="mcetoc_1gthqshlp2i">Related Work </h2><p><span data-preserver-spaces="true">The paper follows the same basic approach as previous works, </span><strong><span data-preserver-spaces="true">"Active preference learning-based reinforcement learning" </span></strong><span data-preserver-spaces="true">and</span><strong><span data-preserver-spaces="true"> "Programming by Feedback" by Akrour et al. </span></strong></p><p><span data-preserver-spaces="true">A difference from previous works is that this paper uses short clips instead of whole trajectories when getting feedback from the human subject. </span></p><p><span data-preserver-spaces="true">This paper has two orders of magnitude more comparisons but requires considerably less human time. </span></p><p><span data-preserver-spaces="true">The method proposed in this paper collects and processes human feedback, following closely with previous work, </span><strong><span data-preserver-spaces="true">"A Bayesian approach for policy learning from trajectory preference queries" by Wilson et al.</span></strong><span data-preserver-spaces="true"> </span></p><p><span data-preserver-spaces="true">However, Wilson's work uses "synthetic" human feedback drawn from a Bayesian model, whereas this paper uses nonexpert users. </span></p><p><span data-preserver-spaces="true">"</span><strong><span data-preserver-spaces="true">Interactive learning from policy-dependent human feedback</span></strong><span data-preserver-spaces="true">" by MalGlashan et al., </span><span data-preserver-spaces="true">"</span><strong><span data-preserver-spaces="true">Online human training of a myoelectric prosthesis controller via actor-critic reinforcement learning</span></strong><span data-preserver-spaces="true">" by Pilarski et al., </span></p><p><span data-preserver-spaces="true">"</span><strong><span data-preserver-spaces="true">Interactively shaping agents via human reinforcement: The TAMER framework</span></strong><span data-preserver-spaces="true">" by Knox and Stone and </span><strong><span data-preserver-spaces="true">"Learning non-myopically from human-generated reward"</span></strong><span data-preserver-spaces="true"> by Knox are previous works focusing on human feedback-based reinforcement learning.</span></p><p><span data-preserver-spaces="true">The above works have learning only occurring during episodes where the human trainer provides feedback (MalGlashan et al. and Pilarski et al.). However, this approach is not feasible in domains where an RL agent needs thousands of hours of trial and error to converge to an optimal policy. </span></p><p><span data-preserver-spaces="true">The works of Knox and Stone also learn a reward function. However, they consider much simpler settings where the desired policy can be quickly learned. </span></p><p><span data-preserver-spaces="true">The essential </span><strong><span data-preserver-spaces="true">contribution of this paper is to scale human feedback up to deep reinforcement learning and to learn much more complex behaviors.</span></strong><span data-preserver-spaces="true"> </span></p><p><span data-preserver-spaces="true">This paper fits into the recent trend of works that were focused on </span><strong><span data-preserver-spaces="true">scaling reward learning methods to large deep learning systems</span></strong><span data-preserver-spaces="true">, e.g. </span><span data-preserver-spaces="true">Inverse RL (Deep inverse optimal control via policy optimization by Finn et al.), Generative adversarial imitation learning by Ho et al., Generalizing skills with semi-supervised reinforcement learning by Finn et al.</span></p><h2 id="mcetoc_1gthqshlp2j">The methodology </h2><p>The paper tests their new method on the following RL training environments. </p><ol><li><strong>Arcade Learning <em>Environment</em> (ALE)</strong> - For testing RL agents using Atari Games </li><li><strong>MuJoCo</strong> - For testing robotic tasks via a physics simulator </li></ol><p>An RL agent interacts with the environment as follows, at each timestep (\ t \), the agent receives an observation \( o_t \in O \) from the environment and then sends an action \( a_t \in A \) to the environment. <br><br>Traditionally, the environment would provide reward signals, but a human overseer can express preferences between trajectory segments in this method rather than the environment. </p><p>A trajectory segment is a sequence of observations and actions.</p><p>\[ \sigma = ((o_0, a_0), (o_1, a_1), ... ,(o_{k-1}, a_{k-1})) \in (O \times A)^k \]</p><p>A pair of these trajectory segments are presented to a human at any given time. The following is how we denote that the trajectory \( \sigma^1 \) is prefered over trajectory \( \sigma^2 \) by the nonexpert user. </p><p>\[ \sigma^1 \succ \sigma^2 \]</p><p>In informal terms, the goal of the agent in the long run is to produce trajectories which are preferred by the human, while making as few queries as possible to the human, so that less time is utilized from the prespective of the human subject (nonexpert user). </p><h3 id="mcetoc_1gtr5h2ssi9">A breakdown of the process </h3><p>At each timestep, the method proposed in the paper maintains a policy and a reward function estimate. </p><p>The policy is a function that maps observations(from the state space O) to an action (in the action space A). </p><p>The reward function estimate maps observations and actions to a real value (scalar). <br><br>The policy and the reward function estimator are networks that learn over time from training and experience (via updating their weights). <br><br>The policy interacts with the environment to produce a set of trajectories (sequences of state and action pairs), and traditional reinforcement learning algorithms update the policy's parameters to maximize the reward the agent gets over time. <br><br>Specific pairs of segments sampled from the trajectories are taken and sent to a human for comparison and feedback. <br><br>The parameters of the reward function estimator are updated via supervised learning to fit with the comparisons made by the non-expert human user. <br><br>All these actions mentioned above run asynchronously from one to the other without halting the overall learning process of the agent. <br><br>The agent or the reward function estimator doesn't halt until the human subject gives feedback, and the systems run asynchronously in the background.  </p><h3 id="mcetoc_1gtr5h2ssia">The Reward Function Estimator</h3><p>The following diagram below shows their reward function estimator architecture. <br><br></p><figure class="post__image align-center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/9/Group-14.png" alt="" width="961" height="360" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/9/responsive/Group-14-xs.png 300w, https://thenlpstudent.github.io/media/posts/9/responsive/Group-14-sm.png 480w, https://thenlpstudent.github.io/media/posts/9/responsive/Group-14-md.png 768w, https://thenlpstudent.github.io/media/posts/9/responsive/Group-14-lg.png 1024w, https://thenlpstudent.github.io/media/posts/9/responsive/Group-14-xl.png 1360w, https://thenlpstudent.github.io/media/posts/9/responsive/Group-14-2xl.png 1600w"></figure><p>It's a convolutional neural network with multiple convolutional layers, with the input as the state, and the output is a scalar value, which is also normalized via a sigmoid function to be a value between -1 and 1. <br><br>This reward is then given to the RL agent for any action it chooses to do. <br><br>The human non-expert trains the reward function estimator via feedback so that for each different state, suitable rewards are given that fit in line with the human's preferences. </p><h3 id="mcetoc_1gtr5h2ssib">Selecting queries and asking for human feedback</h3><p>One of the main goals of this paper is to minimize human time and ask for as little feedback as possible when training an RL agent to do a complex task. <br><br>The reward function estimator cannot query the human every time.  <br>Ideally, it should query the human subject when unsure what reward to give to a particular state and action pair.<br><br>In their method, they use several reward function estimators in an ensemble (of size 3) and consider all of the networks' predictions in the ensemble, when trying to get feedback from the non-expert human subject. </p><p>The diagram below shows a high-level picture of how the overall process works. </p><p>Many pairs of trajectory segments sampled from the buffer are run through the reward function estimators in the ensemble. Each network tries to predict which pair of trajectories are more likely to align with human preferences by scoring them. (A high reward predicted means the network "thinks" that the given trajectory is better, and vice versa) </p><figure class="post__image"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/9/Group-15.png" alt="" width="2524" height="2341" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/9/responsive/Group-15-xs.png 300w, https://thenlpstudent.github.io/media/posts/9/responsive/Group-15-sm.png 480w, https://thenlpstudent.github.io/media/posts/9/responsive/Group-15-md.png 768w, https://thenlpstudent.github.io/media/posts/9/responsive/Group-15-lg.png 1024w, https://thenlpstudent.github.io/media/posts/9/responsive/Group-15-xl.png 1360w, https://thenlpstudent.github.io/media/posts/9/responsive/Group-15-2xl.png 1600w"></figure><p>Sometimes the networks in the ensemble cannot commonly agree on which two trajectories might be better than the other, meaning those two trajectories would have high variance, the highest pair, which contains the most amount of variance, is sent to the human subject for feedback. <br><br>The two pairs of trajectories selected forms a query, and the feedback from the human helps the reward estimator produce better rewards in the future aligned with human preferences. </p><h3 id="mcetoc_1gtr5h2ssic">How feedback is processed </h3><p>The human subject is shown two short video clips of the pair of trajectories, lasting for about 1 or 2 seconds. <br><br>The subject can then select whether the first clip is the best, if the second clip is the best, or if they can't decide. This response is recorded and used to construct training data for the reward estimator function to train on.</p><p>This forms a tuple  \( \left(\sigma^1, \sigma^2, \mu \right) \) where, \( \sigma^1 \) is the first trajectory, and \( \sigma^2 \) is the second trajectory, and \( \mu \) is the human preference distribution (feedback). <br><br>The below diagram illustrates how \( \mu \) is created. </p><figure class="post__image"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/9/Group-3.png" alt="" width="460" height="478" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/9/responsive/Group-3-xs.png 300w, https://thenlpstudent.github.io/media/posts/9/responsive/Group-3-sm.png 480w, https://thenlpstudent.github.io/media/posts/9/responsive/Group-3-md.png 768w, https://thenlpstudent.github.io/media/posts/9/responsive/Group-3-lg.png 1024w, https://thenlpstudent.github.io/media/posts/9/responsive/Group-3-xl.png 1360w, https://thenlpstudent.github.io/media/posts/9/responsive/Group-3-2xl.png 1600w"></figure><h3 id="mcetoc_1gtr5h2ssid">Fitting the Reward Function </h3><p>The probability that the reward estimator function prefering trajectory \( \sigma^1 \) over \( \sigma^2 \) can be denoted by the following probability expression, </p><p>\[\hat{P}[\sigma^1 \succ \sigma^2] = \frac{\exp \sum \hat{r} \left( o^1_t , a^1_t \right) }{\exp \sum \hat{r} \left( o^1_t , a^1_t \right) + \exp \sum \hat{r} \left( o^2_t , a^2_t \right)}\]<br><br>And the probability that the reward estimator function prefering trajectory \( \sigma^2\) over \( \sigma^1 \) can be denoted by the following probability expression, </p><p>\[\hat{P}[\sigma^2 \succ \sigma^1] = \frac{\exp \sum \hat{r} \left( o^2_t , a^2_t \right) }{\exp \sum \hat{r} \left( o^1_t , a^1_t \right) + \exp \sum \hat{r} \left( o^2_t , a^2_t \right)}\]</p><p><strong>Note that \( \hat{r} \) is the reward estimator function, and \( o_t \) is state, and \( a_t \) is the action taken by the agent, at timestep \( t \). </strong></p><p>We write the loss of the function, as a cross-entropy loss, between the difference in the trajectory preferred by the reward estimator vs. the trajectory preferred by the human subject.</p><p>Note that, \( \mu(1) \) denotes the probability that the human subject prefers trajectory \( \sigma^1 \) and \( \mu(2) \) denotes the probability that the human subject prefers trajectory \( \sigma^2 \) </p><p style="overflow: auto;">\[\text{loss}\left( \hat{r} \right) = - \sum_{\left( \sigma^1, \sigma^2, \mu \right) \in D } \mu\left(1\right)log \hat{P} \left[ \sigma^1 \succ \sigma^2 \right] + \mu\left(2\right)log \hat{P} \left[ \sigma^2 \succ \sigma^1 \right]\]</p><p style="overflow: auto;">\[\text{loss}\left( \hat{r} \right) = - \sum_{\left( \sigma^1, \sigma^2, \mu \right) \in D } \mu\left(1\right)log  \frac{\exp \sum \hat{r} \left( o^1_t , a^1_t \right) }{\exp \sum \hat{r} \left( o^1_t , a^1_t \right) + \exp \sum \hat{r} \left( o^2_t , a^2_t \right)} + \mu\left(2\right)log \frac{\exp \sum \hat{r} \left( o^2_t , a^2_t \right) }{\exp \sum \hat{r} \left( o^1_t , a^1_t \right) + \exp \sum \hat{r} \left( o^2_t , a^2_t \right)}\]</p><h2 id="mcetoc_1gthqshlp2k">Summary of results </h2><h2 id="mcetoc_1gthqshlp2l"></h2><p><span data-preserver-spaces="true">Novel behaviors were observed in the Atari as well as the physics environment, </span></p><ol><li><span data-preserver-spaces="true"><span data-preserver-spaces="true">The Hopper robot performs a sequence of backflips. This behavior was trained using 900 queries in less than an hour. The agent learns to perform a backflip, land upright, and repeat consistently. </span></span></li><li><span data-preserver-spaces="true">The Half-Cheetah robot moves forward while standing on one leg. This behavior was trained using 800 queries in under an hour.</span></li><li><span data-preserver-spaces="true">Keeping alongside other cars in Enduro. This was trained with roughly 1,300 queries and 4 million frames of interaction with the environment; the agent learns to stay almost exactly even with other moving cars for a substantial fraction of the episode, although it gets confused by changes in the background.</span></li></ol><p>The videos of these novel behaviors can be founded here: <a href="https://drive.google.com/drive/folders/0BwcFziBYuA8RM2NTdllSNVNTWTg?resourcekey=0-w4PuSuFvi3odgQXdBDPQ0g">Deep RL with Human Preferences - Google Drive</a></p><p>Thanks for reading! </p><p> </p></div><footer class="wrapper post__footer"><p class="post__last-updated">This article was updated on April 12, 2023</p><ul class="post__tag"><li><a href="https://thenlpstudent.github.io/tags/gpt/">GPT</a></li><li><a href="https://thenlpstudent.github.io/tags/reinforcement-learning/">Reinforcement learning</a></li><li><a href="https://thenlpstudent.github.io/tags/reward-functions/">Reward functions</a></li></ul><div class="post__share"></div><div class="post__bio bio"><div class="bio__info"><h3 class="bio__name"><a href="https://thenlpstudent.github.io/authors/chirath-nissanka/" class="invert" rel="author">The NLP Student</a></h3></div></div></footer></article><nav class="post__nav"><div class="post__nav-inner"><div class="post__nav-prev"><svg width="1.041em" height="0.416em" aria-hidden="true"><use xlink:href="https://thenlpstudent.github.io/assets/svg/svg-map.svg#arrow-prev"/></svg> <a href="https://thenlpstudent.github.io/looking-at-entropy.html" class="invert post__nav-link" rel="prev"><span>Previous</span> What is Information Entropy?</a></div><div class="post__nav-next"><a href="https://thenlpstudent.github.io/summary-of-the-generalization-bounds-via-convex-analysis-paper.html" class="invert post__nav-link" rel="next"><span>Next</span> Summary of the &quot;Generalization Bounds via Convex Analysis&quot; Paper [Draft] </a><svg width="1.041em" height="0.416em" aria-hidden="true"><use xlink:href="https://thenlpstudent.github.io/assets/svg/svg-map.svg#arrow-next"/></svg></div></div></nav></main><footer class="footer"><div class="footer__copyright"><p>Powered by <a href="https://getpublii.com" target="_blank" rel="nofollow noopener">Publii</a></p></div><button class="footer__bttop js-footer__bttop" aria-label="Back to top"><svg><title>Back to top</title><use xlink:href="https://thenlpstudent.github.io/assets/svg/svg-map.svg#toparrow"/></svg></button></footer></div><script>window.publiiThemeMenuConfig = {    
        mobileMenuMode: 'sidebar',
        animationSpeed: 300,
        submenuWidth: 'auto',
        doubleClickTime: 500,
        mobileMenuExpandableSubmenus: true, 
        relatedContainerForOverlayMenuSelector: '.top',
   };</script><script defer="defer" src="https://thenlpstudent.github.io/assets/js/scripts.min.js?v=f4c4d35432d0e17d212f2fae4e0f8247"></script><script>var images = document.querySelectorAll('img[loading]');

        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>