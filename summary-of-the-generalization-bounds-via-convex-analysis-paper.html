<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Summary of the &quot;Generalization Bounds via Convex Analysis&quot; Paper - The NLP Student&#x27;s Blog</title><meta name="description" content="The 'Generalization Bounds via Convex Analysis' paper by Gergely Neu, and Gábor Lugosi, discusses the generalization error of supervised learning algorithms and how it can be bounded regarding the mutual information between their input and output. The authors generalize this result beyond the standard choice&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://thenlpstudent.github.io/summary-of-the-generalization-bounds-via-convex-analysis-paper.html"><link rel="alternate" type="application/atom+xml" href="https://thenlpstudent.github.io/feed.xml"><link rel="alternate" type="application/json" href="https://thenlpstudent.github.io/feed.json"><meta property="og:title" content="Summary of the 'Generalization Bounds via Convex Analysis' Paper"><meta property="og:image" content="https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400.jpg"><meta property="og:image:width" content="314"><meta property="og:image:height" content="314"><meta property="og:site_name" content="The NLP Student's Blog"><meta property="og:description" content="The 'Generalization Bounds via Convex Analysis' paper by Gergely Neu, and Gábor Lugosi, discusses the generalization error of supervised learning algorithms and how it can be bounded regarding the mutual information between their input and output. The authors generalize this result beyond the standard choice&hellip;"><meta property="og:url" content="https://thenlpstudent.github.io/summary-of-the-generalization-bounds-via-convex-analysis-paper.html"><meta property="og:type" content="article"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@studentnlp"><meta name="twitter:title" content="Summary of the 'Generalization Bounds via Convex Analysis' Paper"><meta name="twitter:description" content="The 'Generalization Bounds via Convex Analysis' paper by Gergely Neu, and Gábor Lugosi, discusses the generalization error of supervised learning algorithms and how it can be bounded regarding the mutual information between their input and output. The authors generalize this result beyond the standard choice&hellip;"><meta name="twitter:image" content="https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400.jpg"><link rel="shortcut icon" href="https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400-2.jpg" type="image/png"><style>:root{--body-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--heading-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--logo-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--menu-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol"}</style><link rel="stylesheet" href="https://thenlpstudent.github.io/assets/css/style.css?v=8ac49514f3b5a54ab40b9772cb61e8d3"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://thenlpstudent.github.io/summary-of-the-generalization-bounds-via-convex-analysis-paper.html"},"headline":"Summary of the \"Generalization Bounds via Convex Analysis\" Paper","datePublished":"2023-05-26T20:22","dateModified":"2023-05-26T20:22","image":{"@type":"ImageObject","url":"https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400.jpg","height":314,"width":314},"description":"The 'Generalization Bounds via Convex Analysis' paper by Gergely Neu, and Gábor Lugosi, discusses the generalization error of supervised learning algorithms and how it can be bounded regarding the mutual information between their input and output. The authors generalize this result beyond the standard choice&hellip;","author":{"@type":"Person","name":"The NLP Student","url":"https://thenlpstudent.github.io/authors/chirath-nissanka/"},"publisher":{"@type":"Organization","name":"The NLP Student","logo":{"@type":"ImageObject","url":"https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400.jpg","height":314,"width":314}}}</script><script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script></head><body><div class="site-container"><header class="top" id="js-header"><a class="logo" href="https://thenlpstudent.github.io/"><img src="https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400.jpg" alt="The NLP Student&#x27;s Blog" width="314" height="314"></a></header><main><article class="post"><div class="hero"><header class="hero__content"><div class="wrapper"><div class="post__meta"><time datetime="2023-05-26T20:22">May 26, 2023</time></div><h1>Summary of the &quot;Generalization Bounds via Convex Analysis&quot; Paper</h1><div class="post__meta post__meta--author"><a href="https://thenlpstudent.github.io/authors/chirath-nissanka/" class="feed__author invert">The NLP Student</a></div></div></header></div><div class="wrapper post__entry"><p><span data-preserver-spaces="true">The <strong>"Generalization Bounds via Convex Analysis" paper by Gergely Neu, and Gábor Lugosi</strong>, discusses the generalization error of supervised learning algorithms and how it can be bounded regarding the mutual information between their input and output.</span></p><p><span data-preserver-spaces="true">The authors generalize this result beyond the standard choice of Shannon's mutual information to measure the dependence between the input and output.</span></p><p><strong><span data-preserver-spaces="true">Shannon's mutual information</span></strong><span data-preserver-spaces="true"> is a measure of the amount of information that is shared by two random variables.</span></p><p><span data-preserver-spaces="true">The </span><strong><span data-preserver-spaces="true">mutual information between two random variables, X and Y, is the difference between X's entropy and X's conditional entropy given Y. </span></strong></p><p><span data-preserver-spaces="true">In previous papers, the generalization error of supervised learning algorithms was bounded by </span><strong><span data-preserver-spaces="true">Shannon's mutual information measure</span></strong><span data-preserver-spaces="true">. </span></p><p><span data-preserver-spaces="true">However, this paper shows that it's possible to replace the mutual information by any </span><strong><span data-preserver-spaces="true">strongly convex function</span></strong><span data-preserver-spaces="true"> of the joint input-output distribution with the </span><strong><span data-preserver-spaces="true">subgaussianity condition</span></strong><span data-preserver-spaces="true"> on the losses replaced by a bound on an appropriately chosen </span><strong><span data-preserver-spaces="true">norm</span></strong><span data-preserver-spaces="true"> capturing the geometry of the dependence measure.</span></p><p><span data-preserver-spaces="true">This fundamental proof allows us to derive a range of generalization bounds that are either entirely new or strengthen previously known ones. </span></p><p><span data-preserver-spaces="true">Examples include bounds in terms of</span><strong><span data-preserver-spaces="true"> p-norm divergences</span></strong><span data-preserver-spaces="true"> and the </span><strong><span data-preserver-spaces="true">Wasserstein-2 distance</span></strong><span data-preserver-spaces="true">, which are applicable for heavy-tailed loss distributions and highly smooth loss functions.</span></p><h2>Definitions</h2><p><span data-preserver-spaces="true">A function is considered</span><strong><span data-preserver-spaces="true"> strongly convex</span></strong><span data-preserver-spaces="true"> if it is convex and its second derivative is greater than or equal to some positive constant. In other words, a function is strongly convex if it is always "curving up" at a rate that is at least as fast as some positive constant. This property makes strongly convex functions useful in optimization problems because they have a </span><strong><span data-preserver-spaces="true">unique minimum that can be found efficiently. </span></strong></p><p><span data-preserver-spaces="true">A Subgaussian random variable is one whose tail probabilities decay exponentially. A subgaussianity condition is a condition that requires the loss of any fixed hypothesis to have a subgaussian tail.</span></p><p><span data-preserver-spaces="true">A norm is a function that assigns a non-negative real number to a vector space and behaves like the distance from the origin. </span></p><p><span data-preserver-spaces="true">A divergence is a function that takes two probability distributions as input and returns a number that measures how much they differ. The number returned must be non-negative and equal to zero if and only if the two distributions are identical. </span></p><h2>Problem</h2><p>\[gen(W_n, S_n) = L(W_n, S_n) - \mathbf{E} [\ell(W_n, \tilde{Z}) | W_n] \]</p><p>\(gen(W_n, S_n) \) : The generalization error of the learning algorithm. </p><p>\( L(W_n, S_n)\) : The training loss</p><p>\( \mathbf{E} [\ell(W_n, \tilde{Z}) | W_n] \) : The expected loss from testing </p><h2>Proof</h2><p> </p></div><footer class="wrapper post__footer"><p class="post__last-updated">This article was updated on May 26, 2023</p><div class="post__share"></div><div class="post__bio bio"><div class="bio__info"><h3 class="bio__name"><a href="https://thenlpstudent.github.io/authors/chirath-nissanka/" class="invert" rel="author">The NLP Student</a></h3></div></div></footer></article><nav class="post__nav"><div class="post__nav-inner"><div class="post__nav-prev"><svg width="1.041em" height="0.416em" aria-hidden="true"><use xlink:href="https://thenlpstudent.github.io/assets/svg/svg-map.svg#arrow-prev"/></svg> <a href="https://thenlpstudent.github.io/deep-reinforcement-learning-from-human-preferences-paper-explained.html" class="invert post__nav-link" rel="prev"><span>Previous</span> &quot;Deep Reinforcement Learning From Human Preferences&quot; Paper Explained</a></div></div></nav><div class="post__related related"><div class="wrapper"><h2 class="h5 related__title">You should also read:</h2><article class="related__item"><div class="feed__meta"><time datetime="2023-04-09T07:07" class="feed__date">April 9, 2023</time></div><h3 class="h1"><a href="https://thenlpstudent.github.io/deep-reinforcement-learning-from-human-preferences-paper-explained.html" class="invert">&quot;Deep Reinforcement Learning From Human Preferences&quot; Paper Explained</a></h3></article></div></div></main><footer class="footer"><div class="footer__copyright"><p>Powered by <a href="https://getpublii.com" target="_blank" rel="nofollow noopener">Publii</a></p></div><button class="footer__bttop js-footer__bttop" aria-label="Back to top"><svg><title>Back to top</title><use xlink:href="https://thenlpstudent.github.io/assets/svg/svg-map.svg#toparrow"/></svg></button></footer></div><script>window.publiiThemeMenuConfig = {    
        mobileMenuMode: 'sidebar',
        animationSpeed: 300,
        submenuWidth: 'auto',
        doubleClickTime: 500,
        mobileMenuExpandableSubmenus: true, 
        relatedContainerForOverlayMenuSelector: '.top',
   };</script><script defer="defer" src="https://thenlpstudent.github.io/assets/js/scripts.min.js?v=f4c4d35432d0e17d212f2fae4e0f8247"></script><script>var images = document.querySelectorAll('img[loading]');

        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>