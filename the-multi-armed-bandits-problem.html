<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>The Multi-Armed Bandit Problem  - The NLP Student&#x27;s Blog</title><meta name="description" content="In solving the problem of exploration vs. exploitation in reinforcement learning, we use bandit problems to understand and apply algorithms that balance RL agents' exploration and exploitation behaviors. What exactly is the multi-armed bandit problem, and how can you solve this type of problem in&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://thenlpstudent.github.io/the-multi-armed-bandits-problem.html"><link rel="alternate" type="application/atom+xml" href="https://thenlpstudent.github.io/feed.xml"><link rel="alternate" type="application/json" href="https://thenlpstudent.github.io/feed.json"><meta property="og:title" content="The Multi-Armed Bandit Problem "><meta property="og:image" content="https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400.jpg"><meta property="og:image:width" content="314"><meta property="og:image:height" content="314"><meta property="og:site_name" content="The NLP Student's Blog"><meta property="og:description" content="In solving the problem of exploration vs. exploitation in reinforcement learning, we use bandit problems to understand and apply algorithms that balance RL agents' exploration and exploitation behaviors. What exactly is the multi-armed bandit problem, and how can you solve this type of problem in&hellip;"><meta property="og:url" content="https://thenlpstudent.github.io/the-multi-armed-bandits-problem.html"><meta property="og:type" content="article"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@studentnlp"><meta name="twitter:title" content="The Multi-Armed Bandit Problem "><meta name="twitter:description" content="In solving the problem of exploration vs. exploitation in reinforcement learning, we use bandit problems to understand and apply algorithms that balance RL agents' exploration and exploitation behaviors. What exactly is the multi-armed bandit problem, and how can you solve this type of problem in&hellip;"><meta name="twitter:image" content="https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400.jpg"><link rel="shortcut icon" href="https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400-2.jpg" type="image/png"><style>:root{--body-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--heading-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--logo-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--menu-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol"}</style><link rel="stylesheet" href="https://thenlpstudent.github.io/assets/css/style.css?v=8ac49514f3b5a54ab40b9772cb61e8d3"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://thenlpstudent.github.io/the-multi-armed-bandits-problem.html"},"headline":"The Multi-Armed Bandit Problem ","datePublished":"2021-10-23T20:04","dateModified":"2021-10-27T08:04","image":{"@type":"ImageObject","url":"https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400.jpg","height":314,"width":314},"description":"In solving the problem of exploration vs. exploitation in reinforcement learning, we use bandit problems to understand and apply algorithms that balance RL agents' exploration and exploitation behaviors. What exactly is the multi-armed bandit problem, and how can you solve this type of problem in&hellip;","author":{"@type":"Person","name":"The NLP Student","url":"https://thenlpstudent.github.io/authors/chirath-nissanka/"},"publisher":{"@type":"Organization","name":"The NLP Student","logo":{"@type":"ImageObject","url":"https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400.jpg","height":314,"width":314}}}</script><script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script></head><body><div class="site-container"><header class="top" id="js-header"><a class="logo" href="https://thenlpstudent.github.io/"><img src="https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400.jpg" alt="The NLP Student&#x27;s Blog" width="314" height="314"></a></header><main><article class="post"><div class="hero"><header class="hero__content"><div class="wrapper"><div class="post__meta"><time datetime="2021-10-23T20:04">October 23, 2021</time></div><h1>The Multi-Armed Bandit Problem </h1><div class="post__meta post__meta--author"><a href="https://thenlpstudent.github.io/authors/chirath-nissanka/" class="feed__author invert">The NLP Student</a></div></div></header></div><div class="wrapper post__entry"><p>In solving the problem of exploration vs. exploitation in reinforcement learning, we use bandit problems to understand and apply algorithms that balance RL agents' exploration and exploitation behaviors. <br><br>What exactly is the multi-armed bandit problem, and how can you solve this type of problem in RL? <br><br>This blog post focuses on doing just that. </p><div class="post__toc"><h3>Table of Contents</h3><ul><li><a href="#mcetoc_1fimqv2jdp4">What is the Multi Armed Bandit Problem? </a><ul><li><a href="#mcetoc_1fimqv2jdp5">Stationary vs Non Stationary Bandit Problem </a></li></ul></li><li><a href="#mcetoc_1fimqv2jdp6">Solve by taking estimates </a><ul><li><a href="#mcetoc_1fimqv2jdp7">Incremental Updates</a></li><li><a href="#mcetoc_1fimt3ol2qu">Solving non-stationary bandit problems </a></li></ul></li><li><a href="#mcetoc_1fivpg6e31e">Pseudocode, Code, and Results </a></li><li><a href="#mcetoc_1fimqv2jdp9">References </a></li></ul></div><h2 id="mcetoc_1fimqv2jdp4">What is the Multi Armed Bandit Problem? </h2><p>Consider the following situation that you find yourself in. <br>You are faced repeatedly with 'k' different actions to take each time. <br>After each action that you take at a given time, you receive a numerical reward.<br><br>This reward is picked out from a stationary probability distribution that depends on your selected action. <br><br>Your goal in this problem is to maximize the total reward you receive over a period of time by selecting the best possible action at each time so that you maximize your reward. <br><br>In this problem, each action has an average reward. This average reward is the value of the action, or how good that action is. <br><br>If you knew the value of each action, then solving this problem would be very simple. Just pick the action with the highest value to get the biggest reward possible at the end. <br><br>However, you don't know the value of each of these actions. Therefore, you would need to explore and come up with estimates on the value of each action by taking random actions in the first few timesteps. <br><br>Once you have good guesses after the first few timesteps, then you are presented with a choice. <br><br>Either you can exploit, which means pick the action with the largest value estimate or continue to explore and re-adjust your current estimates of the values of your actions. </p><h3 id="mcetoc_1fimqv2jdp5">Stationary vs Non Stationary Bandit Problem </h3><p>The rewards for the actions you or the reinforcement learning (RL) agent takes at the current timestep are sampled from a probability distribution. E.g., Gaussian Distribution. <br><br>In a stationary Bandit problem, the probability distribution doesn't change at each timestep, which means that the true value of each action is constant across all timesteps. However, in a non-stationary bandit problem, the probability distribution changes at every timestep. <br><br>Therefore the true value of each action at each timestep is quite different from other timesteps. <br><br>However, we can solve both variations of such bandit problems by estimating the value of each action, continuing to re-estimate them by trying different actions (exploration), or picking the highest calculated value action you currently have (exploitation) </p><h2 id="mcetoc_1fimqv2jdp6">Solve by taking estimates </h2><p>One way to solve the Multi-Armed Bandit problem is through estimating the true value of each action.</p><p>Solving by estimating the true value of each action can either be applied to stationary and non-stationary bandit problems. </p><p>Firstly we need a few mathematical notations to formalize the bandit problem before finding a way to solve it.</p><ul><li>\( A_t \)  The action the agent picks at time \(t\)</li><li>\( R_t \)    The reward the agent earns at time \(t\)</li><li>\( q_*(a) \)   The true value of action \(a\)</li><li>\( Q_t(a) \)   The estimated value of action \(a\) at timestep \(t\) </li></ul><p>After exploration, we would want our RL agent to find closer estimates to the true value of action 'a'. </p><p>\[ Q_t(a) \approx q_*(a) \]</p><p>To estimate the value of each action, we keep track of the total sum of rewards collected when we took a specific action 'a' and the number of times we took that action 'a'.</p><p>By dividing as follows, we get an average estimate of how good that action is at the current timestep. </p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/3/captur1.PNG" alt="" width="818" height="94" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/3/responsive/captur1-xs.PNG 300w, https://thenlpstudent.github.io/media/posts/3/responsive/captur1-sm.PNG 480w, https://thenlpstudent.github.io/media/posts/3/responsive/captur1-md.PNG 768w, https://thenlpstudent.github.io/media/posts/3/responsive/captur1-lg.PNG 1024w, https://thenlpstudent.github.io/media/posts/3/responsive/captur1-xl.PNG 1360w, https://thenlpstudent.github.io/media/posts/3/responsive/captur1-2xl.PNG 1600w"></figure><p>The below shows our definition for how the agent picks the suitable action at timestep t. <br><br>This equation is the greedy approach (exploitation) in which the agent selects the highest estimated action value as its next action. <br><br>The agent picks the action with the highest estimate, hoping that it would bring the highest possible reward for timestep 't'. </p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/3/captur2.PNG" alt="" width="258" height="67" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/3/responsive/captur2-xs.PNG 300w, https://thenlpstudent.github.io/media/posts/3/responsive/captur2-sm.PNG 480w, https://thenlpstudent.github.io/media/posts/3/responsive/captur2-md.PNG 768w, https://thenlpstudent.github.io/media/posts/3/responsive/captur2-lg.PNG 1024w, https://thenlpstudent.github.io/media/posts/3/responsive/captur2-xl.PNG 1360w, https://thenlpstudent.github.io/media/posts/3/responsive/captur2-2xl.PNG 1600w"></figure><h3 id="mcetoc_1fimqv2jdp7">Incremental Updates</h3><p>To keep things simple let us assume the following change in notation, <br>  </p><p>Since the above is computationally costly, we can incrementally compute the mean by re-arranging the mean calculation using a little bit of algebra. </p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/3/incremental-2.PNG" alt="" width="564" height="474" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/3/responsive/incremental-2-xs.PNG 300w, https://thenlpstudent.github.io/media/posts/3/responsive/incremental-2-sm.PNG 480w, https://thenlpstudent.github.io/media/posts/3/responsive/incremental-2-md.PNG 768w, https://thenlpstudent.github.io/media/posts/3/responsive/incremental-2-lg.PNG 1024w, https://thenlpstudent.github.io/media/posts/3/responsive/incremental-2-xl.PNG 1360w, https://thenlpstudent.github.io/media/posts/3/responsive/incremental-2-2xl.PNG 1600w"></figure><p>This mean update has the following form used throughout in reinforcement learning for problems other than bandits. <br><br>The new estimate of the value of an action is the old estimate moved in the direction of the error between the current reward seen (i.e., 'target') and the old estimate. </p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/3/updaterule.PNG" alt="" width="822" height="67" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/3/responsive/updaterule-xs.PNG 300w, https://thenlpstudent.github.io/media/posts/3/responsive/updaterule-sm.PNG 480w, https://thenlpstudent.github.io/media/posts/3/responsive/updaterule-md.PNG 768w, https://thenlpstudent.github.io/media/posts/3/responsive/updaterule-lg.PNG 1024w, https://thenlpstudent.github.io/media/posts/3/responsive/updaterule-xl.PNG 1360w, https://thenlpstudent.github.io/media/posts/3/responsive/updaterule-2xl.PNG 1600w"></figure><h3 id="mcetoc_1fimt3ol2qu">Solving non-stationary bandit problems </h3><p><span data-preserver-spaces="true">When the optimal value changes from timestep to timestep, then you have a non-stationary bandit problem.</span></p><p><span data-preserver-spaces="true">The probability distribution in which rewards are sampled from the agent for the action it picks comes from a different probability distribution randomly at every timestep.</span></p><p><span data-preserver-spaces="true">Therefore the true value of each action is different at each timestep. </span></p><p><span data-preserver-spaces="true">Therefore we cannot rely on average sampling.</span></p><p><span data-preserver-spaces="true">We need a mechanism in which we give more weight to rewards that we see recently instead of values we saw in the past and move the estimates of our action values towards recently seen reward signals. </span></p><p><span data-preserver-spaces="true">This is done using a fixed step size parameter \( \alpha \), a </span><strong><span data-preserver-spaces="true">constant</span></strong><span data-preserver-spaces="true"> </span><strong><span data-preserver-spaces="true">parameter</span></strong><span data-preserver-spaces="true"> that lies between</span><strong><span data-preserver-spaces="true"> 0 and 1. </span></strong></p><p>$$\begin{aligned} Q_{n+1} &amp;= Q_n + \alpha[R_n + Q_n] \\ &amp;= \alpha R_n + (1 - \alpha)Q_n \\ &amp;= \alpha R_n + (1 - \alpha)Q_n \\ &amp;= \alpha R_n + (1 - \alpha)[\alpha R_{n-1} + (1-\alpha)Q_{n-1}] \\ &amp;= \alpha R_n + (1 - \alpha)[\alpha R_{n-1} + (1-\alpha)Q_{n-1}] \\ &amp;= \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha)^2 \alpha R_{n-2} + .... + (1- \alpha)^nQ_1 \\ &amp;= (1- \alpha)^nQ_1 + \sum^{n}_{i=1}\alpha(1 - \alpha)^{n-i}R_i \end{aligned}$$</p><p>By having a constant step size, the action values are updated so that the estimates are moved in the direction of the recent reward, as they have a higher weight and the older reward signals have a lower influence in the direction of where the estimates would move. </p><p>This makes the RL agents get more good action value estimates in non-stationary bandit problems. </p><h2 id="mcetoc_1fivpg6e31e"><span data-preserver-spaces="true">Pseudocode, Code, and Results </span></h2><p>The following code snippet shows the pseudocode implementation of solving the bandit problem by estimating the value of each action. </p><figure class="post__image"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/3/carbon7-2.png" alt="" width="1806" height="1076" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/3/responsive/carbon7-2-xs.png 300w, https://thenlpstudent.github.io/media/posts/3/responsive/carbon7-2-sm.png 480w, https://thenlpstudent.github.io/media/posts/3/responsive/carbon7-2-md.png 768w, https://thenlpstudent.github.io/media/posts/3/responsive/carbon7-2-lg.png 1024w, https://thenlpstudent.github.io/media/posts/3/responsive/carbon7-2-xl.png 1360w, https://thenlpstudent.github.io/media/posts/3/responsive/carbon7-2-2xl.png 1600w"></figure><p>The following code snippet is the complete implementation of solving a ten-armed bandit problem. At each timestep, the agent can pick any ten actions and get a reward based on a gaussian distribution. <br><br>This distribution doesn't change over timesteps. Therefore, this is a stationary bandit problem. </p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/3/maincode1-2.png" alt="" width="2354" height="3068" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/3/responsive/maincode1-2-xs.png 300w, https://thenlpstudent.github.io/media/posts/3/responsive/maincode1-2-sm.png 480w, https://thenlpstudent.github.io/media/posts/3/responsive/maincode1-2-md.png 768w, https://thenlpstudent.github.io/media/posts/3/responsive/maincode1-2-lg.png 1024w, https://thenlpstudent.github.io/media/posts/3/responsive/maincode1-2-xl.png 1360w, https://thenlpstudent.github.io/media/posts/3/responsive/maincode1-2-2xl.png 1600w"></figure><p>The following code snippet is modified slightly to show how we can solve a non-stationary bandit problem.<br><br>We can solve the problem using a step-size constant instead of counting how many times action 'a' was chosen before timestep t. </p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/3/stepsize.png" alt="" width="2354" height="1278" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/3/responsive/stepsize-xs.png 300w, https://thenlpstudent.github.io/media/posts/3/responsive/stepsize-sm.png 480w, https://thenlpstudent.github.io/media/posts/3/responsive/stepsize-md.png 768w, https://thenlpstudent.github.io/media/posts/3/responsive/stepsize-lg.png 1024w, https://thenlpstudent.github.io/media/posts/3/responsive/stepsize-xl.png 1360w, https://thenlpstudent.github.io/media/posts/3/responsive/stepsize-2xl.png 1600w"></figure><p>The average reward at each timestep is stored in a list for each timestep.</p><p>For example, the following graph shows how the epsilon parameter influences the epsilon-greedy algorithm to obtain high amounts of rewards over 10,000 timesteps. </p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/3/f22.png" alt="" width="640" height="480" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/3/responsive/f22-xs.png 300w, https://thenlpstudent.github.io/media/posts/3/responsive/f22-sm.png 480w, https://thenlpstudent.github.io/media/posts/3/responsive/f22-md.png 768w, https://thenlpstudent.github.io/media/posts/3/responsive/f22-lg.png 1024w, https://thenlpstudent.github.io/media/posts/3/responsive/f22-xl.png 1360w, https://thenlpstudent.github.io/media/posts/3/responsive/f22-2xl.png 1600w"></figure><p>The graph below shows how average sampling methods with a variable step size perform compared to fixed step size sampling methods for non-stationary bandit problems. </p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/3/f2.png" alt="" width="640" height="480" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/3/responsive/f2-xs.png 300w, https://thenlpstudent.github.io/media/posts/3/responsive/f2-sm.png 480w, https://thenlpstudent.github.io/media/posts/3/responsive/f2-md.png 768w, https://thenlpstudent.github.io/media/posts/3/responsive/f2-lg.png 1024w, https://thenlpstudent.github.io/media/posts/3/responsive/f2-xl.png 1360w, https://thenlpstudent.github.io/media/posts/3/responsive/f2-2xl.png 1600w"></figure><p>Thanks for reading this article this far! </p><p>Follow me on Twitter <a href="https://twitter.com/intent/user?screen_name=studentnlp" target="_blank" rel="noopener noreferrer">@studentnlp</a> so you won't miss my latest blog post, and as always, happy hacking!</p><h2 id="mcetoc_1fimqv2jdp9">References </h2><p><span class="ILfuVd"><span class="hgKElc">Sutton, R.S. &amp; Barto, A.G., 2018. Reinforcement learning: An introduction, MIT press.</span></span></p></div><footer class="wrapper post__footer"><p class="post__last-updated">This article was updated on October 27, 2021</p><div class="post__share"></div><div class="post__bio bio"><div class="bio__info"><h3 class="bio__name"><a href="https://thenlpstudent.github.io/authors/chirath-nissanka/" class="invert" rel="author">The NLP Student</a></h3></div></div></footer></article><nav class="post__nav"><div class="post__nav-inner"><div class="post__nav-prev"><svg width="1.041em" height="0.416em" aria-hidden="true"><use xlink:href="https://thenlpstudent.github.io/assets/svg/svg-map.svg#arrow-prev"/></svg> <a href="https://thenlpstudent.github.io/introduction-to-reinforcement-learning.html" class="invert post__nav-link" rel="prev"><span>Previous</span> Basics of Reinforcement Learning: Part I</a></div><div class="post__nav-next"><a href="https://thenlpstudent.github.io/new-post-2.html" class="invert post__nav-link" rel="next"><span>Next</span> One Shot Learning, Few Shot Learning, and Similarity  </a><svg width="1.041em" height="0.416em" aria-hidden="true"><use xlink:href="https://thenlpstudent.github.io/assets/svg/svg-map.svg#arrow-next"/></svg></div></div></nav></main><footer class="footer"><div class="footer__copyright"><p>Powered by <a href="https://getpublii.com" target="_blank" rel="nofollow noopener">Publii</a></p></div><button class="footer__bttop js-footer__bttop" aria-label="Back to top"><svg><title>Back to top</title><use xlink:href="https://thenlpstudent.github.io/assets/svg/svg-map.svg#toparrow"/></svg></button></footer></div><script>window.publiiThemeMenuConfig = {    
        mobileMenuMode: 'sidebar',
        animationSpeed: 300,
        submenuWidth: 'auto',
        doubleClickTime: 500,
        mobileMenuExpandableSubmenus: true, 
        relatedContainerForOverlayMenuSelector: '.top',
   };</script><script defer="defer" src="https://thenlpstudent.github.io/assets/js/scripts.min.js?v=f4c4d35432d0e17d212f2fae4e0f8247"></script><script>var images = document.querySelectorAll('img[loading]');

        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>