<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>The NLP Student </title>
    <link href="https://thenlpstudent.github.io/feed.xml" rel="self" />
    <link href="https://thenlpstudent.github.io" />
    <updated>2022-01-26T21:43:12+05:30</updated>
    <author>
        <name>The NLP Student</name>
    </author>
    <id>https://thenlpstudent.github.io</id>

    <entry>
        <title>Basics of Reinforcement Learning</title>
        <author>
            <name>The NLP Student</name>
        </author>
        <link href="https://thenlpstudent.github.io/basics-of-reinforcement-learning-2.html"/>
        <id>https://thenlpstudent.github.io/basics-of-reinforcement-learning-2.html</id>

        <updated>2022-01-26T21:43:12+05:30</updated>
            <summary>
                <![CDATA[
                     Welcome! In this article, we will be going over the fundamentals of reinforcement learning. We will discuss what reinforcement learning is, how agents learn from rewards and experiences, what policies and value functions are, and how to model environments in a Markov decision process&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                
  <p>
    Welcome! In this article, we will be going over the fundamentals of reinforcement learning. We will discuss what reinforcement learning is, how agents learn from rewards and experiences, what policies and value functions are, and how to model environments in a Markov decision process step-by-step.
  </p>

  <div class="post__toc">
    <h3>Quick Links</h3>
    <ul>
      <li><a href="#what-is-reinforcement-learning-rl">What is Reinforcement Learning (RL)?</a><ul><li><a href="#what-is-a-rewardnbsp">What is a reward?&nbsp;</a></li><li><a href="#the-reward-hypothesisnbsp">The Reward Hypothesis&nbsp;</a></li></ul></li><li><a href="#components-in-reinforcement-learning">Components in Reinforcement learning</a><ul><li><a href="#the-agent-and-the-enviroment">The Agent and the Enviroment</a></li><li><a href="#what-is-the-markov-state">What is the Markov State?</a></li><li><a href="#returns-episodes-and-continuous-tasks">Returns, Episodes and Continuous Tasks</a></li><li><a href="#what-are-value-functions-and-policiesnbspnbsp">What are value functions and policies?&nbsp;&nbsp;</a></li></ul></li><li><a href="#the-markov-decision-process-mdp">The Markov Decision Process (MDP)</a><ul><li><a href="#the-value-functionnbsp">The Value Function&nbsp;</a></li><li><a href="#the-actionvalue-functionnbsp">The Action-Value Function&nbsp;</a></li></ul></li><li><a href="#solving-the-mdpnbsp">Solving the MDP&nbsp;</a><ul><li><a href="#the-optimal-value-and-actionvalue-function">The optimal value and action-value function</a></li></ul></li><li><a href="#grid-world-examplenbsp">Grid world Example&nbsp;</a></li><li><a href="#referencesnbsp">References&nbsp;</a></li>
    </ul>
  </div>
  

    <h2 id="what-is-reinforcement-learning-rl">
      What is Reinforcement Learning (RL)?
    </h2>

  <p>
    Reinforcement learning is a learning process in which agents learn through interacting with the environment and learn to take actions in a way that maximizes its reward signal. The reward signal is given by the environment depending on how well the agent performed in that environment. 
<br>
<br>Reinforcement learning is a computational approach that learns from interaction with an environment. The goal of a reinforcement learning agent is to maximize a numerical reward signal.&nbsp;
  </p>

    <h3 id="what-is-a-rewardnbsp">
      What is a reward?&nbsp;
    </h3>

  <p>
    A reward is a simple number given to the agent by the environment each time the agent interacts with the environment. The agent's goal is to maximize then the total amount of reward it receives, which means that the agent needs to maximize the cumulative reward in the long term, not the immediate reward in the short term.
  </p>

    <h3 id="the-reward-hypothesisnbsp">
      The Reward Hypothesis&nbsp;
    </h3>

  <p>
    The reward hypothesis states that an RL agent's goal is to maximize the expected value of the received scalar signal's cumulative reward. 
<br>
<br>Therefore, reinforcement learning gets based on learning methods that maximize the total cumulative reward by taking actions that help reach this goal.
  </p>

    <h2 id="components-in-reinforcement-learning">
      Components in Reinforcement learning
    </h2>

  <p>
    The two main components in any reinforcement learning-based algorithm are known as the agent and the environment.
  </p>

    <h3 id="the-agent-and-the-enviroment">
      The Agent and the Enviroment
    </h3>

  <p>
    An agent is a learner and the decision-maker inside an environment. The agent observes the environment and takes action, which is how the agent interacts with the environment. The environment then gives a reward to that agent based on the agent's action at that particular time and situation. 
<br>
<br>Everything that is outside of the agent is known as the environment.
  </p>

    <h3 id="what-is-the-markov-state">
      What is the Markov State?
    </h3>

  <p>
    In RL, understanding the concept of a state is vital. The state is the information that we use to determine what might happen next. Simply put, the state is a function of history. 
<br>
<br>The agent has its state, in which case the agent's state is the function of the agent's history and experiences collected by interacting with the environment.
<br>
<br>The environment also has its state, known as the environment state. 
<br>
<br>The agent's or the environment's state is a Markov state if the current state is independent of the past. This is because the past information (history) gets derived from the current state. Therefore the state doesn't depend on history. 
<br>
<br>"The future is independent of the past given the present" - and future states can be computed from the current state, and the history can be discarded. 
<br>
<br>Suppose the agent can access the environment state. In that case, the mathematical model of the RL problem is known as a fully observable Markov decision process, whereas if the agent cannot access the state of the environment, then it's known as a partially observable Markov decision process.
  </p>

    <h3 id="returns-episodes-and-continuous-tasks">
      Returns, Episodes and Continuous Tasks
    </h3>

  <p>
    The agent's goal is to maximize the total cumulative reward it receives from the environment over time. 
<br>
<br>To understand more about the return, we formalize it as follows. This is the basic form of the expected return, which is just the sum of all rewards gathered till the agent reaches the terminal state of the environment (last state of interaction with the environment) from some timestep t. Capital T denotes the final timestep.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/returneq-md.PNG" sizes="(max-width: 48em) 100vw, 768px" srcset="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/returneq-md-xs.PNG 300w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/returneq-md-sm.PNG 480w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/returneq-md-md.PNG 768w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/returneq-md-lg.PNG 1024w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/returneq-md-xl.PNG 1360w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/returneq-md-2xl.PNG 1600w"  height="56" width="344" alt="" />
      
    </figure>

  <p>
    Depending on the environment, the agent can work in episodic tasks or continuing tasks.
<br>
<br>
<br>An episodic task ends at a specific terminal state S+ with different rewards for different outcomes. (All nonterminal states denoted as S.)
<br>
<br>E.g., winning and losing a game of chess, the next episode or next game begins independently of how the previous game ended) The terminal state ends, and the environment resets back into a starting state. 
<br>
<br>
<br>Continous tasks don't have a terminal state. Therefore they go on without a limit. Here taking the total cumulative reward is tricky because the total cumulative reward will be T = ∞.  
<br>
<br>Now we introduce a notation known as the discount factor, a value between 0 and 1. This is so that in continuous environments, the total cumulative reward would not be ∞. 
<br>If we set the discount factor closer to 0, the agent tends to try and optimize to get higher immediate rewards. On the other hand, if the discount factor is closer to 1, the agent looks ahead to long-term rewards.
<br>
<br>Therefore the agent now tries to select actions that maximize the sum of the discounted rewards (discounted return) it receives from timestep t onwards. So at each state, the agent tries to pick the best action at timestep t that maximizes the expected discounted return:
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/1.png" sizes="(max-width: 48em) 100vw, 768px" srcset="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/1-xs.png 300w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/1-sm.png 480w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/1-md.png 768w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/1-lg.png 1024w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/1-xl.png 1360w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/1-2xl.png 1600w"  height="56" width="344" alt="" />
      
    </figure>

  <p>
    Depending on the environment, the agent can work in episodic tasks or continuing tasks.
<br>
<br>
<br>An episodic task ends at a specific terminal state S+ with different rewards for different outcomes. (All nonterminal states denoted as S.)
<br>
<br>E.g., winning and losing a game of chess, the next episode or next game begins independently of how the previous game ended) The terminal state ends, and the environment resets back into a starting state. 
<br>
<br>
<br>Continous tasks don't have a terminal state. Therefore they go on without a limit. Here taking the total cumulative reward is tricky because the total cumulative reward will be T = ∞.  
<br>
<br>Now we introduce a notation known as the discount factor, a value between 0 and 1. This is so that in continuous environments, the total cumulative reward would not be ∞. 
<br>If we set the discount factor closer to 0, the agent tends to try and optimize to get higher immediate rewards. On the other hand, if the discount factor is closer to 1, the agent looks ahead to long-term rewards.
<br>
<br>Therefore the agent now tries to select actions that maximize the sum of the discounted rewards (discounted return) it receives from timestep t onwards. So at each state, the agent tries to pick the best action at timestep t that maximizes the expected discounted return:
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/2.png" sizes="(max-width: 48em) 100vw, 768px" srcset="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/2-xs.png 300w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/2-sm.png 480w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/2-md.png 768w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/2-lg.png 1024w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/2-xl.png 1360w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/2-2xl.png 1600w"  height="66" width="522" alt="" />
      
    </figure>

  <p>
    In the following manner, the current expected discounted return at timestep t relates with the future discounted return at time step t+1. 
<br>
<br>Thus, the current expected discounted return is the immediate reward at timestep t plus the discounted expected return at time step t+1.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/3.png" sizes="(max-width: 48em) 100vw, 768px" srcset="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/3-xs.png 300w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/3-sm.png 480w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/3-md.png 768w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/3-lg.png 1024w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/3-xl.png 1360w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/3-2xl.png 1600w"  height="100" width="408" alt="" />
      
    </figure>

    <h3 id="what-are-value-functions-and-policiesnbspnbsp">
      What are value functions and policies?&nbsp;&nbsp;
    </h3>

  <p>
    Two other essential concepts in Reinforcement Learning are the value function and the policy function of an RL agent. 
<br>
<br>The value function of an RL agent describes how 'good' a state is in the long run.
<br>
<br>It's a function that maps a state to a value that describes how much total expected discounted return the agent would get if the agent takes actions starting from that given state.
<br>
<br>The policy is a function that maps a state with a given action. 
<br>
<br>Next, it describes how the behavior of the agent would be in the environment. Finally, it defines a way in which the agent would interact with the environment.
<br>The policy function can either be discrete or stochastic, in the sense that it can map to certain actions based on an argmax function, where it picks the action that has the highest return (a greedy policy), or picks an action based on a probabilistic distribution.
  </p>

    <h2 id="the-markov-decision-process-mdp">
      The Markov Decision Process (MDP)
    </h2>

  <p>
    The Markov Decision Process is a classic formalization of sequential decision making, where actions influence the immediate reward at time step t and future states those actions and the rewards at those future states. 
<br>
<br>The MDP process formalizes and helps us derive equations on dealing with the trade-off between immediate and delayed rewards. 
<br>
<br>There are two types of values that we estimate by using an MDP, so that we can better deal with the immediate and delayed reward trade-off. 
<br>
<br>We estimate the optimal value function that gives each state S's best possible value given optimal action selections. 
<br>
<br>We also estimate the optimal action-value function, which takes in as arguments the action and the state the agent is in and gives how good of an action that action is depending on the state. 
<br>
<br>Estimating these state-dependent quantities is essential to accurately assign credit to the agent's actions in the environment to make decisions better to effectively trade-off between immediate and delayed rewards. 
<br>
<br>The agent would observe the current state at time step t and get the current reward from transitioning into that state. Then, the state would take action, which hopefully maximizes the discounted return for the next timestep. 
<br>
<br>The environment would process this action if the environment would produce the next state and reward the agent for moving into the new state. 
<br>
<br>
<br>This is how the agent and the environment interact with each other. The agent's goal is to behave optimally in this environment, which is done by maximizing the total discounted return over time.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/4.png" sizes="(max-width: 48em) 100vw, 768px" srcset="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/4-xs.png 300w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/4-sm.png 480w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/4-md.png 768w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/4-lg.png 1024w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/4-xl.png 1360w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/4-2xl.png 1600w"  height="333" width="768" alt="" />
      
    </figure>

  <p>
    While interacting with the environment, this would be the sequential history the agent goes through.
<br>
<br>For example, the agent starts at timestep 0, observes the state, takes action, and obtains a reward, and this cycle repeats.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/5.png" sizes="(max-width: 48em) 100vw, 768px" srcset="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/5-xs.png 300w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/5-sm.png 480w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/5-md.png 768w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/5-lg.png 1024w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/5-xl.png 1360w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/5-2xl.png 1600w"  height="49" width="433" alt="" />
      
    </figure>

  <p>
    The MDP describes the probability in which the agent transitions from one state to the next.
<br>
<br>The likelihood that given the agent is in a given state 's' and takes action' a', the likelihood that that agent would end up in a new state " s' " and gets a reward 'r' is described as follows,&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/6.png" sizes="(max-width: 48em) 100vw, 768px" srcset="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/6-xs.png 300w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/6-sm.png 480w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/6-md.png 768w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/6-lg.png 1024w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/6-xl.png 1360w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/6-2xl.png 1600w"  height="56" width="588" alt="" />
      
    </figure>

  <p>
    The probability in taking action 'a' from state 's' and transitioning to state " s' " is the summation for all rewards in set R (all possible reward values in the MDP), and their transition probability of starting at state 's', taking action 'a', transitioning to state " s' " and getting probability 'r' for all rewards in set R.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/7.png" sizes="(max-width: 48em) 100vw, 768px" srcset="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/7-xs.png 300w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/7-sm.png 480w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/7-md.png 768w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/7-lg.png 1024w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/7-xl.png 1360w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/7-2xl.png 1600w"  height="103" width="710" alt="" />
      
    </figure>

  <p>
    The expected reward for choosing action a from state s can be described as the summation of all rewards in R weighted over all the state transition probabilities from state 's', taking action 'a', landing in the state " s' " and getting a reward 'r' afterward.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/8.png" sizes="(max-width: 48em) 100vw, 768px" srcset="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/8-xs.png 300w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/8-sm.png 480w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/8-md.png 768w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/8-lg.png 1024w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/8-xl.png 1360w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/8-2xl.png 1600w"  height="77" width="670" alt="" />
      
    </figure>

  <p>
    The expected reward the agent will receive at the current state " s' " given that the previous state and action was 's' and 'a', is the summation of all rewards R, weighted by the total probability of transitioning from state 's' to " s' " by taking action 'a' over the probability of transitioning from state 's' to " s' " by taking action 'a' and getting a specific reward 'r' in R.&nbsp;&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/9.png" sizes="(max-width: 48em) 100vw, 768px" srcset="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/9-xs.png 300w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/9-sm.png 480w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/9-md.png 768w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/9-lg.png 1024w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/9-xl.png 1360w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/9-2xl.png 1600w"  height="78" width="737" alt="" />
      
    </figure>

  <p>
    Here it shows that the probability of transitioning from all states " s' " and getting all rewards 'r' from those states given that the agent was in state 's' and takes action 'a' sums to 1. This shows that the function 'p' specifies a probability distribution for each choice of state 's' and action 'a'.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/10.png" sizes="(max-width: 48em) 100vw, 768px" srcset="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/10-xs.png 300w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/10-sm.png 480w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/10-md.png 768w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/10-lg.png 1024w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/10-xl.png 1360w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/10-2xl.png 1600w"  height="81" width="521" alt="" />
      
    </figure>

    <h3 id="the-value-functionnbsp">
      The Value Function&nbsp;
    </h3>

  <p>
    The value function defines how good it is to be in a particular state. 
<br>
<br>This means it gives the total expected discounted reward that is possible to receive, given that the agent continues to take actions from that state onward. 
<br>
<br>The value function depends upon the state, and the policy the agent follows since the selection of actions from state 's' onwards depends on the agent's policy or behavior. 
<br>
<br>The below equation shows that the value function when the agent uses a given policy π and starts at state s, is equivalent by the MDP to the expectation of the total discounted return from timestep t onwards, given that the agent currently at timestep t is on the state 's'.
<br>
<br>Afterward, we can expand the definition of the total discounted return and show that this is true for all states in the MDP.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/11.png" sizes="(max-width: 48em) 100vw, 768px" srcset="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/11-xs.png 300w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/11-sm.png 480w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/11-md.png 768w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/11-lg.png 1024w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/11-xl.png 1360w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/11-2xl.png 1600w"  height="68" width="613" alt="" />
      
    </figure>

  <p>
    We can also evaluate this expectation as follows by the recursive defintion of the total discounted return.
<br>
<br>Here it shows that the value function is the summation of all actions that can be made by the agent, and the probability the agent will choose that action  is weighted by the sum the transitioning probabiltieis of all states and rewards from state 's' by taking action 'a' multiplied by the immediate reward of the given transitioning probabilities plus the discounted value of the value function under policy π for the next state " s' ".
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/12.png" sizes="(max-width: 48em) 100vw, 768px" srcset="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/12-xs.png 300w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/12-sm.png 480w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/12-md.png 768w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/12-lg.png 1024w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/12-xl.png 1360w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/12-2xl.png 1600w"  height="173" width="567" alt="" />
      
    </figure>

    <h3 id="the-actionvalue-functionnbsp">
      The Action-Value Function&nbsp;
    </h3>

  <p>
    The action-value function maps a state and an action to show how much expected total discounted return the agent will receive from the environment if the agent starts at state 's' and takes action 'a' from that time until the environment terminates.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/13.png" sizes="(max-width: 48em) 100vw, 768px" srcset="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/13-xs.png 300w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/13-sm.png 480w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/13-md.png 768w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/13-lg.png 1024w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/13-xl.png 1360w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/13-2xl.png 1600w"  height="87" width="630" alt="" />
      
    </figure>

    <h2 id="solving-the-mdpnbsp">
      Solving the MDP&nbsp;
    </h2>

  <p>
    Solving the MDP entails that we find the optimal value function or the optimal action-value function, which can be derived from the optimal value function in the MDP and allowing the agent to pick the best action that gives the highest possible action-value for the state that the agent is currently at timestep t.&nbsp;
  </p>

    <h3 id="the-optimal-value-and-actionvalue-function">
      The optimal value and action-value function
    </h3>

  <p>
    The optimal value function is the maximum value that can be obtained from the state s given under the policy, which enables the agent to choose such actions from that state onwards which maximizes the expected total discounted return from that state onwards.&nbsp;&nbsp;<br>
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/14.png" sizes="(max-width: 48em) 100vw, 768px" srcset="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/14-xs.png 300w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/14-sm.png 480w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/14-md.png 768w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/14-lg.png 1024w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/14-xl.png 1360w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/14-2xl.png 1600w"  height="47" width="230" alt="" />
      
    </figure>

  <p>
    The optimal action-value function is the maximum action value obtained from state 's' by taking action 'a' under the policy that enables the agent to choose such actions from that state and action onwards, which maximizes the expected total discounted return from that state, action pair onwards.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/15.png" sizes="(max-width: 48em) 100vw, 768px" srcset="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/15-xs.png 300w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/15-sm.png 480w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/15-md.png 768w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/15-lg.png 1024w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/15-xl.png 1360w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/15-2xl.png 1600w"  height="42" width="245" alt="" />
      
    </figure>

  <p>
    The optimal action-value function can be written in terms of the optimal value function as the expectation of the immediate reward plus the discounted optimal value of the next state given that the current state is 's' and the action taken from that state is action 'a'.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/16.png" sizes="(max-width: 48em) 100vw, 768px" srcset="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/16-xs.png 300w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/16-sm.png 480w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/16-md.png 768w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/16-lg.png 1024w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/16-xl.png 1360w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/16-2xl.png 1600w"  height="35" width="417" alt="" />
      
    </figure>

  <p>
    The recurrence relation of the optimal value function is the value of the maximum action 'a' that can be taken which is a summation of all the states and rewards in the sets S and R of the MDP, weighted by the transition probability of moving to those states obtaining those rewards multiplied by the immediate reward plus the discounted optimal value of the next state the agent would move to.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/17.png" sizes="(max-width: 48em) 100vw, 768px" srcset="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/17-xs.png 300w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/17-sm.png 480w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/17-md.png 768w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/17-lg.png 1024w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/17-xl.png 1360w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/17-2xl.png 1600w"  height="208" width="447" alt="" />
      
    </figure>

  <p>
    The recurrence relation of the optimal action-value function is the sum of all states and rewards in S and R of the MDP, weighted by the transition probability multiplied by the immediate reward 'r' plus the discounted best action-value function of the next state the agent would move into by taking action " a' " from that state.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/18.png" sizes="(max-width: 48em) 100vw, 768px" srcset="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/18-xs.png 300w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/18-sm.png 480w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/18-md.png 768w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/18-lg.png 1024w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/18-xl.png 1360w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/18-2xl.png 1600w"  height="109" width="523" alt="" />
      
    </figure>

    <h2 id="grid-world-examplenbsp">
      Grid world Example&nbsp;
    </h2>

  <p>
    In this example, the RL agent has to traverse the following grid world, and any action that doesn't involve traversing from A to A' or B to B' gives a reward of -1. Otherwise, a reward of +10 and +5 is given if the agent gets teleported from A to A' or B to B'.
<br>
<br> 
<br>The agent can only take one step in north, east, south, and west. 
<br>
<br>While the agent updates its value function by interacting and moving in this grid world environment, the value function over time would update as shown in the below table.
<br>
<br>It shows how much discounted total return each cell would give the agent over the long run.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/19.png" sizes="(max-width: 48em) 100vw, 768px" srcset="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/19-xs.png 300w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/19-sm.png 480w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/19-md.png 768w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/19-lg.png 1024w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/19-xl.png 1360w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/19-2xl.png 1600w"  height="182" width="508" alt="" />
      
    </figure>

  <p>
    Once the agent finds the optimal policy that makes the agent optimize on getting the best possible discounted returns from this grid world, the value function would converge to the optimal value function. 
<br>
<br>
<br>By acting greedily upon the optimal value function, we find the optimal policy function.
<br>
<br>If the agent chooses actions that get to better states (cells) with a higher value than the other states accessible from the current state the agent is in, the agent has successfully found the best policy to act optimally in this grid world in the below diagram.&nbsp;&nbsp;<br>
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/20.png" sizes="(max-width: 48em) 100vw, 768px" srcset="file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/20-xs.png 300w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/20-sm.png 480w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/20-md.png 768w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/20-lg.png 1024w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/20-xl.png 1360w ,file:///C:/Users/chirath/Documents/Publii/sites/the-nlp-student/input/media/posts/temp/responsive/20-2xl.png 1600w"  height="266" width="768" alt="" />
      
    </figure>

  <p>
    That brings us to the end of the basics of reinforcement learning part 1.
<br>
<br>This would be a 3 part series. In the second post, we will be going over Multi-armed bandits, an interesting problem in Reinforcement Learning. In the 3rd and final post, we will be going over finding the optimal value function solving the MDP using Dynamic Programming. 
<br>
<br>Stay tuned, and follow me on Twitter <a href="https://twitter.com/StudentNlp" target="_blank">@studentnlp</a>, so you don't miss any new posts. 
<br>
<br>Cheers!&nbsp;
  </p>

    <h2 id="referencesnbsp">
      References&nbsp;
    </h2>

  <ul>
    <li>Sutton, R.S. &amp; Barto, A.G., 2018. Reinforcement learning: An introduction, MIT press.</li><li><a href="https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ" target="_blank">David Silver "Introduction to Reinforcement Learning" DeepMind 2015&nbsp;</a><br></li>
  </ul>

  <p>
    &nbsp;
  </p>

  <p>
    &nbsp;&nbsp;
  </p>

  <p>
    &nbsp;
  </p>

  <p>
    &nbsp;
  </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>The Transformer Explained </title>
        <author>
            <name>The NLP Student</name>
        </author>
        <link href="https://thenlpstudent.github.io/the-transformer-explained.html"/>
        <id>https://thenlpstudent.github.io/the-transformer-explained.html</id>

        <updated>2022-01-26T21:24:59+05:30</updated>
            <summary>
                <![CDATA[
                     Welcome! In this article, we will be going over what a Transformer is, the intuition and the inner workings behind the attention mechanism it employs to process sequential data, and how the Multi-Head Attention mechanism works as implemented by the paper 'Attention is all&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                
  <p>
    Welcome! In this article, we will be going over what a Transformer is, the intuition and the inner workings behind the attention mechanism it employs to process sequential data, and how the Multi-Head Attention mechanism works as implemented by the paper 'Attention is all you need' NeurIPS 2017.
  </p>

  <div class="post__toc">
    <h3>Quick Links&nbsp;</h3>
    <ul>
      <li><a href="#the-big-picturenbsp">The Big Picture&nbsp;</a></li><li><a href="#how-the-transformer-worksnbsp">How the transformer works&nbsp;</a><ul><li><a href="#step-1-word-and-positional-embeddingnbsp">Step 1: Word and Positional Embedding&nbsp;</a></li><li><a href="#step-2-encoding-sequences-using-attentionnbsp">Step 2: Encoding Sequences using Attention&nbsp;</a><ul><li><a href="#what-is-attentionnbsp">What is attention?&nbsp;</a></li><li><a href="#what-are-the-add-and-norm-layersnbsp">What are the 'add' and 'norm' layers?&nbsp;</a></li><li><a href="#the-feedforward-layersnbsp">The feed-forward layers&nbsp;</a></li></ul></li><li><a href="#step-3-decoding-the-input-sequencesnbsp">Step 3: Decoding the input sequences&nbsp;</a></li><li><a href="#step-4-training-and-inferencenbsp">Step 4: Training and Inference&nbsp;</a></li></ul></li><li><a href="#taking-a-deep-dive-into-attentionnbsp">Taking a deep dive into Attention&nbsp;</a><ul><li><a href="#maskingnbsp">Masking&nbsp;</a></li><li><a href="#attention-in-the-decoder-layersnbsp">Attention in the decoder layers&nbsp;</a></li></ul></li><li><a href="#multihead-attention-cause-ngt1-heads-are-better-than-one">Multi-Head Attention: cause n&gt;1 heads are better than one!</a></li><li><a href="#referencesnbsp">References&nbsp;</a></li>
    </ul>
  </div>
  

  <p>
    &nbsp;A transformer is a deep learning architecture that performs well for sequential data-related Machine learning tasks. It is based around an encoder-decoder architecture to handle and process sequential data in parallel. In addition, it uses a mechanism known as Attention to look back and forwards the sequential input and identify long-range patterns and relationships between each component in the sequence.
<br>
<br>The transformer architecture solves the shortcomings of recurrent neural networks and convolutional neural networks when processing sequential data and making good predictions. 
<br>
<br>Recurrent neural networks aren't capable of processing sequential data in parallel. Therefore it increases training time since we can't utilize the full power of GPU processing units for parallel matrix multiplication if data is processed sequentially.
<br>
<br>Convolutional neural networks can also process sequential data in parallel. But due to their window size, they can only look back and forward the input sequence, making them unable to identify good long-distance relationships between the entire sequence. 
<br>
<br>The attention mechanism can look at the whole input sequence at once and identify the long-distance relationships of every element (i.e. word) in the sequence with every other element of the sequence, which is why transformers, in general, perform better when it comes to sequential data processing.
  </p>

    <h2 id="the-big-picturenbsp">
      The Big Picture&nbsp;
    </h2>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/transformer-outline.png" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/transformer-outline-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/transformer-outline-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/transformer-outline-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/transformer-outline-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/transformer-outline-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/transformer-outline-2xl.png 1600w"  height="669" width="768" alt="" />
      <figcaption>High level overview of the transformer architecture and it's key components</figcaption>
    </figure>

  <p>
    &nbsp;
  </p>

  <p>
    In a nutshell, the transformer uses an encoder-decoder architecture, where N (N=5 in the  different encoding layers encode the input sequences, and N different decoder layers decode the information encoded by the encoding layers to predict the next word in sequence, depending on the NLP task at hand.  
<br>
<br>Before sending the input sequences into the encoder layers or to the decoder layers of the transformer, word embeddings and positional embeddings preprocess the input sequences so that the transformer can process the data more intuitively and effectively. 
<br>
<br> We will discuss the concept of word embedding and positional embedding once we dive more deeply into the inner workings of the transformer.
<br>
<br>These encoder layers are stacked on top of each other. The last encoder layer creates an encoded embedding matrix that contains a representation of the learned sequence.
<br>
<br>Finally, the decoder layer utilizes this matrix for a certain natural language processing task such as text classification, text generation, or machine translation. 
<br>
<br>The original paper (Attention is all you need) utilized the transformer to convert English input sentences to German and French, proving that transformers are good at machine translation tasks. 
<br>
<br>So let's dive deep into the inner workings of the transformers' encoder and decoder layers, how Attention works, and how supervised learning can train the transformer to optimize to perform better at machine translation and similar text processing tasks.
  </p>

    <h2 id="how-the-transformer-worksnbsp">
      How the transformer works&nbsp;
    </h2>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/1-do7YDFF2sads0p9BnjzrWA-2-md.png" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-md-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-md-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-md-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-md-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-md-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-md-2xl.png 1600w"  height="671" width="500" alt="" />
      <figcaption>Detailed diagram of the transformer, from the paper 'Attention is all you need' NIPS 2017</figcaption>
    </figure>

  <p>
    The paper that started it all by introducing the transformer at NIPS 2017 was called 'Attention is all you need', where Ashish Vaswani et al. proposed a deep learning model capable of processing sequences utilizing only Attention. Earlier variants of using Attention with sequence data involved connecting an attention layer to a recurrent neural network like a Long Short Term Memory (LSTM) network or a (Gated recurrent unit) GRU network. 
<br>
<br>However, transformers proved that relying solely on the attention mechanism yielded better results, especially in the NLP problem area of language translation. 
<br>
<br>Let's look at how the transformer processes information in a step by step manner and learns to make better predictions overtime.
  </p>

    <h3 id="step-1-word-and-positional-embeddingnbsp">
      Step 1: Word and Positional Embedding&nbsp;
    </h3>

  <p>
    The first step involves creating embeddings for the input sequences by word and positional embedding. 
<br>
<br>Deep learning models can't process words the way humans do. They deal with vectors and matrices to make predictions. Therefore we must first convert words in the sequence into word vectors, representing a word in the vocabulary. 
<br>
<br>Before the input sequences enter the encoders, the inputs are preprocessed and converted to word vectors, representing a word's contextual meaning in numeric form, so transformers can carry out numerical calculations to make predictions. 
<br>
<br>When training a transformer, we have a set of vocabulary that we would like the transformer to learn. A vocabulary is a set of distinct words that you want the transformer to learn to do the NLP problem at hand effectively.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/1-4UHP_q1_FdqMr1T5yvXEKg-md.jpeg" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-md-xs.jpeg 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-md-sm.jpeg 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-md-md.jpeg 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-md-lg.jpeg 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-md-xl.jpeg 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-md-2xl.jpeg 1600w"  height="650" width="485" alt="" />
      <figcaption>A vocabulary list with distinct words with their corresponding indices</figcaption>
    </figure>

  <p>
    These word embeddings convert a given word to a word vector, which is a vector with a dimension of 'dᵐᵒᵈᵉˡ', which is a constant, or referred to as a hyperparameter of the network. The original paper had word vectors with the size of 512, which meant that to represent one word, the transformer converted each word into 512 numbers and stored them in vectors. 
<br>
<br>Word vectors can also be projected onto a 2-D plane, by shrinking their dimensionality from N dimensions to 2 dimensions. Once projected, the words that have similar context are grouped near each other.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/3225.1569667846-md.png" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-md-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-md-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-md-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-md-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-md-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-md-2xl.png 1600w"  height="456" width="768" alt="" />
      <figcaption>Word vectors projected onto a 2-D plane, words that share similar context are close by to each other</figcaption>
    </figure>

  <p>
    Therefore to process the input sequence, we get the input word tokens from the sequence. We map each word to the location of the word in the vocabulary, assuming that the word is in the vocabulary. 
<br>
<br>For example, the word index in the vocabulary could be 21; 21 is the word ID. So for every word in the sequence, we map it to its word ID. 
<br>
<br>The below diagram shows multiple sentences with words being represented as a 2 dimensional matrix.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/Capture-md.PNG" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/Capture-md-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture-md-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture-md-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture-md-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture-md-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture-md-2xl.PNG 1600w"  height="293" width="768" alt="" />
      <figcaption>Think of the sample size (row) as a sentence, and Sequence Length as the number of words in each sentence, a cell is a word</figcaption>
    </figure>

  <p>
    So our word tokens in the input sequences to the transformers are converted into word vectors, so the transformer knows the word's context when it processes it.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/Capture2-md.PNG" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-md-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-md-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-md-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-md-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-md-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-md-2xl.PNG 1600w"  height="240" width="768" alt="" />
      
    </figure>

  <p>
    Once the word embeddings are created in this manner, positional encoding is also done to the input word tokens in the sequences before the sequences are sent off to the encoder layers.
<br>
<br>Positional encoding tells the transformer what position the word vectors are in relative to each other in the sequence. Since sequences are processed in parallel, the transformer doesn't know beforehand what order the word vectors come in. In the perspective of a transformer, all words are in a similar position which is untrue. Therefore positional encoding encodes the word's relative position information into the representation before being processed by the encoder and decoder layers. 
<br>
<br>Positional encoding is done using a sine and cosine function, and computed independtly of the word embeddings.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/Capture3-md.PNG" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-md-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-md-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-md-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-md-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-md-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-md-2xl.PNG 1600w"  height="144" width="518" alt="" />
      
    </figure>

  <ul>
    <li><b>pos</b>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; : Position of the word in the sequence (or sentence)</li><li><b>d_model</b>&nbsp; : Length of the word vector, also known as the embedding size &nbsp;</li><li><b>i&nbsp; &nbsp; </b>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; :  Index value of the word vector&nbsp;&nbsp;<br></li>
  </ul>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/capture4-md.PNG" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/capture4-md-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture4-md-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture4-md-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture4-md-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture4-md-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture4-md-2xl.PNG 1600w"  height="578" width="768" alt="" />
      
    </figure>

    <h3 id="step-2-encoding-sequences-using-attentionnbsp">
      Step 2: Encoding Sequences using Attention&nbsp;
    </h3>

  <p>
    he N encoder layers encode the word vector sequences. Depending on the implementation the number of encoder/decoder layers can vary, the proposed transformer architecture in the paper has N = 6 encoder and decoder layers, stacked on top of one another.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/Group-11-2-md.png" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-md-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-md-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-md-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-md-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-md-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-md-2xl.png 1600w"  height="640" width="405" alt="" />
      
    </figure>

  <p>
    &nbsp;
  </p>

  <ol>
    <li>The input to the layer gets processed by an attention layer.</li><li>An operation known as the 'Add &amp; Norm' is done afterward, then the output is passed down to a feed-forward network.</li><li>Another 'Add &amp; Norm' operation is done to the output and sent onto the next encoder layer.</li><li>The next encoder layer carries out the same operations, and the output is passed onto the next encoder on top of that afterward.&nbsp;</li><li>The decoder layers utilize the output of the last encoder layer to generate the final output. &nbsp;</li>
  </ol>

    <h4 id="what-is-attentionnbsp">
      What is attention?&nbsp;
    </h4>

  <p>
    Before moving forward, it is essential to understand intuitively what the attention layer does in the encoder layer.
<br>
<br>The attention layer tries to create a matrix that gives a score on how much each word relates to every other word in that sequence.
<br>
<br>The following visual depicts what the attention layer tries to do to the input sequences.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/capture5-md.PNG" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/capture5-md-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture5-md-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture5-md-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture5-md-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture5-md-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture5-md-2xl.PNG 1600w"  height="537" width="688" alt="" />
      <figcaption>Softmax scores of an attention weight matrix</figcaption>
    </figure>

  <p>
    The following visual depicts what the attention layer tries to do to the input sequences. The attention layer lets the transformer model know where to focus on each word and its relation to other words. These long-range patterns are vital for various natural language processing tasks. Attention allows the transformer to look at the data at once and identify patterns effectively, which is why transformers perform better than RNNs.
  </p>

    <h4 id="what-are-the-add-and-norm-layersnbsp">
      What are the 'add' and 'norm' layers?&nbsp;
    </h4>

  <p>
    The add and norm layer takes the output generated by the attention layer and the input for the attention layer, adds them together, and passes them as input to the Layer normalization function. 
<br>
<br><b>"Layer Normalization directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases."
</b><br>
<br>Here's the equation for the layer normalization function, where we first calculate the mean of the vector and using the mean, you calculate \row, which becomes the new input to the feed-forward layer.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/Capture6-md.PNG" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-md-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-md-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-md-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-md-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-md-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-md-2xl.PNG 1600w"  height="157" width="252" alt="" />
      
    </figure>

  <p>
    &nbsp;&nbsp;
  </p>

    <h4 id="the-feedforward-layersnbsp">
      The feed-forward layers&nbsp;
    </h4>

  <p>
    The feed-forward layer in the encoder layer is a point-wise feed-forward network, a fully connected feed-forward network consisting of two linear transformations W1, B1, and W2, B2 with a ReLU activation function in between. 
<br>
<br>The inputs for the feed-forward layer are d_model which is the embedding size, and the inner layer has a dimensionality of d_ff. The dimensions of the d_model and d_ff in the proposed architecture are 512, and 2048 respectively.&nbsp;
  </p>

    <h3 id="step-3-decoding-the-input-sequencesnbsp">
      Step 3: Decoding the input sequences&nbsp;
    </h3>

  <p>
    The decoder layers are more or less similar to each other, but their behavior slightly changes when we use the model to train and when we use it for inference.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/Group-2-md.png" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-md-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-md-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-md-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-md-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-md-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-md-2xl.png 1600w"  height="680" width="600" alt="" />
      
    </figure>

  <p>
    Like in the encoder layers, we embed the text input to the decoder layers, calculate their word vectors, and add positional encoding to it. We first input a 'START' token to the decoder layer to predict the next word in the sequence at the very end of the decoder layer.
<br>
<br>The next word predicted by the decoder layers is ignored during training, only used for the loss calculation and backpropagation of the loss to optimize the weights of the transformer.
<br>
<br>However, during inference, we append the predicted word as the next word token to our decoder layer and append it to the sequence to predict the next word at time step t+1. 
<br>
<br>Each decoder layer has two attention layers; one is responsible for finding the connections of the output sequence's words with words that come before it, not after it. 
<br>
<br>Then we send the output scores through an 'add &amp; norm' layer, implemented in the same manner as the encoder layers. 
<br>
<br>The next attention layer finds how relevant the words are in the output sequence compared with the input sequence from the encoder layer output.
<br>
<br>The information from the second attention layer passes along onto another 'add and norm' layer, afterward a point-wise feed-forward layer, and finally through another 'add and norm' layer, and onto the next decoder layer above it.
  </p>

    <h3 id="step-4-training-and-inferencenbsp">
      Step 4: Training and Inference&nbsp;
    </h3>

  <p>
    The final decoder layer has a linear layer, a single linear operation applied onto the output of the last decoder layer, and maps the output of the decoder layer into a vector with the dimensions of the vocabulary size. (E.g. 10000)
<br>
<br><b>A SoftMax function converts the values of that output vector into a probability distribution,</b> and the <b>index with the highest probability is chosen as the network's final output.
</b><br>
<br>This <b>chosen index position is mapped into the corresponding word by looking up the index from the vocabulary. 
</b><br>
<br>In the training phase of the network, we calculate the next word and calculate the loss of the probability distribution of the SoftMax function with the target distribution. This loss is backpropagated across the network, and the weights of every decoder and encoder block are updated.
<br>
<br><b>In the training phase, we use a method known as 'teacher forcing' where we manually input and append the correct word to the decoder layer's input sequence at each timestep to predict the next word.
</b><br>
<br><b>During inference, the predicted next word is appended to the current input sequence of the decoder, and until a 'END' token is generated, the next word is predicted and appended to the sequence.</b>
  </p>

    <h2 id="taking-a-deep-dive-into-attentionnbsp">
      Taking a deep dive into Attention&nbsp;
    </h2>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/SCALDE-md.png" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-md-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-md-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-md-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-md-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-md-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-md-2xl.png 1600w"  height="535" width="500" alt="" />
      <figcaption>Scaled Dot Product Attention</figcaption>
    </figure>

  <p>
    &nbsp;
  </p>

  <p>
    Attention is what makes the transformer understand long-range patterns in the sequence data that it processes and can make predictions considering the context of the input sequence and the relationships between each word vector in each sequence.
<br>
<br>The self-attention mechanism compares every word token in a given sequence with every other word token in the sequence and gauges how much they are important to each other. 
<br>
<br>They model the relationship each word token has with each other in the sequence. Whether that relationship is strong or weak, based on these attention scores, the feedforward layers of the encoder/decoder layer can make better predictions by understanding the dependency of each word token with the rest of the word tokens. 
<br>
<br>The below matrices shows how the attention layer scores word vector pairs based on how strong the relationships is between the two word vectors in the sequence. 
<br>
<br>Note that each word's contextual meaning as well as positional information is taken into account when the attention layer computes the scores.&nbsp;
  </p>

  <div  class="gallery-wrapper gallery-wrapper--center">
    <div class="gallery" data-columns="3">
      <figure class="gallery__item">
      <a href="https://thenlpstudent.github.io/media/posts/1/gallery/attention2-2.PNG" data-size="329x339">
        <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/gallery/attention2-2-thumbnail.PNG" height="339" width="329" alt="" />
      </a>
      
    </figure><figure class="gallery__item">
      <a href="https://thenlpstudent.github.io/media/posts/1/gallery/attention1-2.PNG" data-size="346x345">
        <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/gallery/attention1-2-thumbnail.PNG" height="345" width="346" alt="" />
      </a>
      
    </figure>
    </div>
  </div>

  <p>
    The attention model works by comparing every word token with every other word token using the dot-product operation. The more significant the magnitude of the dot-product, the higher likelihood that the pair of word vectors have a strong relationship and vice versa. Thus, the attention mechanism functions as a lookup dictionary.
<br>
<br>A dictionary has queries, keys, and values.
<br>We match up the queries with the keys and weigh them. The higher the dot product of the query and key pair, the higher the likelihood of them relating to one another.
<br>
<br>With the multiplication of the query and key pairs together with every Value vector in the Value Matrix, we find which word is most likely has a strong connection with which word in the sequence. 
<br>
<br>These query, key, and value are matrices created by multiplying the input sequence by weight matrices Wq, Wk, and Wv. These parameters are fine-tuned when the transformer trains, and over time the attention mechanism can learn the interrelationships between each word token for effective prediction over time. 
<br>
<br>Here are the formulas that generate matrices Q for Query, K for Key, and V for Value.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/Capture.PNG" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/Capture-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture-2xl.PNG 1600w"  height="213" width="205" alt="" />
      
    </figure>

  <p>
    The below diagram shows how the matrix multiplication occurs visually
  </p>

    <figure class="post__image post__image--wide">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/test1-md.PNG" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/test1-md-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/test1-md-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/test1-md-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/test1-md-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/test1-md-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/test1-md-2xl.PNG 1600w"  height="290" width="500" alt="" />
      
    </figure>

  <p>
    Here are the formulae that calculate the attention score for a given input sequence, where M is the mask matrix, which is explained in the next section.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/CodeCogsEqn5.gif" height="77" width="381" alt="" />
      
    </figure>

    <h3 id="maskingnbsp">
      Masking&nbsp;
    </h3>

  <p>
    A design feature of attention is masking. When calculating attention, it's important that we only consider the attention scores of valid word vectors.
<br>
<br>Some sentences might not be of the length as the sequence size in the encoder layer, and sentences can have varying lengths. So we add a unique token known as the 'PAD' token to make all sentences the same length. 
<br>
<br>However, the 'PAD' token is meaningless. Therefore we don't calculate the attention score of that token. 
<br>
<br>To cancel out the attention score of 'PAD' tokens, before we calculate the attention score using the SoftMax function, we add a mask matrix to the attention score.
<br>
<br>A mask matrix in the encoder layer might look something like this.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/mask1-md.PNG" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/mask1-md-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask1-md-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask1-md-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask1-md-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask1-md-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask1-md-2xl.PNG 1600w"  height="421" width="507" alt="" />
      
    </figure>

  <p>
    In the decoder layers, we want to force the network to only make predictions by looking at the word before it. Thus, we force the network to guess the next word in the sequence by looking back at the words before it. This type of mask is known as a look-ahead mask. 
<br>
<br>Here's what a mask in the decoder layer might look like this.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/mask2-md.PNG" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/mask2-md-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask2-md-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask2-md-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask2-md-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask2-md-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask2-md-2xl.PNG 1600w"  height="388" width="447" alt="" />
      
    </figure>

  <p>
    Here's the complete visualization on how attention is calculated
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/softmax-fn-md.PNG" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-md-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-md-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-md-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-md-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-md-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-md-2xl.PNG 1600w"  height="271" width="768" alt="" />
      <figcaption>How the attention weight matrix is calculated using the Softmax function with a suitable mask</figcaption>
    </figure>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/Group-38-md.png" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-md-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-md-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-md-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-md-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-md-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-md-2xl.png 1600w"  height="600" width="768" alt="" />
      <figcaption>How the final output of the self attention layer is calculated by using the V (values) matrix</figcaption>
    </figure>

    <h3 id="attention-in-the-decoder-layersnbsp">
      Attention in the decoder layers&nbsp;
    </h3>

  <p>
    In the decoder layer, we have an attention layer that relies on the encoder's input representation. 
<br>
<br>The encoder's input representation takes the Query and Key values, and the value matrix is taken from the decoder's input. Intuitively this means that we find the relationship between the encoder's input word vectors and how it relates to the decoder's word vectors. 
<br>
<br>For example, in the context of machine translation, if you have English words as input and translating them into German, the encoder's input representation would be English word vectors, and the decoder's input representation would be German word vectors. We find the relationship between the encoder's input representation and the decoder's input representation, essentially finding out how strong or weak the relationship is between English and German word pairs. 
<br>
<br>The below matrix shows how the attention layer gauges and compares every German word in the output sequence with every English word in the input sequence, and models the relationship between both the languages.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/scaledd-2-md.PNG" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-md-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-md-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-md-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-md-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-md-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-md-2xl.PNG 1600w"  height="402" width="350" alt="" />
      
    </figure>

    <h2 id="multihead-attention-cause-ngt1-heads-are-better-than-one">
      Multi-Head Attention: cause n&gt;1 heads are better than one!
    </h2>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/multi-head-attention_l1A3G7a-md.png" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-md-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-md-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-md-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-md-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-md-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-md-2xl.png 1600w"  height="648" width="500" alt="" />
      <figcaption>Multi-Head Attention mechanism in the transformer model, found in both the encoder & decoder layers</figcaption>
    </figure>

  <p>
    <b>Multi-head attention splits the input sequence into multiple parts and applies scaled dot product attention to each part individually. 
</b><br>
<br>This has been shown to improve the quality of attention as more complex relationships between word tokens can be extracted. 
<br>
<br>The input sequence is split based on the number of heads, a hyperparameter set before training the transformer. 
<br>
<br>For example, if the head size is 8, the input sequence is split into eight equal parts by dividing the embedding size by the number of attention heads. This is known as the query size.
<br>
<br>Query size is equal to the embedding size divided by the number of attention heads. 
<br>
<br>A linear layer is used to get the Q, K, and V matrices by multiplying the input matrix by the corresponding weight matrix. In this example, the embedding size is 6, and the number of attention heads used is 2. Therefore the query size is 6/2, which is 3. 
<br>
<br>As you can see in the above diagram, there is a red line separating the weights of the two heads, which is known as the logical split. When implementing, each attention head doesn't have its weight matrix, but the weights for every attention head are in one matrix. Thus, it's designed to easily update the weight matrices without taking up much memory, compute power, or time.&nbsp;
  </p>

    <figure class="post__image post__image--wide">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/part1-md.PNG" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/part1-md-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/part1-md-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/part1-md-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/part1-md-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/part1-md-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/part1-md-2xl.PNG 1600w"  height="341" width="605" alt="" />
      <figcaption>Using a logical split to share weights for the attention heads in the same weight matrices</figcaption>
    </figure>

  <p>
    Same as before, the K matrix transposes to multiply the Q and K matrices together. 
<br>
<br>However, before multiplying both these matrices, we first add an extra dimension known as the head size, which is 2 in this case. 
<br>
<br>This turns the 2D Q and K matrices to 3 dimensional. The V matrix is also reshaped to have a head dimension. Therefore all matrices are reshaped to the form Sequence Size x Head Size x Query Size.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/part2-md.PNG" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/part2-md-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/part2-md-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/part2-md-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/part2-md-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/part2-md-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/part2-md-2xl.PNG 1600w"  height="504" width="768" alt="" />
      <figcaption>Introducing a new dimension known as 'head size' to explicitly separate calculations for different attention heads</figcaption>
    </figure>

  <p>
    Afterward, we carry out the matrix multiplication for Q and K, where we multiply the matrices for each head, Q_head1 with K_head1 and Q_head2 with K_head2. 
<br>
<br>Afterward, every matrix multiplication of Q and K for each head results in a matrix with the shape sequence size x sequence size. 
<br>
<br>Finally, we add masks for each head's Q and K product, and we scale it by the square root of the query size and then apply a SoftMax function over it, creating separate attention weight matrices for each head, as show below.
  </p>

    <figure class="post__image post__image--wide">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/part3-md.PNG" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/part3-md-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/part3-md-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/part3-md-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/part3-md-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/part3-md-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/part3-md-2xl.PNG 1600w"  height="227" width="600" alt="" />
      
    </figure>

  <p>
    Finally, we multiply the corresponding V matrix's heads with attention weight heads giving us attention scores for each head, as show below.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/part4-md.PNG" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/part4-md-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/part4-md-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/part4-md-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/part4-md-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/part4-md-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/part4-md-2xl.PNG 1600w"  height="369" width="600" alt="" />
      
    </figure>

  <p>
    We need to reshape it back into its original form and merge all the results from the different heads into one by dropping the head dimension altogether.
<br>
<br>First, we reshape the matrix into the form Sequence Size x Query Size x Head Size by swapping the head size with the query size, which alters the shape of the matrix to the following form.
<br>
<br>We can get rid of the head dimension, which makes the 3-dimensional matrix back into two dimensions.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/part5-md.PNG" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/part5-md-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/part5-md-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/part5-md-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/part5-md-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/part5-md-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/part5-md-2xl.PNG 1600w"  height="269" width="600" alt="" />
      
    </figure>

  <p>
    &nbsp;
  </p>

  <p>
    Afterward, we multiply the resulting merged matrix using a <b>weight matrix W0</b> with a shape of <b>embedding size x embedding size</b>, and <b>we add a bias vector to the resulting matrix.&nbsp;</b><br>
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/part6-md.PNG" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/part6-md-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/part6-md-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/part6-md-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/part6-md-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/part6-md-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/part6-md-2xl.PNG 1600w"  height="206" width="600" alt="" />
      
    </figure>

  <p>
    This is how multi-head attention is done to a single sample of our input. Likewise, we carry out the following process simultaneously for all sentence samples, thanks to the fact that matrix multiplication is inherently <b>parallelizable</b>.&nbsp;
  </p>

    <h2 id="referencesnbsp">
      References&nbsp;
    </h2>

  <ul>
    <li><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" class="">Attention is all you need - Ashish Vaswani et al, NIPS 2017</a></li><li><a href="https://www.tensorflow.org/text/tutorials/transformer" target="_blank">Tensor flow model for language understanding</a>&nbsp;</li>
  </ul>

  <p>
    &nbsp;&nbsp;
  </p>

  <p>
    &nbsp;
  </p>

  <p>
    &nbsp;
  </p>

  <p>
    &nbsp;
  </p>

  <p>
    &nbsp;
  </p>

  <p>
    &nbsp;
  </p>

  <p>
    &nbsp;
  </p>
            ]]>
        </content>
    </entry>
</feed>
