<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <title>The NLP Student&#x27;s Blog</title>
    <link href="https://thenlpstudent.github.io/feed.xml" rel="self" />
    <link href="https://thenlpstudent.github.io" />
    <updated>2023-05-27T10:26:18+05:30</updated>
    <author>
        <name>The NLP Student</name>
    </author>
    <id>https://thenlpstudent.github.io</id>

    <entry>
        <title>Summary of the &quot;Generalization Bounds via Convex Analysis&quot; Paper [Draft]</title>
        <author>
            <name>The NLP Student</name>
        </author>
        <link href="https://thenlpstudent.github.io/summary-of-the-generalization-bounds-via-convex-analysis-paper.html"/>
        <id>https://thenlpstudent.github.io/summary-of-the-generalization-bounds-via-convex-analysis-paper.html</id>

        <updated>2023-05-27T10:26:18+05:30</updated>
            <summary>
                <![CDATA[
                    The "Generalization Bounds via Convex Analysis" paper by Gergely Neu, and Gábor Lugosi, discusses the generalization error of supervised learning algorithms and how it can be bounded regarding the mutual information between their input and output. The authors generalize this result beyond the standard choice&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><span data-preserver-spaces="true">The <strong>"Generalization Bounds via Convex Analysis" paper by Gergely Neu, and Gábor Lugosi</strong>, discusses the generalization error of supervised learning algorithms and how it can be bounded regarding the mutual information between their input and output.</span></p>
<p><span data-preserver-spaces="true">The authors generalize this result beyond the standard choice of Shannon's mutual information to measure the dependence between the input and output.</span></p>
<p><strong><span data-preserver-spaces="true">Shannon's mutual information</span></strong><span data-preserver-spaces="true"> is a measure of the amount of information that is shared by two random variables.</span></p>
<p><span data-preserver-spaces="true">The </span><strong><span data-preserver-spaces="true">mutual information between two random variables, X and Y, is the difference between X's entropy and X's conditional entropy given Y. </span></strong></p>
<p><span data-preserver-spaces="true">In previous papers, the generalization error of supervised learning algorithms was bounded by </span><strong><span data-preserver-spaces="true">Shannon's mutual information measure</span></strong><span data-preserver-spaces="true">. </span></p>
<p><span data-preserver-spaces="true">However, this paper shows that it's possible to replace the mutual information by any </span><strong><span data-preserver-spaces="true">strongly convex function</span></strong><span data-preserver-spaces="true"> of the joint input-output distribution with the </span><strong><span data-preserver-spaces="true">subgaussianity condition</span></strong><span data-preserver-spaces="true"> on the losses replaced by a bound on an appropriately chosen </span><strong><span data-preserver-spaces="true">norm</span></strong><span data-preserver-spaces="true"> capturing the geometry of the dependence measure.</span></p>
<p><span data-preserver-spaces="true">This fundamental proof allows us to derive a range of generalization bounds that are either entirely new or strengthen previously known ones. </span></p>
<p><span data-preserver-spaces="true">Examples include bounds in terms of</span><strong><span data-preserver-spaces="true"> p-norm divergences</span></strong><span data-preserver-spaces="true"> and the </span><strong><span data-preserver-spaces="true">Wasserstein-2 distance</span></strong><span data-preserver-spaces="true">, which are applicable for heavy-tailed loss distributions and highly smooth loss functions.</span></p>
<h2>Definitions</h2>
<p><span data-preserver-spaces="true">A function is considered</span><strong><span data-preserver-spaces="true"> strongly convex</span></strong><span data-preserver-spaces="true"> if it is convex and its second derivative is greater than or equal to some positive constant. In other words, a function is strongly convex if it is always "curving up" at a rate that is at least as fast as some positive constant. This property makes strongly convex functions useful in optimization problems because they have a </span><strong><span data-preserver-spaces="true">unique minimum that can be found efficiently. </span></strong></p>
<p><span data-preserver-spaces="true">A Subgaussian random variable is one whose tail probabilities decay exponentially. A subgaussianity condition is a condition that requires the loss of any fixed hypothesis to have a subgaussian tail.</span></p>
<p><span data-preserver-spaces="true">A norm is a function that assigns a non-negative real number to a vector space and behaves like the distance from the origin. </span></p>
<p><span data-preserver-spaces="true">A divergence is a function that takes two probability distributions as input and returns a number that measures how much they differ. The number returned must be non-negative and equal to zero if and only if the two distributions are identical. </span></p>
<h2>Problem</h2>
<p>\[gen(W_n, S_n) = L(W_n, S_n) - \mathbf{E} [\ell(W_n, \tilde{Z}) | W_n] \]</p>
<p>\(gen(W_n, S_n) \) : The generalization error of the learning algorithm. </p>
<p>\( L(W_n, S_n)\) : The training loss</p>
<p>\( \mathbf{E} [\ell(W_n, \tilde{Z}) | W_n] \) : The expected loss from testing </p>
<h2>Proof</h2>
<p> </p>
<h2>References </h2>
<p>1. Generalization Bounds via Convex Analysis (<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Lugosi%2C+G">Gábor Lugosi</a>, <a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Neu%2C+G">Gergely Neu) </a>- <a target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2202.04985</a></p>
<p> </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>&quot;Deep Reinforcement Learning From Human Preferences&quot; Paper Explained</title>
        <author>
            <name>The NLP Student</name>
        </author>
        <link href="https://thenlpstudent.github.io/deep-reinforcement-learning-from-human-preferences-paper-explained.html"/>
        <id>https://thenlpstudent.github.io/deep-reinforcement-learning-from-human-preferences-paper-explained.html</id>
            <category term="Reward functions"/>
            <category term="Reinforcement learning"/>
            <category term="GPT"/>

        <updated>2023-04-12T22:12:34+05:30</updated>
            <summary>
                <![CDATA[
                    This paper is the work of a collaboration with Deep Mind and Open AI, improving the field of Deep Reinforcement Learning. The ideas discussed in this paper are also a key component in the training of GPT-4. RL agents need good reward functions to learn complex tasks. However, it&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><span data-preserver-spaces="true"><a href="https://arxiv.org/abs/1706.03741">This paper</a> is the work of a collaboration with </span><strong><span data-preserver-spaces="true">Deep Mind </span></strong><span data-preserver-spaces="true">and </span><strong><span data-preserver-spaces="true">Open AI</span></strong><span data-preserver-spaces="true">, improving the field of</span><strong><span data-preserver-spaces="true"> Deep Reinforcement Learning. </span></strong></p>
<p><strong><span data-preserver-spaces="true">The ideas discussed in this paper are also a key component in the training of GPT-4</span></strong><span data-preserver-spaces="true">. </span></p>
<p><span data-preserver-spaces="true">RL agents need <strong>good reward functions</strong> to<strong> learn complex tasks</strong>. However, it takes work to design reward functions from scratch. </span></p>
<p>Learning a reward function using a neural network is also proposed. Another idea is why not let a human reward the agent altogether.</p>
<p><span data-preserver-spaces="true">This paper attempts to <strong>combine both of those two ideas</strong>. </span></p>
<p><strong><span data-preserver-spaces="true">The method discussed in this paper is a crucial component that Open AI used in training GPT-4</span></strong><span data-preserver-spaces="true">. </span></p>
<div class="post__toc">
<h3>Table of Contents</h3>
<ul>
<li><a href="#mcetoc_1gthqshlp2h">Introduction </a></li>
<li><a href="#mcetoc_1gthqshlp2i">Related Work </a></li>
<li><a href="#mcetoc_1gthqshlp2j">The methodology </a>
<ul>
<li><a href="#mcetoc_1gtr5h2ssi9">A breakdown of the process </a></li>
<li><a href="#mcetoc_1gtr5h2ssia">The Reward Function Estimator</a></li>
<li><a href="#mcetoc_1gtr5h2ssib">Selecting queries and asking for human feedback</a></li>
<li><a href="#mcetoc_1gtr5h2ssic">How feedback is processed </a></li>
<li><a href="#mcetoc_1gtr5h2ssid">Fitting the Reward Function </a></li>
</ul>
</li>
<li><a href="#mcetoc_1gthqshlp2k">Summary of results </a></li>
</ul>
</div>
<h2 id="mcetoc_1gthqshlp2f"></h2>
<h2 id="mcetoc_1gthqshlp2h">Introduction </h2>
<p>Reinforcement learning is a learning paradigm in which an <strong>agent observes the environment, takes an action, and receives a reward. </strong></p>
<p>This process happens continuously in a loop. <br><br>The agent's objective is to maximize the reward it accumulates by taking proper actions at the appropriate timestep t. </p>
<figure class="r48jcc pT0Scc iPVvYb"><img loading="lazy"  src="https://miro.medium.com/v2/resize:fit:1400/1*7cuAqjQ97x1H_sBIeAVVZg.png" alt="Reinforcement Learning 101. Learn the essentials of Reinforcement… | by  Shweta Bhatt | Towards Data Science" data-is-external-image="true"></figure>
<p>Notice that in traditional reinforcement learning, the environment gives the agent its reward.  <br><br>That means we need to design a reward function by looking at the environment at that timestep and see what amount of reward should be given to the agent. <br><br>Designing a simple reward function that approximately captures the intended behavior is problematic in complex tasks. In most cases, this leads to the agent optimizing for the reward function without satisfying the intended preferences. </p>
<p>Instead of designing a reward function, we can use a human to give out a reward by observing the environment. However, this is very expensive and time-consuming. <br><br>Therefore we need to devise a way that uses human feedback very sparingly. </p>
<p>The approach mentioned in the paper is to <strong>learn a reward function from human feedback and then optimize that reward function</strong>. This reward function is responsible for giving out rewards to the agent. <strong>The agent then optimizes its behavior to the learned reward function. </strong></p>
<p>The method proposed in the paper is beneficial in tasks where humans can recognize the desired behavior, but one that you can only demonstrate to the agent what the desired behavior is. <br><br>For example, consider an agent doing a black flip. You know when the agent does a backflip correctly and when an agent doesn't.  <br>But it would be easier to do a backflip if you are an expert at doing a backflip. </p>
<figure class="r48jcc pT0Scc iPVvYb"><img loading="lazy"  src="https://s.yimg.com/ny/api/res/1.2/VoXpdGw1yV8YCS9OiBBxSQ--/YXBwaWQ9aGlnaGxhbmRlcjt3PTY0MDtoPTM2MA--/https://o.aolcdn.com/hss/storage/midas/f2aa87f4f1dc29d0919055d960d5f6cf/205870720/ezgif.com-optimize+%284%29.gif" alt="Watch Boston Dynamics' Atlas robot nail a backflip | Engadget" data-is-external-image="true"></figure><br><br>This method allows agents to be trained by non-experts. <br>This method also scales to significant complex problems and claims to be economical with human feedback. </p>
<p>The algorithm proposed in the paper fits a reward function to given human feedback while simultaneously training a policy to optimize the currently predicted reward function. </p>
<figure class="post__image"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/9/sss.PNG" alt="" width="672" height="291" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/9/responsive/sss-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/9/responsive/sss-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/9/responsive/sss-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/9/responsive/sss-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/9/responsive/sss-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/9/responsive/sss-2xl.PNG 1600w"></figure><br><br>The human feedback is taken from a subject (volunteer, nonexpert), where they compare short video clips of the agent's behavior. </p>
<p>The subject picks one of the video clips with the correct behavior, or if both video clips are equally bad or equally good, they can choose "cannot be decided." <br><br>Since human feedback is taken in real-time, and the reward function changes in real-time, the <strong>agent cannot exploit the reward function </strong>since it constantly modifies (volatile). <strong>This method leads to the agent working in line with human preferences. </strong></p>
<h2 id="mcetoc_1gthqshlp2i">Related Work </h2>
<p><span data-preserver-spaces="true">The paper follows the same basic approach as previous works, </span><strong><span data-preserver-spaces="true">"Active preference learning-based reinforcement learning" </span></strong><span data-preserver-spaces="true">and</span><strong><span data-preserver-spaces="true"> "Programming by Feedback" by Akrour et al. </span></strong></p>
<p><span data-preserver-spaces="true">A difference from previous works is that this paper uses short clips instead of whole trajectories when getting feedback from the human subject. </span></p>
<p><span data-preserver-spaces="true">This paper has two orders of magnitude more comparisons but requires considerably less human time. </span></p>
<p><span data-preserver-spaces="true">The method proposed in this paper collects and processes human feedback, following closely with previous work, </span><strong><span data-preserver-spaces="true">"A Bayesian approach for policy learning from trajectory preference queries" by Wilson et al.</span></strong><span data-preserver-spaces="true"> </span></p>
<p><span data-preserver-spaces="true">However, Wilson's work uses "synthetic" human feedback drawn from a Bayesian model, whereas this paper uses nonexpert users. </span></p>
<p><span data-preserver-spaces="true">"</span><strong><span data-preserver-spaces="true">Interactive learning from policy-dependent human feedback</span></strong><span data-preserver-spaces="true">" by MalGlashan et al., </span><span data-preserver-spaces="true">"</span><strong><span data-preserver-spaces="true">Online human training of a myoelectric prosthesis controller via actor-critic reinforcement learning</span></strong><span data-preserver-spaces="true">" by Pilarski et al., </span></p>
<p><span data-preserver-spaces="true">"</span><strong><span data-preserver-spaces="true">Interactively shaping agents via human reinforcement: The TAMER framework</span></strong><span data-preserver-spaces="true">" by Knox and Stone and </span><strong><span data-preserver-spaces="true">"Learning non-myopically from human-generated reward"</span></strong><span data-preserver-spaces="true"> by Knox are previous works focusing on human feedback-based reinforcement learning.</span></p>
<p><span data-preserver-spaces="true">The above works have learning only occurring during episodes where the human trainer provides feedback (MalGlashan et al. and Pilarski et al.). However, this approach is not feasible in domains where an RL agent needs thousands of hours of trial and error to converge to an optimal policy. </span></p>
<p><span data-preserver-spaces="true">The works of Knox and Stone also learn a reward function. However, they consider much simpler settings where the desired policy can be quickly learned. </span></p>
<p><span data-preserver-spaces="true">The essential </span><strong><span data-preserver-spaces="true">contribution of this paper is to scale human feedback up to deep reinforcement learning and to learn much more complex behaviors.</span></strong><span data-preserver-spaces="true"> </span></p>
<p><span data-preserver-spaces="true">This paper fits into the recent trend of works that were focused on </span><strong><span data-preserver-spaces="true">scaling reward learning methods to large deep learning systems</span></strong><span data-preserver-spaces="true">, e.g. </span><span data-preserver-spaces="true">Inverse RL (Deep inverse optimal control via policy optimization by Finn et al.), Generative adversarial imitation learning by Ho et al., Generalizing skills with semi-supervised reinforcement learning by Finn et al.</span></p>
<h2 id="mcetoc_1gthqshlp2j">The methodology </h2>
<p>The paper tests their new method on the following RL training environments. </p>
<ol>
<li><strong>Arcade Learning <em>Environment</em> (ALE)</strong> - For testing RL agents using Atari Games </li>
<li><strong>MuJoCo</strong> - For testing robotic tasks via a physics simulator </li>
</ol>
<p>An RL agent interacts with the environment as follows, at each timestep (\ t \), the agent receives an observation \( o_t \in O \) from the environment and then sends an action \( a_t \in A \) to the environment. <br><br>Traditionally, the environment would provide reward signals, but a human overseer can express preferences between trajectory segments in this method rather than the environment. </p>
<p>A trajectory segment is a sequence of observations and actions.</p>
<p>\[ \sigma = ((o_0, a_0), (o_1, a_1), ... ,(o_{k-1}, a_{k-1})) \in (O \times A)^k \]</p>
<p>A pair of these trajectory segments are presented to a human at any given time. The following is how we denote that the trajectory \( \sigma^1 \) is prefered over trajectory \( \sigma^2 \) by the nonexpert user. </p>
<p>\[ \sigma^1 \succ \sigma^2 \]</p>
<p>In informal terms, the goal of the agent in the long run is to produce trajectories which are preferred by the human, while making as few queries as possible to the human, so that less time is utilized from the prespective of the human subject (nonexpert user). </p>
<h3 id="mcetoc_1gtr5h2ssi9">A breakdown of the process </h3>
<p>At each timestep, the method proposed in the paper maintains a policy and a reward function estimate. </p>
<p>The policy is a function that maps observations(from the state space O) to an action (in the action space A). </p>
<p>The reward function estimate maps observations and actions to a real value (scalar). <br><br>The policy and the reward function estimator are networks that learn over time from training and experience (via updating their weights). <br><br>The policy interacts with the environment to produce a set of trajectories (sequences of state and action pairs), and traditional reinforcement learning algorithms update the policy's parameters to maximize the reward the agent gets over time. <br><br>Specific pairs of segments sampled from the trajectories are taken and sent to a human for comparison and feedback. <br><br>The parameters of the reward function estimator are updated via supervised learning to fit with the comparisons made by the non-expert human user. <br><br>All these actions mentioned above run asynchronously from one to the other without halting the overall learning process of the agent. <br><br>The agent or the reward function estimator doesn't halt until the human subject gives feedback, and the systems run asynchronously in the background.  </p>
<h3 id="mcetoc_1gtr5h2ssia">The Reward Function Estimator</h3>
<p>The following diagram below shows their reward function estimator architecture. <br><br></p>
<figure class="post__image align-center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/9/Group-14.png" alt="" width="961" height="360" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/9/responsive/Group-14-xs.png 300w ,https://thenlpstudent.github.io/media/posts/9/responsive/Group-14-sm.png 480w ,https://thenlpstudent.github.io/media/posts/9/responsive/Group-14-md.png 768w ,https://thenlpstudent.github.io/media/posts/9/responsive/Group-14-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/9/responsive/Group-14-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/9/responsive/Group-14-2xl.png 1600w"></figure>
<p>It's a convolutional neural network with multiple convolutional layers, with the input as the state, and the output is a scalar value, which is also normalized via a sigmoid function to be a value between -1 and 1. <br><br>This reward is then given to the RL agent for any action it chooses to do. <br><br>The human non-expert trains the reward function estimator via feedback so that for each different state, suitable rewards are given that fit in line with the human's preferences. </p>
<h3 id="mcetoc_1gtr5h2ssib">Selecting queries and asking for human feedback</h3>
<p>One of the main goals of this paper is to minimize human time and ask for as little feedback as possible when training an RL agent to do a complex task. <br><br>The reward function estimator cannot query the human every time.  <br>Ideally, it should query the human subject when unsure what reward to give to a particular state and action pair.<br><br>In their method, they use several reward function estimators in an ensemble (of size 3) and consider all of the networks' predictions in the ensemble, when trying to get feedback from the non-expert human subject. </p>
<p>The diagram below shows a high-level picture of how the overall process works. </p>
<p>Many pairs of trajectory segments sampled from the buffer are run through the reward function estimators in the ensemble. Each network tries to predict which pair of trajectories are more likely to align with human preferences by scoring them. (A high reward predicted means the network "thinks" that the given trajectory is better, and vice versa) </p>
<figure class="post__image"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/9/Group-15.png" alt="" width="2524" height="2341" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/9/responsive/Group-15-xs.png 300w ,https://thenlpstudent.github.io/media/posts/9/responsive/Group-15-sm.png 480w ,https://thenlpstudent.github.io/media/posts/9/responsive/Group-15-md.png 768w ,https://thenlpstudent.github.io/media/posts/9/responsive/Group-15-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/9/responsive/Group-15-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/9/responsive/Group-15-2xl.png 1600w"></figure>
<p>Sometimes the networks in the ensemble cannot commonly agree on which two trajectories might be better than the other, meaning those two trajectories would have high variance, the highest pair, which contains the most amount of variance, is sent to the human subject for feedback. <br><br>The two pairs of trajectories selected forms a query, and the feedback from the human helps the reward estimator produce better rewards in the future aligned with human preferences. </p>
<h3 id="mcetoc_1gtr5h2ssic">How feedback is processed </h3>
<p>The human subject is shown two short video clips of the pair of trajectories, lasting for about 1 or 2 seconds. <br><br>The subject can then select whether the first clip is the best, if the second clip is the best, or if they can't decide. This response is recorded and used to construct training data for the reward estimator function to train on.</p>
<p>This forms a tuple  \( \left(\sigma^1, \sigma^2, \mu \right) \) where, \( \sigma^1 \) is the first trajectory, and \( \sigma^2 \) is the second trajectory, and \( \mu \) is the human preference distribution (feedback). <br><br>The below diagram illustrates how \( \mu \) is created. </p>
<figure class="post__image"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/9/Group-3.png" alt="" width="460" height="478" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/9/responsive/Group-3-xs.png 300w ,https://thenlpstudent.github.io/media/posts/9/responsive/Group-3-sm.png 480w ,https://thenlpstudent.github.io/media/posts/9/responsive/Group-3-md.png 768w ,https://thenlpstudent.github.io/media/posts/9/responsive/Group-3-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/9/responsive/Group-3-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/9/responsive/Group-3-2xl.png 1600w"></figure>
<h3 id="mcetoc_1gtr5h2ssid">Fitting the Reward Function </h3>
<p>The probability that the reward estimator function prefering trajectory \( \sigma^1 \) over \( \sigma^2 \) can be denoted by the following probability expression, </p>
<p>\[\hat{P}[\sigma^1 \succ \sigma^2] = \frac{\exp \sum \hat{r} \left( o^1_t , a^1_t \right) }{\exp \sum \hat{r} \left( o^1_t , a^1_t \right) + \exp \sum \hat{r} \left( o^2_t , a^2_t \right)}\]<br><br>And the probability that the reward estimator function prefering trajectory \( \sigma^2\) over \( \sigma^1 \) can be denoted by the following probability expression, </p>
<p>\[\hat{P}[\sigma^2 \succ \sigma^1] = \frac{\exp \sum \hat{r} \left( o^2_t , a^2_t \right) }{\exp \sum \hat{r} \left( o^1_t , a^1_t \right) + \exp \sum \hat{r} \left( o^2_t , a^2_t \right)}\]</p>
<p><strong>Note that \( \hat{r} \) is the reward estimator function, and \( o_t \) is state, and \( a_t \) is the action taken by the agent, at timestep \( t \). </strong></p>
<p>We write the loss of the function, as a cross-entropy loss, between the difference in the trajectory preferred by the reward estimator vs. the trajectory preferred by the human subject.</p>
<p>Note that, \( \mu(1) \) denotes the probability that the human subject prefers trajectory \( \sigma^1 \) and \( \mu(2) \) denotes the probability that the human subject prefers trajectory \( \sigma^2 \) </p>
<p style="overflow: auto;">\[\text{loss}\left( \hat{r} \right) = - \sum_{\left( \sigma^1, \sigma^2, \mu \right) \in D } \mu\left(1\right)log \hat{P} \left[ \sigma^1 \succ \sigma^2 \right] + \mu\left(2\right)log \hat{P} \left[ \sigma^2 \succ \sigma^1 \right]\]</p>
<p style="overflow: auto;">\[\text{loss}\left( \hat{r} \right) = - \sum_{\left( \sigma^1, \sigma^2, \mu \right) \in D } \mu\left(1\right)log  \frac{\exp \sum \hat{r} \left( o^1_t , a^1_t \right) }{\exp \sum \hat{r} \left( o^1_t , a^1_t \right) + \exp \sum \hat{r} \left( o^2_t , a^2_t \right)} + \mu\left(2\right)log \frac{\exp \sum \hat{r} \left( o^2_t , a^2_t \right) }{\exp \sum \hat{r} \left( o^1_t , a^1_t \right) + \exp \sum \hat{r} \left( o^2_t , a^2_t \right)}\]</p>
<h2 id="mcetoc_1gthqshlp2k">Summary of results </h2>
<h2 id="mcetoc_1gthqshlp2l"></h2>
<p><span data-preserver-spaces="true">Novel behaviors were observed in the Atari as well as the physics environment, </span></p>
<ol>
<li><span data-preserver-spaces="true"><span data-preserver-spaces="true">The Hopper robot performs a sequence of backflips. This behavior was trained using 900 queries in less than an hour. The agent learns to perform a backflip, land upright, and repeat consistently. </span></span></li>
<li><span data-preserver-spaces="true">The Half-Cheetah robot moves forward while standing on one leg. This behavior was trained using 800 queries in under an hour.</span></li>
<li><span data-preserver-spaces="true">Keeping alongside other cars in Enduro. This was trained with roughly 1,300 queries and 4 million frames of interaction with the environment; the agent learns to stay almost exactly even with other moving cars for a substantial fraction of the episode, although it gets confused by changes in the background.</span></li>
</ol>
<p>The videos of these novel behaviors can be founded here: <a href="https://drive.google.com/drive/folders/0BwcFziBYuA8RM2NTdllSNVNTWTg?resourcekey=0-w4PuSuFvi3odgQXdBDPQ0g">Deep RL with Human Preferences - Google Drive</a></p>
<p>Thanks for reading! </p>
<p> </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>What is Information Entropy?</title>
        <author>
            <name>The NLP Student</name>
        </author>
        <link href="https://thenlpstudent.github.io/looking-at-entropy.html"/>
        <id>https://thenlpstudent.github.io/looking-at-entropy.html</id>

        <updated>2023-04-06T13:45:35+05:30</updated>
            <summary>
                <![CDATA[
                    In this article we would be going over the concept of information entropy, a vital topic in machine learning and information theory. Before we understand entropy, a prerequisite is understanding the expected value, a topic in probability theory. Expected value is a mathematical concept that&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>In this article we would be going over the concept of information entropy, a vital topic in <strong>machine learning </strong>and <strong>information theory</strong>. </p>
<div class="post__toc">
<h3>Table of Contents</h3>
<ul>
<li><a href="#mcetoc_1gtal71mm6">What is information theory? </a></li>
<li><a href="#mcetoc_1gtal71mm7">What is expected value (EV)? </a></li>
<li><a href="#mcetoc_1gtanb52q2u">What is entropy? </a></li>
<li><a href="#mcetoc_1gtap5prc5m">References </a></li>
</ul>
</div>
<h2 id="mcetoc_1gtal71mm6">What is information theory? </h2>
<div class="relative h-[30px] w-[30px] p-1 rounded-sm text-white flex items-center justify-center"> </div>
<div class="text-xs flex items-center justify-center gap-1 invisible absolute left-0 top-2 -ml-4 -translate-x-full group-hover:visible !invisible"><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Information theory is a branch of mathematics and computer science that studies the quantification, storage, and communication of information. </span></div>
<div> </div>
<div class="text-xs flex items-center justify-center gap-1 invisible absolute left-0 top-2 -ml-4 -translate-x-full group-hover:visible !invisible"><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">It was developed by<strong> Claude Shannon</strong> in the mid-20th century at Bell Labs as a mathematical framework for understanding and analyzing communication systems.</span></div>
<div> </div>
<div>At its core, <strong>information theory is concerned with measuring the amount of information contained in a message, signal, or data set.</strong></div>
<div> </div>
<div>This can be done by calculating the <strong>entropy</strong> of the system, <strong>which is a measure of the uncertainty or randomness of the information.</strong></div>
<div> </div>
<div>Therefore information theory states that <strong>entropy is a measure of uncertainty or</strong></div>
<div><strong> randomness. </strong></div>
<div> </div>
<h2 id="mcetoc_1gtal71mm7">What is expected value (EV)? </h2>
<p>Before we understand entropy, a prerequisite is understanding the expected value, a topic in probability theory. </p>
<p>Expected value is a mathematical concept that represents the long-term average or average outcome of a random event or process.</p>
<p>It is calculated by multiplying each possible event outcome by its probability and then summing the results.</p>
<p>It's better to learn this concept with an example. </p>
<p>Let's say you flip an unfair coin, the probability of getting heads is 0.6, and the probability of getting tails is 0.4. If you get tails, you win $10, but if you get heads, you lose $5. </p>
<p>Under these conditions, would you consider playing the game? </p>
<p>We can use the expected value to estimate how much we would be expected to win or lose on average at each round. </p>
<p>Let's say you flip the coin 100 times (100 rounds). </p>
<table style="border-collapse: collapse; width: 100%;" border="1">
<tbody>
<tr>
<td style="width: 33.2843%;"> </td>
<td class="align-center" style="width: 33.2843%;"><strong>Heads</strong></td>
<td class="align-center" style="width: 33.2889%;"><strong>Tails</strong></td>
</tr>
<tr>
<td class="align-center" style="width: 33.2843%;"><strong>Probability</strong></td>
<td class="align-center" style="width: 33.2843%;">0.6</td>
<td class="align-center" style="width: 33.2889%;">0.4</td>
</tr>
<tr>
<td class="align-center" style="width: 33.2843%;"><strong>Outcome</strong></td>
<td class="align-center" style="width: 33.2843%;"><strong>- $5</strong></td>
<td class="align-center" style="width: 33.2889%;"><strong>+ $10</strong></td>
</tr>
</tbody>
</table>
<p> Based on the above table, in 100 rounds, we would win </p>
<p>\[100 * 0.4 = 40 \text{ rounds} \]</p>
<p>and the amount we would win in 40 rounds, is </p>
<p>\[ 40 *  \$10 = \$400 \]</p>
<p>Base on the above table, in 100 rounds, we would lose, </p>
<p>\[100 * 0.6 = 60 \text{ rounds}\]</p>
<p>and the amount we would lose in 60 rounds, is,</p>
<p>\[ 60 *  -\$5 = -\$300 \]</p>
<p>If we combine the above deductions into one single expression and divide by 100, we can get the average earnings/losses per round; we get the below expression</p>
<p>\[ \frac{(100 * 0.6 *  \$10) + ( 100 *   0.4 *   -\$5)}{100}  \]</p>
<p>However, we can simplify </p>
<p>\[ \frac{(\cancel{100} * 0.6 *  \$10) + ( \cancel{100} *   0.4 *   -\$5)}{\cancel{100}}  = 0.6 *  \$10 + (0.4 *  -\$5) = $4\]</p>
<p><span data-preserver-spaces="true">Therefore, on average, we earn $4 per round, the </span><strong><span data-preserver-spaces="true">expected value (EV)</span></strong><span data-preserver-spaces="true"> of playing the coin toss game with the unfair coin. </span></p>
<p><span data-preserver-spaces="true">More generally, the expected value can be written as follows, </span></p>
<p>\[ E[X] = \sum_i x_i * P(X=x_i) \]</p>
<p>X is a random variable (like the coin toss), and \(x_i\) denotes the outcome of the event, which is associated with the random variable (like getting heads or tails at a given moment), and \(P (X=x_i) \) is the probability that you would get heads/tails (e.g., \(P(X = tails) = 0.4 \) </p>
<h2 id="mcetoc_1gtanb52q2u">What is entropy? </h2>
<p>Entropy is a measure of uncertainty. <br><br>Let's say you have orange and blue balls in a bag, as shown below. </p>
<p>If you randomly select a ball without looking into the bag, there is a higher chance that you will pick an orange color ball. </p>
<p>It is almost certain that you would pick an orange color ball over a blue ball.</p>
<p><span data-preserver-spaces="true">Therefore, the uncertainty is low in which ball you would pick. </span></p>
<p>If you picked an orange ball, you wouldn't be surprised, but if you picked a blue ball you would be surprised. </p>
<p><span data-preserver-spaces="true">To break down entropy, we need to understand </span><strong><span data-preserver-spaces="true">surprise</span></strong><span data-preserver-spaces="true">. </span></p>
<figure class="post__image"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/7/Group-12.png" alt="" width="298" height="301" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/7/responsive/Group-12-xs.png 300w ,https://thenlpstudent.github.io/media/posts/7/responsive/Group-12-sm.png 480w ,https://thenlpstudent.github.io/media/posts/7/responsive/Group-12-md.png 768w ,https://thenlpstudent.github.io/media/posts/7/responsive/Group-12-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/7/responsive/Group-12-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/7/responsive/Group-12-2xl.png 1600w"></figure>
<p><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Now let's look at another scenario. </span></p>
<p><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">In the below image, there are more blue balls than orange ones. </span></p>
<p><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">The probability of picking a blue ball is higher than picking an orange ball. </span></p>
<p><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">If we were to pick a blue ball, we wouldn't be that much surprised. </span></p>
<p><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">But if we picked an orange ball, we would be very surprised.  </span></p>
<figure class="post__image"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/7/Group-2.png" alt="" width="306" height="309" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/7/responsive/Group-2-xs.png 300w ,https://thenlpstudent.github.io/media/posts/7/responsive/Group-2-sm.png 480w ,https://thenlpstudent.github.io/media/posts/7/responsive/Group-2-md.png 768w ,https://thenlpstudent.github.io/media/posts/7/responsive/Group-2-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/7/responsive/Group-2-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/7/responsive/Group-2-2xl.png 1600w"></figure>
<p>So we now think that surprise has an inverse relationship with probability. </p>
<p>A higher probability should make for a lower surprise, and a lower probability should make for a greater surprise. </p>
<p>However, as tempting as this would be, we can't define surprise like this, where \( p(x) \) is the probability of \(x\)</p>
<p>\[ \text{surprise} = \frac{1}{p(x)} \]</p>
<p>The graph for the above function would look something like this, </p>
<figure class="post__image"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/7/gg22.PNG" alt="" width="582" height="800" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/7/responsive/gg22-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/7/responsive/gg22-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/7/responsive/gg22-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/7/responsive/gg22-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/7/responsive/gg22-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/7/responsive/gg22-2xl.PNG 1600w"></figure>
<p>Since probability ranges between 0 and 1, the graph halts at 1. </p>
<p>So if a probability event is certain (i.e., \( p(x) = 1 \)), then the surprise should be zero. Since we won't be surprised, however, the expression we came up with doesn't reflect that. </p>
<p>So, let's take the log of \( \frac{1}{x} \). </p>
<p>\[ \text{surprise} = log (\frac{1}{x}) \]</p>
<p>Here's the graph</p>
<figure class="post__image"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/7/sss.PNG" alt="" width="686" height="347" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/7/responsive/sss-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/7/responsive/sss-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/7/responsive/sss-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/7/responsive/sss-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/7/responsive/sss-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/7/responsive/sss-2xl.PNG 1600w"></figure>
<p><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">This is a much better expression, as now, when the probability is 1 for a certain event, the surprise is 0. </span></p>
<p><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">And if the probability for an event is closer to zero (unlikely to happen), then the surprise is closer to infinity! </span></p>
<p><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Entropy is then just expected surprise of a random variable X. </span></p>
<p><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">To calculate expected surprise, we use the expression for EV. (Expected Values) </span></p>
<p><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">\[ E[\text{Surprise}] = \sum_i log \left( \frac{1}{p(x_i)} \right) * p(x_i) \]</span></p>
<p><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Now we can simplify this expression and derive the entropy expression that Shannon described in his paper in 1948, "A mathematical theory of communication." </span></p>
<p><strong><span data-preserver-spaces="true">Note that surprise is synonymous with uncertainty. </span></strong></p>
<p>\[ \begin{equation} \label{eq1} \begin{split} \text{Entropy} &amp; = \sum_i log \left( \frac{1}{p(x_i)} \right) * p(x_i) \\ &amp; = \sum_i p(x_i) * log \left( \frac{1}{p(x_i)} \right) \\ &amp; = \sum_i p(x_i) * ( log(1) - log(p(x_i))) \\ &amp; = \sum_i p(x_i) * log(1) - p(x_i)*log(p(x_i)) \\ &amp; = \sum_i - p(x_i)*log(p(x_i)) \\ &amp; = - \sum_i p(x_i)*log(p(x_i)) \\ \end{split} \end{equation} \]</p>
<h2>Visualizing entropy via Python</h2>
<p>We have three boxes (pens) with orange and blue balls. Each box (or pen) has a different probability of how the balls spawn. </p>
<p>In the first box, more orange balls than blue balls would spawn. </p>
<p>In the second box, more blue balls than orange balls would spawn. </p>
<p>In the third box, both blue and orange boxes would spawn with a probability of 0.5. Both of them are equally likely to spawn. </p>
<p>The bars denote the amount of entropy for each box, respectively.</p>
<p>Since it's more likely that an orange or blue ball to spawn in the first and second boxes, their entropy is lower, as its uncertainty is less. </p>
<p>But in the third box, since we don't know the ball's color, that would spawn next, then the entropy is high, meaning the uncertainty is high. </p>
<figure class="post__image"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/7/ludecomp.gif" alt="" width="1265" height="993"></figure>
<p>The implementation code can be found in the following gist:  <a href="https://gist.github.com/thenlpstudent/d20b5551568dcdbac9ba78d98e29ae24">https://gist.github.com/thenlpstudent/d20b5551568dcdbac9ba78d98e29ae24</a></p>
<p>Thanks for reading!</p>
<h2 id="mcetoc_1gtap5prc5m">References </h2>
<ol>
<li>A Mathematical Theory of Communication By C. E. SHANNON<strong> (<a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">shannon1948.dvi (harvard.edu)) </a></strong></li>
<li>Stat Quest, Expected Values and Entropy 
<ol>
<li><a href="https://www.youtube.com/watch?v=KLs_7b7SKi4">Expected Values, Main Ideas!!! - YouTube</a></li>
<li><a href="https://www.youtube.com/watch?v=YtebGVx-Fxw">Entropy (for data science) Clearly Explained!!! - YouTube</a></li>
</ol>
</li>
</ol>
<p> </p>
<p> </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>One Shot Learning, Few Shot Learning, and Similarity </title>
        <author>
            <name>The NLP Student</name>
        </author>
        <link href="https://thenlpstudent.github.io/new-post-2.html"/>
        <id>https://thenlpstudent.github.io/new-post-2.html</id>

        <updated>2023-04-05T15:40:27+05:30</updated>
            <summary>
                <![CDATA[
                    In this article we would be going over one shot learning, and few shot learning. Let's say you take a trip to the zoo with a kid. The kid is excited to learn about the various animals in the zoo, their names, and how they&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>In this article we would be going over one shot learning, and few shot learning. </p>
<div class="post__toc">
<h3>Table of Contents</h3>
<ul>
<li><a href="#mcetoc_1gt0shkn76d">The basic idea</a>
<ul>
<li><a href="#mcetoc_1gt0shkn76e">Let's break down the process</a></li>
</ul>
</li>
<li><a href="#mcetoc_1gt0shkn76f">The Main Concepts</a>
<ul>
<li><a href="#mcetoc_1gt2fu1u17">The Training set</a></li>
<li><a href="#mcetoc_1gt2fvb3pg">The Support set</a></li>
<li><a href="#mcetoc_1gt2fvb3ph">The Query</a></li>
</ul>
</li>
<li><a href="#mcetoc_1gt8e8muqea">Training Models to Detect Similarity Using Siamese Networks </a>
<ul>
<li><a href="#mcetoc_1gt83aduh9g">The Contrastive Loss Function</a></li>
<li><a href="#mcetoc_1gt2g3pla1a">Triplet Loss </a></li>
</ul>
</li>
<li><a href="#mcetoc_1gt85s9nlbd">References </a></li>
</ul>
</div>
<h2 id="mcetoc_1gt0shkn76d"><strong>The basic idea</strong></h2>
<p>Let's say you take a trip to the zoo with a kid. The kid is excited to learn about the various animals in the zoo, their names, and how they look. </p>
<p><span data-preserver-spaces="true">The kid sees an </span><strong><span data-preserver-spaces="true">otter</span></strong><span data-preserver-spaces="true">. She asks you what that animal is called since the kid doesn't know yet that the animal she sees is called an otter. </span></p>
<figure class="n3VNCb pT0Scc KAlRDb align-center"><img loading="lazy"  style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal); outline: 3px solid rgba(var(--color-primary-rgb), 0.55)  !important;" role="" src="https://www.seattleaquarium.org/sites/default/files/images/_MG_5379-Edit%20web.jpg" alt="ENJOY THE AQUARIUM FROM HOME: JOIN US FOR SEA OTTER WEEK! | Seattle Aquarium" width="481" height="341" aria-label="" data-noaft="1" data-is-external-image="true"></figure><br><br></p>
<p><span data-preserver-spaces="true">Instead of telling the kid that the animal is an otter, </span><strong><span data-preserver-spaces="true">we show her a chart</span></strong><span data-preserver-spaces="true"> </span><strong><span data-preserver-spaces="true">detailing all the animals in the zoo</span></strong><span data-preserver-spaces="true">. </span></p>
<p><span data-preserver-spaces="true">Next to the image of each animal is the corresponding name of that animal. </span></p>
<p><span data-preserver-spaces="true">The kid compares all the animals in the chart with the one he sees in front of her, deduces that the animal she sees is an </span><strong><span data-preserver-spaces="true">otter</span></strong><span data-preserver-spaces="true">. </span></p>
<h3 id="mcetoc_1gt0shkn76e">Let's break down the process</h3>
<p><span data-preserver-spaces="true">Understanding how the kid deduces that the animal is an otter by looking at and comparing each image in the chart is vital to understand the motivation and inspiration behind one-shot and few-shot learning. </span></p>
<p><span data-preserver-spaces="true">Let's say the image the kid sees with her eye is the </span><strong><span data-preserver-spaces="true">query image</span></strong><span data-preserver-spaces="true">. </span></p>
<p><span data-preserver-spaces="true">So the </span><strong><span data-preserver-spaces="true">query image </span></strong><span data-preserver-spaces="true">is, </span></p>
<figure class="post__image align-center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/4/Capture-2.PNG" alt="" width="303" height="213" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/4/responsive/Capture-2-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/4/responsive/Capture-2-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/4/responsive/Capture-2-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/4/responsive/Capture-2-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/4/responsive/Capture-2-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/4/responsive/Capture-2-2xl.PNG 1600w"></figure>
<p><span data-preserver-spaces="true">The images in the chart are as follows; let's call the pictures of the chart the </span><strong><span data-preserver-spaces="true">support set</span></strong><span data-preserver-spaces="true">.</span></p>
<p><span data-preserver-spaces="true">So the</span><strong><span data-preserver-spaces="true"> support set</span></strong><span data-preserver-spaces="true"> is as follows,</span></p>
<figure class="post__image align-center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/4/Group-11.png" alt="" width="1845" height="576" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/4/responsive/Group-11-xs.png 300w ,https://thenlpstudent.github.io/media/posts/4/responsive/Group-11-sm.png 480w ,https://thenlpstudent.github.io/media/posts/4/responsive/Group-11-md.png 768w ,https://thenlpstudent.github.io/media/posts/4/responsive/Group-11-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/4/responsive/Group-11-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/4/responsive/Group-11-2xl.png 1600w"></figure>
<p><span data-preserver-spaces="true">The kid </span><strong><span data-preserver-spaces="true">compares</span></strong><span data-preserver-spaces="true"> the </span><strong><span data-preserver-spaces="true">query image with every image in the support set, </span></strong><span data-preserver-spaces="true">and finds out which image is the most similar to the query image. </span></p>
<p><span data-preserver-spaces="true">In her head, she might assign scores to each image in the support set, depending on how well the query image matches with the support set photos.</span></p>
<p><span data-preserver-spaces="true">Then she would pick the image most similar to the query image from the support set, and since the label is right beside the support set image, she knows what that animal is called. </span></p>
<p><span data-preserver-spaces="true">This is the very idea behind one-shot and few-shot learning. </span></p>
<h2 id="mcetoc_1gt0shkn76f">The Main Concepts</h2>
<p>In few shot learning, there are 3 main concepts that you should be aware of. </p>
<ol>
<li>The training set </li>
<li>The support set </li>
<li>The query </li>
</ol>
<h3 id="mcetoc_1gt2fu1u17">The Training set</h3>
<p>The training set contains labeled input elements that you use to train the model. For each class (e.g., Husky, Elephant), you have multiple sets of images or inputs so that the model can learn variations of the same input class, which makes the prediction accuracy much higher.<figure class="post__image"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/4/test2.PNG" alt="" width="1010" height="574" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/4/responsive/test2-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/4/responsive/test2-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/4/responsive/test2-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/4/responsive/test2-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/4/responsive/test2-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/4/responsive/test2-2xl.PNG 1600w"></figure>
<p>But compared with traditional classification, few-shot learning focuses on understanding the degree of similarity between two images or inputs.<br><br>By sampling two random samples from the training set, and labeling them either 1 or 0, depending on if the two samples are similar or different, the model learns to predict how similar those two images are.</p>
<figure class="post__image"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/4/sim-2.PNG" alt="" width="1356" height="845" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/4/responsive/sim-2-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/4/responsive/sim-2-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/4/responsive/sim-2-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/4/responsive/sim-2-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/4/responsive/sim-2-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/4/responsive/sim-2-2xl.PNG 1600w"></figure>
<p>The model tries to learn a similarity function by comparing two images at a time with the training set, whereas you can see in the above image when comparing two input images, the model can predict how <strong>similar two huskies are, giving it a score of 0.8</strong>, which means the model knows that the two images have the same context. <br><br>However, when the model is given two images that aren't similar in context <strong>(i.e., husky and elephant),</strong> the <strong>model gives it a similarity score of 0.2</strong>, which is low, meaning those two images aren't similar to one another. </p>
<h3 id="mcetoc_1gt2fvb3pg">The Support set</h3>
<p>The support set is used to help make a prediction.</p>
<p>The model that has learned a suitable similarity function can now compare the query image with every given image in the support set and assign scores,<strong> a higher score if the model thinks that the query image and the image from the support set are similar. </strong><br><br>Then once the model compares all given images in the support set, <strong>the prediction is whatever input class that has the highest similarity score with the query image. </strong></p>
<h3 id="mcetoc_1gt2fvb3ph"><figure class="post__image"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/4/ss.PNG" alt="" width="1243" height="213" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/4/responsive/ss-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/4/responsive/ss-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/4/responsive/ss-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/4/responsive/ss-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/4/responsive/ss-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/4/responsive/ss-2xl.PNG 1600w"></figure>The Query</h3>
<p>The query input is the input that we provide to the model to predict its class. <br><br>The <strong>query input is then compared with every sample in the support set,</strong> and via the similarity function, the model learned can use to determine the scores of how well the query matches with each sample in the support set. <br><br>The<strong> sample with the highest score is picked as the final prediction</strong>, and the name of the class of that selected sample is the final classification class. </p>
<figure class="post__image align-center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/4/qq.PNG" alt="" width="339" height="277" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/4/responsive/qq-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/4/responsive/qq-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/4/responsive/qq-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/4/responsive/qq-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/4/responsive/qq-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/4/responsive/qq-2xl.PNG 1600w"></figure>
<p>So in the following example, if we use the 'rabbit' image as the query image, then the query would have the highest similarity score with the 'rabit' sample image in the support set, so the final prediction would be 'rabbit.' </p>
<h2 id="mcetoc_1gt8e8muqea"><span data-preserver-spaces="true">Training Models to Detect Similarity Using Siamese Networks </span></h2>
<p>A commonly used machine learning neural net model that performs similarity prediction is a Siamese Network. </p>
<p>It's named because the architecture of the network resembles Siamese twins. <br><br>The below image depicts a Siamese network. </p>
<h3 id="mcetoc_1gt83aduh9f"><figure class="post__image"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/4/tttt.PNG" alt="" width="1785" height="839" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/4/responsive/tttt-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/4/responsive/tttt-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/4/responsive/tttt-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/4/responsive/tttt-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/4/responsive/tttt-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/4/responsive/tttt-2xl.PNG 1600w"></figure></h3>
<p>In the above diagram, the inputs are two rabbit images. The output of the network is a similarity score of 0.98, which means the network has predicted that those two images are from the same class and thus similar. </p>
<p>The inputs are first fed into a convolutional neural network, where the networks have the same weights and biases (shared weights &amp; biases). </p>
<p><strong>The output of the convolutional neural network is a vector representation of the input images</strong>, called h(image1) and h(image2), where image1 and image2 are inputs, and 'h' is the convolutional neural network function. </p>
<p>Once you have vector representations of the images, then you can use a distance metric, like <strong>Euclidean distance</strong>, to measure how far apart these two vectors lie on the projected plane. </p>
<p>The idea here is that if the two images are alike, then they are close to each other, but if the two images are far apart (e.g., an image of a rabbit and an image of a dog), then the vectors would be far away from each other. </p>
<p>Once we get the distance metric, we squash the scalar value via a sigmoid function to a range between 0 and 1. </p>
<p>A<strong> sigmoid function</strong> is as follows,</p>
<figure class="post__image"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/4/sigmod.PNG" alt="" width="288" height="108" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/4/responsive/sigmod-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/4/responsive/sigmod-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/4/responsive/sigmod-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/4/responsive/sigmod-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/4/responsive/sigmod-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/4/responsive/sigmod-2xl.PNG 1600w"></figure>
<p>The graph of the sigmoid function looks like this, </p>
<figure class="post__image"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/4/sigmoidgg-3.PNG" alt="" width="702" height="361" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/4/responsive/sigmoidgg-3-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/4/responsive/sigmoidgg-3-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/4/responsive/sigmoidgg-3-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/4/responsive/sigmoidgg-3-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/4/responsive/sigmoidgg-3-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/4/responsive/sigmoidgg-3-2xl.PNG 1600w"></figure>
<p>As you can see in the above graph, the distance metric, once plugged into the sigmoid function, would be squashed to a value between 0 and 1. </p>
<p>To train this type of network, we use a <strong>"contrastive loss"</strong> function</p>
<h3 id="mcetoc_1gt83aduh9g">The Contrastive Loss Function</h3>
<p><span data-preserver-spaces="true">Contrastive Loss is a metric-learning loss function introduced by Yann Le Cunn et al. in 2005 in the paper</span><strong><span data-preserver-spaces="true"> "Dimensionality Reduction by Learning an Invariant Mapping."</span></strong><span data-preserver-spaces="true"> </span></p>
<p><span data-preserver-spaces="true">A contrastive loss function is a perfect fit to train a Siamese network because the <strong>goal of a Siamese network isn't to classify an input but rather to differentiate between two given inputs. </strong></span></p>
<figure class="wp-image-19444 entered lazyloaded"><img loading="lazy"  src="https://b2633864.smushcdn.com/2633864/wp-content/uploads/2021/01/contrastive_loss_keras_constrastive_loss_function_updated.png?size=582x64&amp;lossy=1&amp;strip=1&amp;webp=1" sizes="(max-width: 582px) 100vw, 582px" srcset="https://b2633864.smushcdn.com/2633864/wp-content/uploads/2021/01/contrastive_loss_keras_constrastive_loss_function_updated.png?size=126x14&amp;lossy=1&amp;strip=1&amp;webp=1 126w, https://b2633864.smushcdn.com/2633864/wp-content/uploads/2021/01/contrastive_loss_keras_constrastive_loss_function_updated-300x33.png?lossy=1&amp;strip=1&amp;webp=1 300w, https://b2633864.smushcdn.com/2633864/wp-content/uploads/2021/01/contrastive_loss_keras_constrastive_loss_function_updated.png?size=378x42&amp;lossy=1&amp;strip=1&amp;webp=1 378w, https://b2633864.smushcdn.com/2633864/wp-content/uploads/2021/01/contrastive_loss_keras_constrastive_loss_function_updated.png?size=504x55&amp;lossy=1&amp;strip=1&amp;webp=1 504w, https://b2633864.smushcdn.com/2633864/wp-content/uploads/2021/01/contrastive_loss_keras_constrastive_loss_function_updated.png?lossy=1&amp;strip=1&amp;webp=1 582w" alt="" width="582" height="64" data-lazy-srcset="https://b2633864.smushcdn.com/2633864/wp-content/uploads/2021/01/contrastive_loss_keras_constrastive_loss_function_updated.png?size=126x14&amp;lossy=1&amp;strip=1&amp;webp=1 126w, https://b2633864.smushcdn.com/2633864/wp-content/uploads/2021/01/contrastive_loss_keras_constrastive_loss_function_updated-300x33.png?lossy=1&amp;strip=1&amp;webp=1 300w, https://b2633864.smushcdn.com/2633864/wp-content/uploads/2021/01/contrastive_loss_keras_constrastive_loss_function_updated.png?size=378x42&amp;lossy=1&amp;strip=1&amp;webp=1 378w, https://b2633864.smushcdn.com/2633864/wp-content/uploads/2021/01/contrastive_loss_keras_constrastive_loss_function_updated.png?size=504x55&amp;lossy=1&amp;strip=1&amp;webp=1 504w, https://b2633864.smushcdn.com/2633864/wp-content/uploads/2021/01/contrastive_loss_keras_constrastive_loss_function_updated.png?lossy=1&amp;strip=1&amp;webp=1 582w" data-lazy-sizes="(max-width: 582px) 100vw, 582px" data-lazy-src="https://b2633864.smushcdn.com/2633864/wp-content/uploads/2021/01/contrastive_loss_keras_constrastive_loss_function_updated.png?size=582x64&amp;lossy=1&amp;strip=1&amp;webp=1" data-ll-status="loaded" data-is-external-image="true"></figure>
<p><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">The above is the formulae that describe contrastive loss. </span></p>
<p><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">We can break this formula down into smaller chunks so that we can easily understand what's going on. <br></span></p>
<p><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">The formulae are of two parts, as shown in the below diagram.  </span></p>
<figure class="post__image"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/4/ttttt.PNG" alt="" width="786" height="188" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/4/responsive/ttttt-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/4/responsive/ttttt-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/4/responsive/ttttt-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/4/responsive/ttttt-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/4/responsive/ttttt-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/4/responsive/ttttt-2xl.PNG 1600w"></figure>
<p><span data-preserver-spaces="true">Notice that </span><strong><span data-preserver-spaces="true">Y</span></strong><span data-preserver-spaces="true"> is the desired outcome of the network.</span></p>
<p><span data-preserver-spaces="true">If the two inputs are similar, we expect the network to output a value close to one or one. </span></p>
<p><span data-preserver-spaces="true">If the two inputs are different, we expect the network to output a value that's closer to zero or zero. </span></p>
<p><span data-preserver-spaces="true">When </span><strong><span data-preserver-spaces="true">Y </span></strong><span data-preserver-spaces="true">is equal to one, then the green part of the formulae can be ignored, as it cancels to zero. (1 - 1) = 0. </span></p>
<p><span data-preserver-spaces="true">When </span><strong><span data-preserver-spaces="true">Y</span></strong><span data-preserver-spaces="true"> is equal to zero, then the red part of the formulae can be ignored as it cancels to zero (0*D^2 = 0). </span></p>
<p><span data-preserver-spaces="true">So depending on the context, the contrastive loss function changes to evaluate how well the network performed. </span></p>
<p><span data-preserver-spaces="true">If we plot the red part of the formulae on a graph, we get a normal quadratic graph. </span></p>
<figure class="post__image align-center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/4/gra1.PNG" alt="" width="158" height="114" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/4/responsive/gra1-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/4/responsive/gra1-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/4/responsive/gra1-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/4/responsive/gra1-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/4/responsive/gra1-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/4/responsive/gra1-2xl.PNG 1600w"></figure>
<figure class="post__image"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/4/grr.PNG" alt="" width="473" height="229" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/4/responsive/grr-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/4/responsive/grr-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/4/responsive/grr-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/4/responsive/grr-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/4/responsive/grr-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/4/responsive/grr-2xl.PNG 1600w"></figure>
<p>Notice that the <strong>minimum </strong>of this graph is when <strong>D (or x) is zero. </strong></p>
<p><strong>This means if the inputs are similar, to get the minimum loss, the distance (D) between the two embedded vectors (vector representations of the inputs) must be close to one another, or the distance between those two vectors must be almost zero. </strong><br><br>However, when you plot the green part of the contrastive loss formulae, you get the following graph. </p>
<figure class="post__image"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/4/ggg22.PNG" alt="" width="1026" height="170" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/4/responsive/ggg22-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/4/responsive/ggg22-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/4/responsive/ggg22-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/4/responsive/ggg22-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/4/responsive/ggg22-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/4/responsive/ggg22-2xl.PNG 1600w"></figure>
<h3 id="mcetoc_1gt83aduh9h"><figure class="post__image"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/4/ggrr.PNG" alt="" width="348" height="230" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/4/responsive/ggrr-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/4/responsive/ggrr-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/4/responsive/ggrr-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/4/responsive/ggrr-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/4/responsive/ggrr-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/4/responsive/ggrr-2xl.PNG 1600w"></figure></h3>
<p>This part of the formulae is a max function.</p>
<p>It is the maximum between either a difference between a margin (M) minus the distance between the two vector representations of the input or zero, squared. </p>
<p>It means that if the distance between the two vectors is greater than a predefined margin, that means the network performed well, and the loss is zero.</p>
<p>However, if the two vectors are close-by, then the network performs poorly. <br>Because if you have two different inputs, the idea here is that your network must try to differentiate them, and say that they are different, so the vectors need to be as far away from each other as possible. </p>
<p>The <strong>margin (M) is a hyperparameter of the Siamese network. </strong></p>
<p>Using this loss function, you can now use backpropagation to find the gradients of the weights and biases in the network, and using gradient descent, the network can be trained so that the network converges to minimize the contrastive loss function. </p>
<h3 id="mcetoc_1gt2g3pla1a">Triplet Loss </h3>
<div class="flex flex-grow flex-col gap-3">
<div class="min-h-[20px] flex flex-col items-start gap-4 whitespace-pre-wrap">
<div class="markdown prose w-full break-words dark:prose-invert light">
<p>The idea behind triplet loss is to train a neural network to learn good representations of input data such that similar inputs are mapped close to each other and dissimilar inputs are mapped far apart from each other in a learned feature space. </p>
<p>The triplet loss function takes in three inputs: an <strong>anchor input</strong>, a <strong>positive input</strong>, and a <strong>negative input</strong>.</p>
<p>The anchor and positive inputs are samples from the same class, while the negative input is a sample from a different class.</p>
<p>The<strong> goal of triplet loss is to minimize the distance between the anchor and positive inputs while maximizing the distance between the anchor and negative inputs.</strong></p>
<p>This encourages the network to learn features that can distinguish between similar and dissimilar inputs.</p>
<figure class="post__image"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/4/anchor2-2.PNG" alt="" width="1509" height="815" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/4/responsive/anchor2-2-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/4/responsive/anchor2-2-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/4/responsive/anchor2-2-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/4/responsive/anchor2-2-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/4/responsive/anchor2-2-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/4/responsive/anchor2-2-2xl.PNG 1600w"></figure>
<p>Formally, let \(X_a\), \(X_p\), and \(X_n\) be an <strong>anchor, positive, and negative sample</strong>, respectively.</p>
<p>Let \(d(X_a, X_p)\) be the Euclidean distance between the anchor and positive samples and \(d(X_a, X_n)\) be the Euclidean distance between the anchor and negative samples.</p>
<p>The triplet loss can be defined as:</p>
<p>\( L = max(0, d(X_a, X_p) - d(X_a, X_n) + margin) \)</p>
<p>where<strong> margin is a hyperparameter that specifies the minimum desired difference between the distances</strong>.</p>
<p>The loss is zero when the distance between the anchor and negative samples is greater than the distance between the anchor and positive samples plus the margin.</p>
<p>Otherwise, the loss is positive and the network is encouraged to update its parameters to increase the margin.</p>
<p>When you plot the loss function, it looks like this, </p>
<figure class="post__image"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/4/graph3d.PNG" alt="" width="691" height="606" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/4/responsive/graph3d-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/4/responsive/graph3d-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/4/responsive/graph3d-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/4/responsive/graph3d-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/4/responsive/graph3d-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/4/responsive/graph3d-2xl.PNG 1600w"></figure>
<p>Notice in the above graph \( x = d(X_a, X_p) \) and \( y = d(X_a, X_n) \) respectively. </p>
<p>When the distance between the positive sample and the anchor decrease, and the distance between the negative sample and the anchor increase, the loss is flat, meaning its minimum.</p>
<p>However, if the opposite happens, then the loss increases. </p>
<p>This is the intuition behind triplet loss.</p>
<p> </p>
<p>I hope you enjoyed reading this article on few-shot classifiers.</p>
<p>In a future article, I will be diving much deeper into this topic of research in the field of Machine Learning.<br><br>Thanks for reading!</p>
<h2 id="mcetoc_1gt85s9nlbd">References </h2>
<ol>
<li>Shushen Wang's lecture series on Few Shot Learning Part 1 and 2. 
<ol>
<li><a href="https://www.youtube.com/watch?v=hE7eGew4eeg&amp;t=598s">https://www.youtube.com/watch?v=hE7eGew4eeg</a></li>
<li><a href="https://www.youtube.com/watch?v=4S-XDefSjTM">https://www.youtube.com/watch?v=4S-XDefSjTM</a></li>
</ol>
</li>
<li><span dir="ltr" role="presentation">Dimensionality Reduction by Learning an Invariant Mapping - Yann LeCun et al, (<a href="http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf">http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf</a>)</span></li>
</ol>
<p> </p>
</div>
</div>
</div>
<h2 id="mcetoc_1gt2g9g7h4e"></h2>
<h3 id="mcetoc_1gt2g9g7h4f"></h3>
            ]]>
        </content>
    </entry>
    <entry>
        <title>The Multi-Armed Bandit Problem </title>
        <author>
            <name>The NLP Student</name>
        </author>
        <link href="https://thenlpstudent.github.io/the-multi-armed-bandits-problem.html"/>
        <id>https://thenlpstudent.github.io/the-multi-armed-bandits-problem.html</id>

        <updated>2021-10-27T08:04:03+05:30</updated>
            <summary>
                <![CDATA[
                    In solving the problem of exploration vs. exploitation in reinforcement learning, we use bandit problems to understand and apply algorithms that balance RL agents' exploration and exploitation behaviors. What exactly is the multi-armed bandit problem, and how can you solve this type of problem in&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>In solving the problem of exploration vs. exploitation in reinforcement learning, we use bandit problems to understand and apply algorithms that balance RL agents' exploration and exploitation behaviors. <br><br>What exactly is the multi-armed bandit problem, and how can you solve this type of problem in RL? <br><br>This blog post focuses on doing just that. </p>
<div class="post__toc">
<h3>Table of Contents</h3>
<ul>
<li><a href="#mcetoc_1fimqv2jdp4">What is the Multi Armed Bandit Problem? </a>
<ul>
<li><a href="#mcetoc_1fimqv2jdp5">Stationary vs Non Stationary Bandit Problem </a></li>
</ul>
</li>
<li><a href="#mcetoc_1fimqv2jdp6">Solve by taking estimates </a>
<ul>
<li><a href="#mcetoc_1fimqv2jdp7">Incremental Updates</a></li>
<li><a href="#mcetoc_1fimt3ol2qu">Solving non-stationary bandit problems </a></li>
</ul>
</li>
<li><a href="#mcetoc_1fivpg6e31e">Pseudocode, Code, and Results </a></li>
<li><a href="#mcetoc_1fimqv2jdp9">References </a></li>
</ul>
</div>
<h2 id="mcetoc_1fimqv2jdp4">What is the Multi Armed Bandit Problem? </h2>
<p>Consider the following situation that you find yourself in. <br>You are faced repeatedly with 'k' different actions to take each time. <br>After each action that you take at a given time, you receive a numerical reward.<br><br>This reward is picked out from a stationary probability distribution that depends on your selected action. <br><br>Your goal in this problem is to maximize the total reward you receive over a period of time by selecting the best possible action at each time so that you maximize your reward. <br><br>In this problem, each action has an average reward. This average reward is the value of the action, or how good that action is. <br><br>If you knew the value of each action, then solving this problem would be very simple. Just pick the action with the highest value to get the biggest reward possible at the end. <br><br>However, you don't know the value of each of these actions. Therefore, you would need to explore and come up with estimates on the value of each action by taking random actions in the first few timesteps. <br><br>Once you have good guesses after the first few timesteps, then you are presented with a choice. <br><br>Either you can exploit, which means pick the action with the largest value estimate or continue to explore and re-adjust your current estimates of the values of your actions. </p>
<h3 id="mcetoc_1fimqv2jdp5">Stationary vs Non Stationary Bandit Problem </h3>
<p>The rewards for the actions you or the reinforcement learning (RL) agent takes at the current timestep are sampled from a probability distribution. E.g., Gaussian Distribution. <br><br>In a stationary Bandit problem, the probability distribution doesn't change at each timestep, which means that the true value of each action is constant across all timesteps. However, in a non-stationary bandit problem, the probability distribution changes at every timestep. <br><br>Therefore the true value of each action at each timestep is quite different from other timesteps. <br><br>However, we can solve both variations of such bandit problems by estimating the value of each action, continuing to re-estimate them by trying different actions (exploration), or picking the highest calculated value action you currently have (exploitation) </p>
<h2 id="mcetoc_1fimqv2jdp6">Solve by taking estimates </h2>
<p>One way to solve the Multi-Armed Bandit problem is through estimating the true value of each action.</p>
<p>Solving by estimating the true value of each action can either be applied to stationary and non-stationary bandit problems. </p>
<p>Firstly we need a few mathematical notations to formalize the bandit problem before finding a way to solve it.</p>
<ul>
<li>\( A_t \)  The action the agent picks at time \(t\)</li>
<li>\( R_t \)    The reward the agent earns at time \(t\)</li>
<li>\( q_*(a) \)   The true value of action \(a\)</li>
<li>\( Q_t(a) \)   The estimated value of action \(a\) at timestep \(t\) </li>
</ul>
<p>After exploration, we would want our RL agent to find closer estimates to the true value of action 'a'. </p>
<p>\[ Q_t(a) \approx q_*(a) \]</p>
<p>To estimate the value of each action, we keep track of the total sum of rewards collected when we took a specific action 'a' and the number of times we took that action 'a'.</p>
<p>By dividing as follows, we get an average estimate of how good that action is at the current timestep. </p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/3/captur1.PNG" alt="" width="818" height="94" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/3/responsive/captur1-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/3/responsive/captur1-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/3/responsive/captur1-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/3/responsive/captur1-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/3/responsive/captur1-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/3/responsive/captur1-2xl.PNG 1600w"></figure>
<p>The below shows our definition for how the agent picks the suitable action at timestep t. <br><br>This equation is the greedy approach (exploitation) in which the agent selects the highest estimated action value as its next action. <br><br>The agent picks the action with the highest estimate, hoping that it would bring the highest possible reward for timestep 't'. </p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/3/captur2.PNG" alt="" width="258" height="67" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/3/responsive/captur2-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/3/responsive/captur2-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/3/responsive/captur2-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/3/responsive/captur2-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/3/responsive/captur2-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/3/responsive/captur2-2xl.PNG 1600w"></figure>
<h3 id="mcetoc_1fimqv2jdp7">Incremental Updates</h3>
<p>To keep things simple let us assume the following change in notation, <br>  </p>
<p>Since the above is computationally costly, we can incrementally compute the mean by re-arranging the mean calculation using a little bit of algebra. </p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/3/incremental-2.PNG" alt="" width="564" height="474" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/3/responsive/incremental-2-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/3/responsive/incremental-2-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/3/responsive/incremental-2-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/3/responsive/incremental-2-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/3/responsive/incremental-2-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/3/responsive/incremental-2-2xl.PNG 1600w"></figure>
<p>This mean update has the following form used throughout in reinforcement learning for problems other than bandits. <br><br>The new estimate of the value of an action is the old estimate moved in the direction of the error between the current reward seen (i.e., 'target') and the old estimate. </p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/3/updaterule.PNG" alt="" width="822" height="67" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/3/responsive/updaterule-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/3/responsive/updaterule-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/3/responsive/updaterule-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/3/responsive/updaterule-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/3/responsive/updaterule-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/3/responsive/updaterule-2xl.PNG 1600w"></figure>
<h3 id="mcetoc_1fimt3ol2qu">Solving non-stationary bandit problems </h3>
<p><span data-preserver-spaces="true">When the optimal value changes from timestep to timestep, then you have a non-stationary bandit problem.</span></p>
<p><span data-preserver-spaces="true">The probability distribution in which rewards are sampled from the agent for the action it picks comes from a different probability distribution randomly at every timestep.</span></p>
<p><span data-preserver-spaces="true">Therefore the true value of each action is different at each timestep. </span></p>
<p><span data-preserver-spaces="true">Therefore we cannot rely on average sampling.</span></p>
<p><span data-preserver-spaces="true">We need a mechanism in which we give more weight to rewards that we see recently instead of values we saw in the past and move the estimates of our action values towards recently seen reward signals. </span></p>
<p><span data-preserver-spaces="true">This is done using a fixed step size parameter \( \alpha \), a </span><strong><span data-preserver-spaces="true">constant</span></strong><span data-preserver-spaces="true"> </span><strong><span data-preserver-spaces="true">parameter</span></strong><span data-preserver-spaces="true"> that lies between</span><strong><span data-preserver-spaces="true"> 0 and 1. </span></strong></p>
<p>$$\begin{aligned} Q_{n+1} &amp;= Q_n + \alpha[R_n + Q_n] \\ &amp;= \alpha R_n + (1 - \alpha)Q_n \\ &amp;= \alpha R_n + (1 - \alpha)Q_n \\ &amp;= \alpha R_n + (1 - \alpha)[\alpha R_{n-1} + (1-\alpha)Q_{n-1}] \\ &amp;= \alpha R_n + (1 - \alpha)[\alpha R_{n-1} + (1-\alpha)Q_{n-1}] \\ &amp;= \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha)^2 \alpha R_{n-2} + .... + (1- \alpha)^nQ_1 \\ &amp;= (1- \alpha)^nQ_1 + \sum^{n}_{i=1}\alpha(1 - \alpha)^{n-i}R_i \end{aligned}$$</p>
<p>By having a constant step size, the action values are updated so that the estimates are moved in the direction of the recent reward, as they have a higher weight and the older reward signals have a lower influence in the direction of where the estimates would move. </p>
<p>This makes the RL agents get more good action value estimates in non-stationary bandit problems. </p>
<h2 id="mcetoc_1fivpg6e31e"><span data-preserver-spaces="true">Pseudocode, Code, and Results </span></h2>
<p>The following code snippet shows the pseudocode implementation of solving the bandit problem by estimating the value of each action. </p>
<figure class="post__image"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/3/carbon7-2.png" alt="" width="1806" height="1076" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/3/responsive/carbon7-2-xs.png 300w ,https://thenlpstudent.github.io/media/posts/3/responsive/carbon7-2-sm.png 480w ,https://thenlpstudent.github.io/media/posts/3/responsive/carbon7-2-md.png 768w ,https://thenlpstudent.github.io/media/posts/3/responsive/carbon7-2-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/3/responsive/carbon7-2-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/3/responsive/carbon7-2-2xl.png 1600w"></figure>
<p>The following code snippet is the complete implementation of solving a ten-armed bandit problem. At each timestep, the agent can pick any ten actions and get a reward based on a gaussian distribution. <br><br>This distribution doesn't change over timesteps. Therefore, this is a stationary bandit problem. </p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/3/maincode1-2.png" alt="" width="2354" height="3068" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/3/responsive/maincode1-2-xs.png 300w ,https://thenlpstudent.github.io/media/posts/3/responsive/maincode1-2-sm.png 480w ,https://thenlpstudent.github.io/media/posts/3/responsive/maincode1-2-md.png 768w ,https://thenlpstudent.github.io/media/posts/3/responsive/maincode1-2-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/3/responsive/maincode1-2-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/3/responsive/maincode1-2-2xl.png 1600w"></figure>
<p>The following code snippet is modified slightly to show how we can solve a non-stationary bandit problem.<br><br>We can solve the problem using a step-size constant instead of counting how many times action 'a' was chosen before timestep t. </p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/3/stepsize.png" alt="" width="2354" height="1278" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/3/responsive/stepsize-xs.png 300w ,https://thenlpstudent.github.io/media/posts/3/responsive/stepsize-sm.png 480w ,https://thenlpstudent.github.io/media/posts/3/responsive/stepsize-md.png 768w ,https://thenlpstudent.github.io/media/posts/3/responsive/stepsize-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/3/responsive/stepsize-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/3/responsive/stepsize-2xl.png 1600w"></figure>
<p>The average reward at each timestep is stored in a list for each timestep.</p>
<p>For example, the following graph shows how the epsilon parameter influences the epsilon-greedy algorithm to obtain high amounts of rewards over 10,000 timesteps. </p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/3/f22.png" alt="" width="640" height="480" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/3/responsive/f22-xs.png 300w ,https://thenlpstudent.github.io/media/posts/3/responsive/f22-sm.png 480w ,https://thenlpstudent.github.io/media/posts/3/responsive/f22-md.png 768w ,https://thenlpstudent.github.io/media/posts/3/responsive/f22-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/3/responsive/f22-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/3/responsive/f22-2xl.png 1600w"></figure>
<p>The graph below shows how average sampling methods with a variable step size perform compared to fixed step size sampling methods for non-stationary bandit problems. </p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/3/f2.png" alt="" width="640" height="480" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/3/responsive/f2-xs.png 300w ,https://thenlpstudent.github.io/media/posts/3/responsive/f2-sm.png 480w ,https://thenlpstudent.github.io/media/posts/3/responsive/f2-md.png 768w ,https://thenlpstudent.github.io/media/posts/3/responsive/f2-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/3/responsive/f2-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/3/responsive/f2-2xl.png 1600w"></figure>
<p>Thanks for reading this article this far! </p>
<p>Follow me on Twitter <a href="https://twitter.com/intent/user?screen_name=studentnlp" target="_blank" rel="noopener noreferrer">@studentnlp</a> so you won't miss my latest blog post, and as always, happy hacking!</p>
<h2 id="mcetoc_1fimqv2jdp9">References </h2>
<p><span class="ILfuVd"><span class="hgKElc">Sutton, R.S. &amp; Barto, A.G., 2018. Reinforcement learning: An introduction, MIT press.</span></span></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Basics of Reinforcement Learning: Part I</title>
        <author>
            <name>The NLP Student</name>
        </author>
        <link href="https://thenlpstudent.github.io/introduction-to-reinforcement-learning.html"/>
        <id>https://thenlpstudent.github.io/introduction-to-reinforcement-learning.html</id>

        <updated>2021-10-21T07:51:32+05:30</updated>
            <summary>
                <![CDATA[
                    Welcome! In this article, we will be going over the fundamentals of reinforcement learning. We will discuss what reinforcement learning is, how agents learn from rewards and experiences, what policies and value functions are, and how to model environments in a Markov decision process step-by-step.
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>Welcome! In this article, we will be going over the fundamentals of reinforcement learning. We will discuss what reinforcement learning is, how agents learn from rewards and experiences, what policies and value functions are, and how to model environments in a Markov decision process step-by-step.</p>
<div class="post__toc">
<h3>Table of Contents</h3>
<ul>
<li><a href="#mcetoc_1fickllfsg7">What is Reinforcement Learning (RL)? </a>
<ul>
<li><a href="#mcetoc_1fickllfsg8">What is a reward? </a></li>
<li><a href="#mcetoc_1fickllfsg9">The Reward Hypothesis </a></li>
</ul>
</li>
<li><a href="#mcetoc_1fickllfsga">Components in Reinfocement learning</a>
<ul>
<li><a href="#mcetoc_1fickllfsgb">The Agent and the Enviroment  </a></li>
<li><a href="#mcetoc_1fickllfsgc">What is the Markov State?</a></li>
<li><a href="#mcetoc_1ficm0ep4ij">Returns, Epsoides and Continuous Tasks  </a></li>
<li><a href="#mcetoc_1fie1hfh3oq">What are value functions and policies?  </a></li>
</ul>
</li>
<li><a href="#mcetoc_1fickqocai8">The Markov Decision Process (MDP)</a>
<ul>
<li><a href="#mcetoc_1fickllfsgf">The Value Function </a></li>
<li><a href="#mcetoc_1fickllfsgg">The Action-Value Function</a></li>
</ul>
</li>
<li><a href="#mcetoc_1fickllfsgk">Solving the MDP</a>
<ul>
<li><a href="#mcetoc_1fickqocaib">The optimal value and action-value function</a></li>
</ul>
</li>
<li><a href="#mcetoc_1fif6had5p3">Grid world Example </a></li>
<li><a href="#mcetoc_1fie5fapjov">References </a></li>
</ul>
</div>
<h2 id="mcetoc_1fickllfsg7">What is Reinforcement Learning (RL)? </h2>
<p>Reinforcement learning is a learning process in which agents learn through interacting with the environment and learn to take actions in a way that maximizes its reward signal. The reward signal is given by the environment depending on how well the agent performed in that environment. <br><br>Reinforcement learning is a computational approach that learns from interaction with an environment. The goal of a reinforcement learning agent is to maximize a numerical reward signal. </p>
<h3 id="mcetoc_1fickllfsg8">What is a reward? </h3>
<p>A reward is a simple number given to the agent by the environment each time the agent interacts with the environment. The agent's goal is to maximize then the total amount of reward it receives, which means that the agent needs to maximize the cumulative reward in the long term, not the immediate reward in the short term. </p>
<h3 id="mcetoc_1fickllfsg9">The Reward Hypothesis </h3>
<p>The reward hypothesis states that an RL agent's goal is to maximize the expected value of the received scalar signal's cumulative reward. <br><br>Therefore, reinforcement learning gets based on learning methods that maximize the total cumulative reward by taking actions that help reach this goal.</p>
<h2 id="mcetoc_1fickllfsga">Components in Reinfocement learning</h2>
<p>The two main components in any reinforcement learning-based algorithm are known as the agent and the environment.</p>
<h3 id="mcetoc_1fickllfsgb">The Agent and the Enviroment  </h3>
<p>An agent is a learner and the decision-maker inside an environment. The agent observes the environment and takes action, which is how the agent interacts with the environment. The environment then gives a reward to that agent based on the agent's action at that particular time and situation. <br><br>Everything that is outside of the agent is known as the environment.</p>
<h3 id="mcetoc_1fickllfsgc">What is the Markov State?</h3>
<p>In RL, understanding the concept of a state is vital. The state is the information that we use to determine what might happen next. Simply put, the state is a function of history. <br><br>The agent has its state, in which case the agent's state is the function of the agent's history and experiences collected by interacting with the environment.</p>
<p>The environment also has its state, known as the environment state. <br><br>The agent's or the environment's state is a <strong>Markov state</strong> if the current state is independent of the past. This is because the past information (history) gets derived from the current state. Therefore the state doesn't depend on history. <br><br><strong>"The future is independent of the past given the present"</strong> - and future states can be computed from the current state, and the history can be discarded. </p>
<p>Suppose the agent can access the environment state. In that case, the mathematical model of the RL problem is known as a fully observable Markov decision process, whereas if the agent cannot access the state of the environment, then it's known as a partially observable Markov decision process. </p>
<h3 id="mcetoc_1ficm0ep4ij">Returns, Epsoides and Continuous Tasks  </h3>
<p>The agent's goal is to maximize the total cumulative reward it receives from the environment over time. <br><br>To understand more about the return, we formalize it as follows. This is the basic form of the expected return, which is just the sum of all rewards gathered till the agent reaches the terminal state of the environment (last state of interaction with the environment) from some timestep t. Capital T denotes the final timestep.</p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/2/returneq.PNG" alt="" width="375" height="61" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/2/responsive/returneq-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/returneq-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/returneq-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/returneq-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/returneq-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/returneq-2xl.PNG 1600w"></figure>
<p>Depending on the environment, the agent can work in episodic tasks or continuing tasks.</p>
<p><br>An episodic task ends at a specific terminal state S+ with different rewards for different outcomes. (All nonterminal states denoted as S.)</p>
<p>E.g., winning and losing a game of chess, the next episode or next game begins independently of how the previous game ended) The terminal state ends, and the environment resets back into a starting state. </p>
<p><br>Continous tasks don't have a terminal state. Therefore they go on without a limit. Here taking the total cumulative reward is tricky because the total cumulative reward will be T = ∞.  <br><br>Now we introduce a notation known as the discount factor, a value between 0 and 1. This is so that in continuous environments, the total cumulative reward would not be ∞. <br>If we set the discount factor closer to 0, the agent tends to try and optimize to get higher immediate rewards. On the other hand, if the discount factor is closer to 1, the agent looks ahead to long-term rewards.</p>
<p>Therefore the agent now tries to select actions that maximize the sum of the discounted rewards (discounted return) it receives from timestep t onwards. So at each state, the agent tries to pick the best action at timestep t that maximizes the expected discounted return:</p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/2/returnmodified.PNG" alt="" width="522" height="66" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/2/responsive/returnmodified-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/returnmodified-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/returnmodified-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/returnmodified-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/returnmodified-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/returnmodified-2xl.PNG 1600w"></figure>
<p>In the following manner, the current expected discounted return at timestep t relates with the future discounted return at time step t+1. <br><br>Thus, the current expected discounted return is the immediate reward at timestep t plus the discounted expected return at time step t+1.</p>
<figure class="post__image post__image--center"><img loading="lazy"  style="outline: 3px solid rgba(var(--primary-color-rgb), 0.55)  !important;" src="https://thenlpstudent.github.io/media/posts/2/returnwhole.PNG" alt="" width="408" height="100" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/2/responsive/returnwhole-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/returnwhole-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/returnwhole-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/returnwhole-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/returnwhole-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/returnwhole-2xl.PNG 1600w"></figure>
<h3 id="mcetoc_1fie1hfh3oq">What are value functions and policies?  </h3>
<p>Two other essential concepts in Reinforcement Learning are the value function and the policy function of an RL agent. <br><br>The value function of an RL agent describes how 'good' a state is in the long run.<br><br>It's a function that maps a state to a value that describes how much total expected discounted return the agent would get if the agent takes actions starting from that given state.<br><br>The policy is a function that maps a state with a given action. <br><br>Next, it describes how the behavior of the agent would be in the environment. Finally, it defines a way in which the agent would interact with the environment.<br>The policy function can either be discrete or stochastic, in the sense that it can map to certain actions based on an argmax function, where it picks the action that has the highest return (a greedy policy), or picks an action based on a probabilistic distribution.</p>
<h2 id="mcetoc_1fickqocai8">The Markov Decision Process (MDP)</h2>
<p>The Markov Decision Process is a classic formalization of sequential decision making, where actions influence the immediate reward at time step t and future states those actions and the rewards at those future states. <br><br>The MDP process formalizes and helps us derive equations on dealing with the trade-off between immediate and delayed rewards. </p>
<p>There are two types of values that we estimate by using an MDP, so that we can better deal with the immediate and delayed reward trade-off. <br><br>We estimate the optimal value function that gives each state S's best possible value given optimal action selections. <br><br>We also estimate the optimal action-value function, which takes in as arguments the action and the state the agent is in and gives how good of an action that action is depending on the state. </p>
<p>Estimating these state-dependent quantities is essential to accurately assign credit to the agent's actions in the environment to make decisions better to effectively trade-off between immediate and delayed rewards. <br><br>The agent would observe the current state at time step t and get the current reward from transitioning into that state. Then, the state would take action, which hopefully maximizes the discounted return for the next timestep. </p>
<p>The environment would process this action if the environment would produce the next state and reward the agent for moving into the new state. </p>
<p><br>This is how the agent and the environment interact with each other. The agent's goal is to behave optimally in this environment, which is done by maximizing the total discounted return over time. </p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/2/Group-5-2.png" alt="" width="556" height="241" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/2/responsive/Group-5-2-xs.png 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/Group-5-2-sm.png 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/Group-5-2-md.png 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/Group-5-2-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/Group-5-2-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/Group-5-2-2xl.png 1600w"></figure>
<p>While interacting with the environment, this would be the sequential history the agent goes through.</p>
<p>For example, the agent starts at timestep 0, observes the state, takes action, and obtains a reward, and this cycle repeats. </p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/2/bellman_1.PNG" alt="" width="433" height="49" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/2/responsive/bellman_1-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_1-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_1-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_1-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_1-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_1-2xl.PNG 1600w"></figure>
<p>The MDP describes the probability in which the agent transitions from one state to the next.</p>
<p>The likelihood that given the agent is in a given state 's' and takes action' a', the likelihood that that agent would end up in a new state " s' " and gets a reward 'r' is described as follows, </p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/2/bellman_2.PNG" alt="" width="588" height="56" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/2/responsive/bellman_2-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_2-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_2-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_2-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_2-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_2-2xl.PNG 1600w"></figure>
<figure class="post__image post__image--center">The probability in taking action 'a' from state 's' and transitioning to state " s' " is the summation for all rewards in set R (all possible reward values in the MDP), and their transition probability of starting at state 's', taking action 'a', transitioning to state " s' " and getting probability 'r' for all rewards in set R.<br><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/2/bellman_5.PNG" alt="" width="710" height="103" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/2/responsive/bellman_5-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_5-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_5-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_5-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_5-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_5-2xl.PNG 1600w"></figure>
<p>The expected reward for choosing action a from state s can be described as the summation of all rewards in R weighted over all the state transition probabilities from state 's', taking action 'a', landing in the state " s' " and getting a reward 'r' afterward. </p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/2/bellman_6.PNG" alt="" width="670" height="77" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/2/responsive/bellman_6-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_6-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_6-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_6-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_6-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_6-2xl.PNG 1600w"></figure>
<p>The expected reward the agent will receive at the current state " s' " given that the previous state and action was 's' and 'a', is the summation of all rewards R, weighted by the total probability of transitioning from state 's' to " s' " by taking action 'a' over the probability of transitioning from state 's' to " s' " by taking action 'a' and getting a specific reward 'r' in R.  </p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/2/bellman_7.PNG" alt="" width="737" height="78" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/2/responsive/bellman_7-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_7-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_7-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_7-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_7-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_7-2xl.PNG 1600w"></figure>
<p>Here it shows that the probability of transitioning from all states " s' " and getting all rewards 'r' from those states given that the agent was in state 's' and takes action 'a' sums to 1. This shows that the function 'p' specifies a probability distribution for each choice of state 's' and action 'a'. </p>
<figure class="post__image post__image--center"><img loading="lazy"  style="font-size: 18.4px; outline: 3px solid rgba(var(--primary-color-rgb), 0.55)  !important;" src="https://thenlpstudent.github.io/media/posts/2/bellamn_3.PNG" alt="" width="521" height="81" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/2/responsive/bellamn_3-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellamn_3-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellamn_3-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellamn_3-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellamn_3-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellamn_3-2xl.PNG 1600w"></figure>
<h3 id="mcetoc_1fickllfsgf">The Value Function </h3>
<p><span data-preserver-spaces="true">The value function defines how good it is to be in a particular state. </span></p>
<p><span data-preserver-spaces="true">This means it gives the total expected discounted reward that is possible to receive, given that the agent continues to take actions from that state onward. </span></p>
<p><span data-preserver-spaces="true">The value function depends upon the state, and the policy the agent follows since the selection of actions from state 's' onwards depends on the agent's policy or behavior. </span></p>
<p><span data-preserver-spaces="true">The below equation shows that the value function when the agent uses a given policy </span><strong><span data-preserver-spaces="true">π </span></strong><span data-preserver-spaces="true">and starts at state s, is equivalent by the MDP to the expectation of the total discounted return from timestep t onwards, given that the agent currently at timestep t is on the state 's'.</span></p>
<p><span data-preserver-spaces="true">Afterward, we can expand the definition of the total discounted return and show that this is true for all states in the MDP. </span></p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/2/v_policy_s.PNG" alt="" width="613" height="68" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s-2xl.PNG 1600w"></figure>
<p>We can also evaluate this expectation as follows by the recursive defintion of the total discounted return.</p>
<p>Here it shows that the value function is the summation of all actions that can be made by the agent, and the probability the agent will choose that action  is weighted by the sum the transitioning probabiltieis of all states and rewards from state 's' by taking action 'a' multiplied by the immediate reward of the given transitioning probabilities plus the discounted value of the value function under policy <strong>π </strong>for the next state " s' ".</p>
<p> </p>
<figure class="post__image post__image--center"><img loading="lazy"  style="font-size: 18.4px; outline: 3px solid rgba(var(--primary-color-rgb), 0.55)  !important;" src="https://thenlpstudent.github.io/media/posts/2/v_policy_s_s.PNG" alt="" width="567" height="173" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s_s-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s_s-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s_s-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s_s-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s_s-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s_s-2xl.PNG 1600w"></figure>
<div> </div>
<h3 id="mcetoc_1fickllfsgg">The Action-Value Function</h3>
<p>The action-value function maps a state and an action to show how much expected total discounted return the agent will receive from the environment if the agent starts at state 's' and takes action 'a' from that time until the environment terminates. </p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/2/q_policy_s_a.PNG" alt="" width="630" height="87" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/2/responsive/q_policy_s_a-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_policy_s_a-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_policy_s_a-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_policy_s_a-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_policy_s_a-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_policy_s_a-2xl.PNG 1600w"></figure>
<h2 id="mcetoc_1fickllfsgk">Solving the MDP</h2>
<p>Solving the MDP entails that we find the optimal value function or the optimal action-value function, which can be derived from the optimal value function in the MDP and allowing the agent to pick the best action that gives the highest possible action-value for the state that the agent is currently at timestep t. </p>
<h3 id="mcetoc_1fickqocaib">The optimal value and action-value function</h3>
<p>The optimal value function is the maximum value that can be obtained from the state s given under the policy, which enables the agent to choose such actions from that state onwards which maximizes the expected total discounted return from that state onwards. </p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/2/optimal_val_policy.PNG" alt="" width="230" height="47" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/2/responsive/optimal_val_policy-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/optimal_val_policy-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/optimal_val_policy-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/optimal_val_policy-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/optimal_val_policy-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/optimal_val_policy-2xl.PNG 1600w"></figure>
<p>The optimal action-value function is the maximum action value obtained from state 's' by taking action 'a' under the policy that enables the agent to choose such actions from that state and action onwards, which maximizes the expected total discounted return from that state, action pair onwards. </p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/2/optimal_q_value.PNG" alt="" width="245" height="42" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/2/responsive/optimal_q_value-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/optimal_q_value-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/optimal_q_value-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/optimal_q_value-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/optimal_q_value-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/optimal_q_value-2xl.PNG 1600w"></figure>
<p>The optimal action-value function can be written in terms of the optimal value function as the expectation of the immediate reward plus the discounted optimal value of the next state given that the current state is 's' and the action taken from that state is action 'a'. </p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/2/q_optimal_s_a_expe-2.PNG" alt="" width="417" height="35" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_s_a_expe-2-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_s_a_expe-2-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_s_a_expe-2-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_s_a_expe-2-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_s_a_expe-2-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_s_a_expe-2-2xl.PNG 1600w"></figure>
<p>The recurrence relation of the optimal value function is the value of the maximum action 'a' that can be taken which is a summation of all the states and rewards in the sets S and R of the MDP, weighted by the transition probability of moving to those states obtaining those rewards multiplied by the immediate reward plus the discounted optimal value of the next state the agent would move to. </p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/2/big_equ.PNG" alt="" width="447" height="208" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/2/responsive/big_equ-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/big_equ-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/big_equ-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/big_equ-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/big_equ-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/big_equ-2xl.PNG 1600w"></figure>
<p>The recurrence relation of the optimal action-value function is the sum of all states and rewards in S and R of the MDP, weighted by the transition probability multiplied by the immediate reward 'r' plus the discounted best action-value function of the next state the agent would move into by taking action " a' " from that state. </p>
<figure class="post__image post__image--center"><img loading="lazy"  src="https://thenlpstudent.github.io/media/posts/2/q_optimal_stup.PNG" alt="" width="523" height="109" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_stup-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_stup-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_stup-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_stup-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_stup-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_stup-2xl.PNG 1600w"></figure>
<h2 id="mcetoc_1fif6had5p3">Grid world Example </h2>
<p>In this example, the RL agent has to traverse the following grid world, and any action that doesn't involve traversing from A to A' or B to B' gives a reward of -1. Otherwise, a reward of +10 and +5 is given if the agent gets teleported from A to A' or B to B'.</p>
<p> <br>The agent can only take one step in north, east, south, and west. </p>
<p>While the agent updates its value function by interacting and moving in this grid world environment, the value function over time would update as shown in the below table.</p>
<p>It shows how much discounted total return each cell would give the agent over the long run. </p>
<figure class="post__image post__image--center" ><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/2/policyexample.PNG" alt="Reinforcement Learning: 2nd Edition (Finite Markov Decision Processes)" width="508" height="182" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/2/responsive/policyexample-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/policyexample-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/policyexample-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/policyexample-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/policyexample-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/policyexample-2xl.PNG 1600w">
<figcaption >Reinforcement Learning: 2nd Edition (Finite Markov Decision Processes)</figcaption>
</figure>
<p>Once the agent finds the optimal policy that makes the agent optimize on getting the best possible discounted returns from this grid world, the value function would converge to the optimal value function. </p>
<p><br>By acting greedily upon the optimal value function, we find the optimal policy function.</p>
<p>If the agent chooses actions that get to better states (cells) with a higher value than the other states accessible from the current state the agent is in, the agent has successfully found the best policy to act optimally in this grid world in the below diagram. </p>
<figure class="post__image post__image--center" ><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/2/gridworld2.PNG" alt="" width="779" height="270" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/2/responsive/gridworld2-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/gridworld2-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/gridworld2-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/gridworld2-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/gridworld2-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/gridworld2-2xl.PNG 1600w">
<figcaption >Reinforcement Learning: 2nd Edition (Finite Markov Decision Processes)</figcaption>
</figure>
<p><span data-preserver-spaces="true">That brings us to the end of the basics of reinforcement learning part 1.</span></p>
<p><span data-preserver-spaces="true">This would be a 3 part series. In the second post, we will be going over Multi-armed bandits, an interesting problem in Reinforcement Learning. In the 3rd and final post, we will be going over finding the optimal value function solving the MDP using Dynamic Programming. </span></p>
<p><span data-preserver-spaces="true">Stay tuned, and follow me on Twitter </span><a target="_blank" href="https://twitter.com/intent/user?screen_name=studentnlp" class="editor-rtfLink" rel="noopener"><span data-preserver-spaces="true">@studentnlp</span></a><span data-preserver-spaces="true">, so you don't miss any new posts. </span></p>
<p><span data-preserver-spaces="true">Cheers! </span></p>
<h2 id="mcetoc_1fie5fapjov">References </h2>
<ul>
<li><span class="ILfuVd"><span class="hgKElc">Sutton, R.S. &amp; Barto, A.G., 2018. Reinforcement learning: An introduction, MIT press.</span></span></li>
<li><a href="https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ" target="_blank" rel="noopener noreferrer">David Silver "Introduction to Reinforcement Learning" DeepMind 2015</a> </li>
</ul>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>The Transformer Explained</title>
        <author>
            <name>The NLP Student</name>
        </author>
        <link href="https://thenlpstudent.github.io/transformers-and-attention.html"/>
        <id>https://thenlpstudent.github.io/transformers-and-attention.html</id>

        <updated>2021-09-09T17:11:20+05:30</updated>
            <summary>
                <![CDATA[
                    Welcome! In this article, we will be going over what a Transformer is, the intuition and the inner workings behind the attention mechanism it employs to process sequential data, and how the Multi-Head Attention mechanism works as implemented by the paper 'Attention is all you&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                
  <p>
    Welcome! In this article, we will be going over what a <b>Transformer </b>is, the intuition and the inner workings behind the <b>attention mechanism</b> it employs to process sequential data, and how the <b>Multi-Head Attention mechanism</b> works as implemented by the paper '<b>Attention is all you need</b>' <b>NeurIPS 2017.&nbsp;</b>
  </p>

  <p>
    
  </p>

  <div class="post__toc">
    <h3>Quick Links&nbsp;</h3>
    <ul>
      <li><a href="#the-big-picture">The Big Picture</a></li><li><a href="#how-the-transformer-works">How the Transformer works</a><ul><li><a href="#step-1-word-and-positional-embeddingnbsp">Step 1: Word and Positional Embedding&nbsp;</a></li><li><a href="#step-2-encoding-sequences-using-attention">Step 2: Encoding Sequences using Attention</a><ul><li><a href="#what-is-attentionnbsp">What is attention?&nbsp;</a></li><li><a href="#what-is-the-add-and-norm-layernbsp">What is the 'add and norm' layer?&nbsp;</a></li><li><a href="#the-feedforward-layer">The feed-forward layer</a></li></ul></li><li><a href="#step-3-decoding-the-input-sequencesnbsp">Step 3: Decoding the input sequences&nbsp;</a></li><li><a href="#step-4-training-and-inferencenbsp">Step 4: Training and Inference&nbsp;</a></li></ul></li><li><a href="#understanding-attentionnbsp">Understanding Attention&nbsp;</a><ul><li><a href="#maskingnbsp">Masking&nbsp;</a></li><li><a href="#attention-in-the-decoder-layersnbsp">Attention in the decoder layers&nbsp;</a></li></ul></li><li><a href="#multihead-attention-cause-ngt1-heads-are-better-than-onenbsp">Multi-Head Attention: cause n&gt;1 heads are better than one!&nbsp;</a></li><li><a href="#referencesnbsp">References&nbsp;</a></li>
    </ul>
  </div>
  

  <p>
    A <b>transformer </b>is a deep learning architecture that performs well for sequential data-related Machine learning tasks. It is based around an encoder-decoder architecture to handle and process sequential data in parallel. In addition, it uses a mechanism known as Attention to look back and forwards the sequential input and identify long-range patterns and relationships between each component in the sequence. <br><br>The transformer architecture solves the shortcomings of recurrent neural networks and convolutional neural networks when processing sequential data and making good predictions.&nbsp;
  </p>

  <p>
    Recurrent neural networks aren't capable of processing sequential data in parallel. Therefore it increases training time since we can't utilize the full power of GPU processing units for parallel matrix multiplication if data is processed sequentially. <br><br>Convolutional neural networks can also process sequential data in parallel. But due to their window size, they can only look back and forward the input sequence, making them unable to identify good long-distance relationships between the entire sequence.&nbsp;
  </p>

  <p>
    The attention mechanism can look at the whole input sequence at once and identify the long-distance relationships of every element (i.e. word) in the sequence with every other element of the sequence, which is why transformers, in general, perform better when it comes to sequential data processing.&nbsp;
  </p>

    <h2 id="the-big-picture">
      The Big Picture
    </h2>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/transformers.png" height="713" width="818" alt="Transformer architecture, bird's eye view diagram"  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/transformers-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/transformers-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/transformers-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/transformers-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/transformers-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/transformers-2xl.png 1600w">
      <figcaption>High level overview of the transformer architecture and it's key components </figcaption>
    </figure>

  <p>
    <br>In a nutshell, the transformer uses an encoder-decoder architecture, where N (N=5 in the&nbsp; different encoding layers encode the input sequences, and N different decoder layers decode the information encoded by the encoding layers to predict the next word in sequence, depending on the NLP task at hand.&nbsp;&nbsp;
  </p>

  <p>
    Before sending the input sequences into the encoder layers or to the decoder layers of the transformer, word embeddings and positional embeddings preprocess the input sequences so that the transformer can process the data more intuitively and effectively.&nbsp;
  </p>

  <p>
    &nbsp;We will discuss the concept of word embedding and positional embedding once we dive more deeply into the inner workings of the transformer. 
  </p>

  <p>
    These encoder layers are stacked on top of each other. The last encoder layer creates an encoded embedding matrix that contains a representation of the learned sequence.
  </p>

  <p>
    Finally, the decoder layer utilizes this matrix for a certain natural language processing task such as text classification, text generation, or machine translation.&nbsp;
  </p>

  <p>
    The original paper (Attention is all you need) utilized the transformer to convert English input sentences to German and French, proving that transformers are good at machine translation tasks.&nbsp;
  </p>

  <p>
    So let's dive deep into the inner workings of the transformers' encoder and decoder layers, how Attention works, and how supervised learning can train the transformer to optimize to perform better at machine translation and similar text processing tasks.&nbsp;
  </p>

    <h2 id="how-the-transformer-works">
      How the Transformer works
    </h2>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/1-do7YDFF2sads0p9BnjzrWA-2.png" height="671" width="500" alt="Detailed diagram of the transformer"  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-2xl.png 1600w">
      <figcaption>Detailed diagram of the transformer, from the paper 'Attention is all you need' NIPS 2017</figcaption>
    </figure>

  <p>
    The paper that started it all by introducing the transformer at NIPS 2017 was called 'Attention is all you need', where Ashish Vaswani et al. proposed a deep learning model capable of processing sequences utilizing only Attention. Earlier variants of using Attention with sequence data involved connecting an attention layer to a recurrent neural network like a Long Short Term Memory (LSTM) network or a (Gated recurrent unit) GRU network.&nbsp;
  </p>

  <p>
    However, transformers proved that relying solely on the attention mechanism yielded better results, especially in the NLP problem area of language translation.&nbsp;
  </p>

  <p>
    Let's look at how the transformer processes information in a step by step manner and learns to make better predictions overtime.&nbsp;
  </p>

    <h3 id="step-1-word-and-positional-embeddingnbsp">
      Step 1: Word and Positional Embedding&nbsp;
    </h3>

  <p>
    The first step involves creating embeddings for the input sequences by word and positional embedding.&nbsp;
  </p>

  <p>
    Deep learning models can't process words the way humans do. They deal with vectors and matrices to make predictions. Therefore we must first convert words in the sequence into word vectors, representing a word in the vocabulary.&nbsp;
  </p>

  <p>
    Before the input sequences enter the encoders, the inputs are preprocessed and converted to word vectors, representing a word's contextual meaning in numeric form, so transformers can carry out numerical calculations to make predictions.&nbsp;
  </p>

  <p>
    When training a transformer, we have a set of vocabulary that we would like the transformer to learn. A vocabulary is a set of distinct words that you want the transformer to learn to do the NLP problem at hand effectively.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/1-4UHP_q1_FdqMr1T5yvXEKg.jpeg" height="650" width="485" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-xs.jpeg 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-sm.jpeg 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-md.jpeg 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-lg.jpeg 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-xl.jpeg 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-2xl.jpeg 1600w">
      <figcaption>A vocabulary list with distinct words with their corresponding indices </figcaption>
    </figure>

  <p>
    These word embeddings convert a given word to a word vector, which is a vector with a dimension of 'dᵐᵒᵈᵉˡ', which is a constant, or referred to as a hyperparameter of the network. The original paper had word vectors with the size of 512, which meant that to represent one word, the transformer converted each word into 512 numbers and stored them in vectors.&nbsp;
  </p>

  <p>
    Word vectors can also be projected onto a 2-D plane, by shrinking their dimensionality from N dimensions to 2 dimensions. Once projected, the words that have similar context are grouped near each other.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/3225.1569667846.png" height="922" width="1552" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-2xl.png 1600w">
      <figcaption>Word vectors projected onto a 2-D plane, words that share similar context are close by to each other</figcaption>
    </figure>

  <p>
    Therefore to process the input sequence, we get the input word tokens from the sequence. We map each word to the location of the word in the vocabulary, assuming that the word is in the vocabulary.&nbsp;
  </p>

  <p>
    For example, the word index in the vocabulary could be 21; 21 is the word ID. So for every word in the sequence, we map it to its word ID.&nbsp;
  </p>

  <p>
    The below diagram shows multiple sentences with words being represented as a 2 dimensional matrix.&nbsp;
  </p>

    <figure class="post__image post__image--wide">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/Capture.PNG" height="354" width="929" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/Capture-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture-2xl.PNG 1600w">
      <figcaption>Think of the sample size (row) as a sentence, and Sequence Length as the number of words in each sentence, a cell is a word</figcaption>
    </figure>

  <p>
    So our word tokens in the input sequences to the transformers are converted into word vectors, so the transformer knows the word's context when it processes it.
  </p>

    <figure class="post__image post__image--wide">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/Capture2.PNG" height="423" width="1351" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-2xl.PNG 1600w">
      
    </figure>

  <p>
    Once the word embeddings are created in this manner, positional encoding is also done to the input word tokens in the sequences before the sequences are sent off to the encoder layers.
  </p>

  <p>
    Positional encoding tells the transformer what position the word vectors are in relative to each other in the sequence. Since sequences are processed in parallel, the transformer doesn't know beforehand what order the word vectors come in. In the perspective of a transformer, all words are in a similar position which is untrue. Therefore positional encoding encodes the word's relative position information into the representation before being processed by the encoder and decoder layers.&nbsp;<br>
  </p>

  <p>
    Positional encoding is done using a sine and cosine function, and computed independtly of the word embeddings.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/Capture3.PNG" height="144" width="518" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-2xl.PNG 1600w">
      
    </figure>

  <ul>
    <li><b style="font-style: italic;">pos&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; : </b>Position of the word in the sequence (or sentence)&nbsp;</li><li><b style="font-style: italic;">d_model : </b>Length of the word vector, also known as the embedding size&nbsp;</li><li><b style="font-style: italic;">i&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; :&nbsp; </b>Index value of the word vector<b style="font-style: italic;">&nbsp;&nbsp;</b></li>
  </ul>

  <p>
    The sine function is applied to even indexes of <b style="font-style: italic;">i </b>of the word vector, whereas the cosine function is applied to all odd indexes of the word vector.&nbsp;
  </p>

  <p>
    This diagram shows the big picture of word and position embedding done in the transformer<br>
  </p>

    <figure class="post__image post__image--wide">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/capture4.PNG" height="814" width="1082" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/capture4-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture4-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture4-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture4-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture4-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture4-2xl.PNG 1600w">
      
    </figure>

    <h3 id="step-2-encoding-sequences-using-attention">
      Step 2: Encoding Sequences using Attention
    </h3>

  <p>
    The N encoder layers encode the word vector sequences. Depending on the implementation the number of encoder/decoder layers can vary, the proposed transformer architecture in the paper has N = 6 encoder and decoder layers, stacked on top of one another.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/Group-11-2.png" height="640" width="405" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-2xl.png 1600w">
      <figcaption>Components inside an encoder layer of a transformer</figcaption>
    </figure>

  <ol>
    <li>The input to the layer gets processed by an attention layer.&nbsp;</li><li>An operation known as the 'Add &amp; Norm' is done afterward, then the output is passed down to a feed-forward network.<br></li><li>Another 'Add &amp; Norm' operation is done to the output and sent onto the next encoder layer.<br></li><li>The next encoder layer carries out the same operations, and the output is passed onto the next encoder on top of that afterward.&nbsp;<br></li><li>The decoder layers utilize the output of the last encoder layer to generate the final output.&nbsp;<br></li>
  </ol>

    <h4 id="what-is-attentionnbsp">
      What is attention?&nbsp;
    </h4>

  <p>
    Before moving forward, it is essential to understand intuitively what the attention layer does in the encoder layer. <br><br>The attention layer tries to create a matrix that gives a <b>score on how much each word relates to every other word in that sequence. </b><br><br>The following visual depicts what the attention layer tries to do to the input sequences.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/capture5.PNG" height="537" width="688" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/capture5-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture5-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture5-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture5-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture5-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture5-2xl.PNG 1600w">
      <figcaption>Softmax scores of an attention weight matrix </figcaption>
    </figure>

  <p>
    The following visual depicts what the attention layer tries to do to the input sequences. The attention layer lets the transformer model know where to focus on each word and its relation to other words. These long-range patterns are vital for various natural language processing tasks. Attention allows the transformer to look at the data at once and identify patterns effectively, which is why transformers perform better than RNNs. &nbsp;
  </p>

    <h4 id="what-is-the-add-and-norm-layernbsp">
      What is the 'add and norm' layer?&nbsp;
    </h4>

  <p>
    The add and norm layer takes the output generated by the attention layer and the input for the attention layer, adds them together, and passes them as input to the  Layer normalization function.&nbsp;
  </p>

  <p>
    "<b>Layer Normalization directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases.</b>"
  </p>

  <p>
    Here's the equation for the layer normalization function, where we first calculate the mean of the vector and using the mean, you calculate \row, which becomes the new input to the feed-forward layer.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/Capture6.PNG" height="157" width="252" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-2xl.PNG 1600w">
      
    </figure>

    <h4 id="the-feedforward-layer">
      The feed-forward layer
    </h4>

  <p>
    The feed-forward layer in the encoder layer is a point-wise feed-forward network, a fully connected feed-forward network <b>consisting of two linear transformations W1, B1, and W2, B2 </b>with a <b>ReLU </b>activation function in between.&nbsp;
  </p>

  <p>
    The inputs for the feed-forward layer are d_model which is the embedding size, and the inner layer has a dimensionality of d_ff. The dimensions of the d_model and d_ff in the proposed architecture are 512, and 2048 respectively.&nbsp;
  </p>

    <h3 id="step-3-decoding-the-input-sequencesnbsp">
      Step 3: Decoding the input sequences&nbsp;
    </h3>

  <p>
    The decoder layers are more or less similar to each other, but their behavior slightly changes when we use the model to train and when we use it for inference.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/Group-2.png" height="680" width="600" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-2xl.png 1600w">
      
    </figure>

  <p>
    Like in the encoder layers, we embed the text input to the decoder layers, calculate their word vectors, and add positional encoding to it. We first input a 'START' token to the decoder layer to predict the next word in the sequence at the very end of the decoder layer.
  </p>

  <p>
    The next word predicted by the decoder layers is ignored during training, only used for the loss calculation and backpropagation of the loss to optimize the weights of the transformer.
  </p>

  <p>
    However, during inference, we append the predicted word as the next word token to our decoder layer and append it to the sequence to predict the next word at time step t+1.&nbsp;
  </p>

  <p>
    Each decoder layer has two attention layers; one is responsible for finding the connections of the output sequence's words with words that come before it, not after it.&nbsp;
  </p>

  <p>
    Then we send the output scores through an 'add &amp; norm' layer, implemented in the same manner as the encoder layers.&nbsp;
  </p>

  <p>
    The next attention layer finds how relevant the words are in the output sequence compared with the input sequence from the encoder layer output.
  </p>

  <p>
    The information from the second attention layer passes along onto another 'add and norm' layer, afterward a point-wise feed-forward layer, and finally through another 'add and norm' layer, and onto the next decoder layer above it.&nbsp;
  </p>

    <h3 id="step-4-training-and-inferencenbsp">
      Step 4: Training and Inference&nbsp;
    </h3>

  <p>
    The final decoder layer has a linear layer, a single linear operation applied onto the output of the last decoder layer, and maps the output of the decoder layer into a vector with the dimensions of the vocabulary size. (E.g. 10000)
  </p>

  <p>
    <b>A SoftMax function converts the values of that output vector into a probability distribution</b>, and the<b> index with the highest probability is chosen as the network's final output.</b>
  </p>

  <p>
    This <b>chosen index position is mapped into the corresponding word by looking up the index from the vocabulary.&nbsp;</b>
  </p>

  <p>
    In the training phase of the network, we calculate the next word and calculate the loss of the probability distribution of the SoftMax function with the target distribution. This loss is backpropagated across the network, and the weights of every decoder and encoder block are updated. 
  </p>

  <p>
    In the <b>training phase, we use a method known as 'teacher forcing' </b>where we manually input and append the correct word to the decoder layer's input sequence at each timestep to predict the next word.
  </p>

  <p>
    During <b>inference, the predicted next word is appended to the current input sequence of the decoder, and until a 'END' token is generated, the next word is predicted and appended to the sequence.</b>
  </p>

    <h2 id="understanding-attentionnbsp">
      Understanding Attention&nbsp;
    </h2>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/SCALDE.png" height="535" width="500" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-2xl.png 1600w">
      <figcaption>Scaled Dot Product Attention </figcaption>
    </figure>

  <p>
    <b>Attention </b>is what makes the transformer understand long-range patterns in the sequence data that it processes and can make predictions considering the context of the input sequence and the relationships between each word vector in each sequence. 
  </p>

  <p>
    The <b>self-attention mechanism compares every word token in a given sequence with every other word token in the sequence and gauges how much they are important to each other.</b>&nbsp;
  </p>

  <p>
    They model the relationship each word token has with each other in the sequence. Whether that relationship is strong or weak, based on these attention scores, the feedforward layers of the encoder/decoder layer can make better predictions by understanding the dependency of each word token with the rest of the word tokens.&nbsp;
  </p>

  <p>
    The below matrices shows how the attention layer scores word vector pairs based on how strong the relationships is between the two word vectors in the sequence.&nbsp;
  </p>

  <p>
    Note that each word's contextual meaning as well as positional information is taken into account when the attention layer computes the scores.&nbsp;
  </p>

  <div  class="gallery-wrapper">
    <div class="gallery" data-columns="3">
      <figure class="gallery__item">
      <a href="https://thenlpstudent.github.io/media/posts/1/gallery/attention2-2.PNG" data-size="329x339">
        <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/gallery/attention2-2-thumbnail.PNG" height="339" width="329" alt="" >
      </a>
      
    </figure><figure class="gallery__item">
      <a href="https://thenlpstudent.github.io/media/posts/1/gallery/attention1-2.PNG" data-size="346x345">
        <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/gallery/attention1-2-thumbnail.PNG" height="345" width="346" alt="" >
      </a>
      
    </figure>
    </div>
  </div>

  <p>
    The attention model works by comparing every word token with every other word token using the dot-product operation. The more significant the magnitude of the dot-product, the higher likelihood that the pair of word vectors have a strong relationship and vice versa. Thus, the attention mechanism functions as a <b>lookup dictionary.</b>
  </p>

  <p>
    A dictionary has <b>queries, keys, and values</b>. <br>We match up the queries with the keys and weigh them. The higher the dot product of the query and key pair, the higher the likelihood of them relating to one another. <br>
  </p>

  <p>
    With the multiplication of the query and key pairs together with every Value vector in the Value Matrix, we find which word is most likely has a strong connection with which word in the sequence.&nbsp;
  </p>

  <p>
    These query, key, and value are matrices created by multiplying the input sequence by weight matrices<b> Wq, Wk, </b>and <b>Wv</b>. These parameters are fine-tuned when the transformer trains, and over time the attention mechanism can learn the interrelationships between each word token for effective prediction over time.&nbsp;
  </p>

  <p>
    Here are the formulas that generate matrices Q for Query, K for Key, and V for Value.&nbsp;<br>
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/CodeCogsEqn1.gif" height="32" width="145" alt="" >
      
    </figure>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/CodeCogsEqn2.gif" height="27" width="153" alt="" >
      
    </figure>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/CodeCogsEqn3.gif" height="27" width="147" alt="" >
      
    </figure>

  <p>
    The below diagram shows how the matrix multiplication occurs visually&nbsp;
  </p>

    <figure class="post__image post__image--wide">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/test1.PNG" height="290" width="500" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/test1-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/test1-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/test1-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/test1-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/test1-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/test1-2xl.PNG 1600w">
      
    </figure>

  <p>
    Here are the formulae that calculate the attention score for a given input sequence, where M is the mask matrix, which is explained in the next section.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/CodeCogsEqn5.gif" height="77" width="381" alt="" >
      
    </figure>

    <h3 id="maskingnbsp">
      Masking&nbsp;
    </h3>

  <p>
    A design feature of attention is masking.&nbsp;<b>When calculating attention, it's important that we only consider the attention scores of valid word vectors.</b>
  </p>

  <p>
    Some sentences might not be of the length as the sequence size in the encoder layer, and sentences can have varying lengths. So we add a unique token known as the <b>'PAD' token</b> to make all sentences the same length.&nbsp;
  </p>

  <p>
    However, the 'PAD' token is meaningless. Therefore we don't calculate the attention score of that token.&nbsp;
  </p>

  <p>
    To cancel out the attention score of 'PAD' tokens, before we calculate the attention score using the SoftMax function, we add a mask matrix to the attention score.
  </p>

  <p>
    A mask matrix in the encoder layer might look something like this.&nbsp;&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/mask1.PNG" height="421" width="507" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/mask1-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask1-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask1-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask1-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask1-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask1-2xl.PNG 1600w">
      
    </figure>

  <p>
    In the decoder layers, we want to force the network to only make predictions by looking at the word before it. Thus, we force the network to guess the next word in the sequence by looking back at the words before it. This type of mask is known as a look-ahead mask.&nbsp;
  </p>

  <p>
    Here's what a mask in the decoder layer might look like this.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/mask2.PNG" height="388" width="447" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/mask2-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask2-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask2-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask2-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask2-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask2-2xl.PNG 1600w">
      
    </figure>

  <p>
    Here's the complete visualization on how attention is calculated
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/softmax-fn.PNG" height="313" width="886" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-2xl.PNG 1600w">
      <figcaption>How the attention weight matrix is calculated using the Softmax function with a suitable mask</figcaption>
    </figure>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/Group-38.png" height="727" width="930" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-2xl.png 1600w">
      <figcaption>How the final output of the self attention layer is calculated by using the V (values) matrix</figcaption>
    </figure>

    <h3 id="attention-in-the-decoder-layersnbsp">
      Attention in the decoder layers&nbsp;
    </h3>

  <p>
    In the decoder layer, we have an attention layer that relies on the encoder's input representation.&nbsp;
  </p>

  <p>
    The encoder's input representation takes the Query and Key values, and the value matrix is taken from the decoder's input. Intuitively this means that we find the relationship between the encoder's input word vectors and how it relates to the decoder's word vectors.&nbsp;
  </p>

  <p>
    For example, in the context of machine translation, if you have English words as input and translating them into German, the encoder's input representation would be English word vectors, and the decoder's input representation would be German word vectors. We find the relationship between the encoder's input representation and the decoder's input representation, essentially finding out how strong or weak the relationship is between English and German word pairs.&nbsp;
  </p>

  <p>
    The below matrix shows how the attention layer gauges and compares every German word in the output sequence with every English word in the input sequence, and models the relationship between both the languages.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/scaledd-2.PNG" height="402" width="350" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-2xl.PNG 1600w">
      
    </figure>

    <h2 id="multihead-attention-cause-ngt1-heads-are-better-than-onenbsp">
      Multi-Head Attention: cause n&gt;1 heads are better than one!&nbsp;
    </h2>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/multi-head-attention_l1A3G7a.png" height="648" width="500" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-2xl.png 1600w">
      <figcaption>Multi-Head Attention mechanism in the transformer model, found in both the encoder & decoder layers</figcaption>
    </figure>

  <p>
    <b>Multi-head attention splits the input sequence into multiple parts and applies scaled dot product attention to each part individually</b>.&nbsp;
  </p>

  <p>
    This has been shown to improve the quality of attention as more complex relationships between word tokens can be extracted.&nbsp;
  </p>

  <p>
    The input sequence is split based on the number of heads, a hyperparameter set before training the transformer.&nbsp;
  </p>

  <p>
    For example, if the head size is 8, the input sequence is split into eight equal parts by dividing the embedding size by the number of attention heads. This is known as the <b>query size</b>.
  </p>

  <p>
    Query size is equal to the embedding size divided by the number of attention heads.&nbsp;
  </p>

  <p>
    A linear layer is used to get the Q, K, and V matrices by multiplying the input matrix by the corresponding weight matrix. In this example, the <b>embedding size is 6</b>, and the <b>number of attention heads used is 2</b>. <b>Therefore the query size is 6/2, which is 3.&nbsp;</b>
  </p>

  <p>
    As you can see in the above diagram, there is a red line separating the weights of the two heads, which is known as the logical split. When implementing, each attention head doesn't have its weight matrix, but the weights for every attention head are in one matrix. Thus, it's designed to easily update the weight matrices without taking up much memory, compute power, or time.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/part1.PNG" height="341" width="605" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/part1-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/part1-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/part1-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/part1-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/part1-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/part1-2xl.PNG 1600w">
      <figcaption>Using a logical split to share weights for the attention heads in the same weight matrices </figcaption>
    </figure>

  <p>
    Same as before, the K matrix transposes to multiply the Q and K matrices together.&nbsp;
  </p>

  <p>
    However, before multiplying both these matrices, we first add an extra dimension known as the head size, which is 2 in this case.&nbsp;
  </p>

  <p>
    This turns the 2D Q and K matrices to 3 dimensional. The V matrix is also reshaped to have a head dimension. Therefore all matrices are reshaped to the form Sequence Size x Head Size x Query Size.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/part2.PNG" height="533" width="812" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/part2-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/part2-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/part2-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/part2-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/part2-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/part2-2xl.PNG 1600w">
      <figcaption>Introducing a new dimension known as 'head size' to explicitly separate calculations for different attention heads</figcaption>
    </figure>

  <p>
    Afterward, we carry out the matrix multiplication for Q and K, 	where we multiply the matrices for each head, Q_head1 with K_head1 and Q_head2 with K_head2.&nbsp;
  </p>

  <p>
    Afterward, every matrix multiplication of Q and K for each head results in a matrix with the shape sequence size x sequence size.&nbsp;
  </p>

  <p>
    Finally, we add masks for each head's Q and K product, and we scale it by the square root of the query size and then apply a SoftMax function over it, creating separate attention weight matrices for each head, as show below.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/part3.PNG" height="227" width="600" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/part3-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/part3-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/part3-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/part3-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/part3-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/part3-2xl.PNG 1600w">
      
    </figure>

  <p>
    Finally, we multiply the corresponding V matrix's heads with attention weight heads giving us attention scores for each head, as show below.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/part4.PNG" height="369" width="600" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/part4-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/part4-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/part4-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/part4-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/part4-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/part4-2xl.PNG 1600w">
      
    </figure>

  <p>
    We need to reshape it back into its original form and merge all the results from the different heads into one by dropping the head dimension altogether. <br><br>First, we reshape the matrix into the form Sequence Size x Query Size x Head Size by swapping the head size with the query size, which alters the shape of the matrix to the following form. <br><br>We can get rid of the head dimension, which makes the 3-dimensional matrix back into two dimensions.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/part5.PNG" height="269" width="600" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/part5-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/part5-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/part5-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/part5-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/part5-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/part5-2xl.PNG 1600w">
      
    </figure>

  <p>
    Afterward, we multiply the resulting merged matrix using a <b>weight matrix W0</b> with a shape of <b>embedding size x embedding size</b>, and we add a<b> bias vector to the resulting matrix.</b>
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/part6.PNG" height="206" width="600" alt=""  sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/part6-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/part6-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/part6-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/part6-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/part6-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/part6-2xl.PNG 1600w">
      
    </figure>

  <p>
    This is how multi-head attention is done to a single sample of our input.<b> Likewise, we carry out the following process simultaneously for all sentence samples</b>, thanks to the fact that <b>matrix multiplication is inherently parallelizable.&nbsp;</b>
  </p>

    <h2 id="referencesnbsp">
      References&nbsp;
    </h2>

  <ul>
    <li><a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is all you need - Ashish Vaswani et al, NIPS 2017</a><br></li><li><a href="https://www.tensorflow.org/text/tutorials/transformer">Tensor flow model for language understanding&nbsp;</a></li>
  </ul>

  <p>
    
  </p>

  <p>
    &nbsp;
  </p>

  <p>
    &nbsp;
  </p>
            ]]>
        </content>
    </entry>
</feed>
