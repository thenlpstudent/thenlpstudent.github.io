{
    "version": "https://jsonfeed.org/version/1",
    "title": "The NLP Student&#x27;s Blog",
    "description": "",
    "home_page_url": "https://thenlpstudent.github.io",
    "feed_url": "https://thenlpstudent.github.io/feed.json",
    "user_comment": "",
    "icon": "https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400.jpg",
    "author": {
        "name": "The NLP Student"
    },
    "items": [
        {
            "id": "https://thenlpstudent.github.io/new-post-2.html",
            "url": "https://thenlpstudent.github.io/new-post-2.html",
            "title": "New post 2",
            "author": {
                "name": "The NLP Student"
            },
            "tags": [
            ],
            "date_published": "2023-03-31T18:55:13+05:30",
            "date_modified": "2023-03-31T18:55:16+05:30"
        },
        {
            "id": "https://thenlpstudent.github.io/the-multi-armed-bandits-problem.html",
            "url": "https://thenlpstudent.github.io/the-multi-armed-bandits-problem.html",
            "title": "The Multi-Armed Bandit Problem ",
            "summary": "In solving the problem of exploration vs. exploitation in reinforcement learning, we use bandit problems to understand and apply algorithms that balance RL agents' exploration and exploitation behaviors. What exactly is the multi-armed bandit problem, and how can you solve this type of problem in&hellip;",
            "content_html": "<p>In solving the problem of exploration vs. exploitation in reinforcement learning, we use bandit problems to understand and apply algorithms that balance RL agents' exploration and exploitation behaviors. <br><br>What exactly is the multi-armed bandit problem, and how can you solve this type of problem in RL? <br><br>This blog post focuses on doing just that. </p>\n<div class=\"post__toc\">\n<h3>Table of Contents</h3>\n<ul>\n<li><a href=\"#mcetoc_1fimqv2jdp4\">What is the Multi Armed Bandit Problem? </a>\n<ul>\n<li><a href=\"#mcetoc_1fimqv2jdp5\">Stationary vs Non Stationary Bandit Problem </a></li>\n</ul>\n</li>\n<li><a href=\"#mcetoc_1fimqv2jdp6\">Solve by taking estimates </a>\n<ul>\n<li><a href=\"#mcetoc_1fimqv2jdp7\">Incremental Updates</a></li>\n<li><a href=\"#mcetoc_1fimt3ol2qu\">Solving non-stationary bandit problems </a></li>\n</ul>\n</li>\n<li><a href=\"#mcetoc_1fivpg6e31e\">Pseudocode, Code, and Results </a></li>\n<li><a href=\"#mcetoc_1fimqv2jdp9\">References </a></li>\n</ul>\n</div>\n<h2 id=\"mcetoc_1fimqv2jdp4\">What is the Multi Armed Bandit Problem? </h2>\n<p>Consider the following situation that you find yourself in. <br>You are faced repeatedly with 'k' different actions to take each time. <br>After each action that you take at a given time, you receive a numerical reward.<br><br>This reward is picked out from a stationary probability distribution that depends on your selected action. <br><br>Your goal in this problem is to maximize the total reward you receive over a period of time by selecting the best possible action at each time so that you maximize your reward. <br><br>In this problem, each action has an average reward. This average reward is the value of the action, or how good that action is. <br><br>If you knew the value of each action, then solving this problem would be very simple. Just pick the action with the highest value to get the biggest reward possible at the end. <br><br>However, you don't know the value of each of these actions. Therefore, you would need to explore and come up with estimates on the value of each action by taking random actions in the first few timesteps. <br><br>Once you have good guesses after the first few timesteps, then you are presented with a choice. <br><br>Either you can exploit, which means pick the action with the largest value estimate or continue to explore and re-adjust your current estimates of the values of your actions. </p>\n<h3 id=\"mcetoc_1fimqv2jdp5\">Stationary vs Non Stationary Bandit Problem </h3>\n<p>The rewards for the actions you or the reinforcement learning (RL) agent takes at the current timestep are sampled from a probability distribution. E.g., Gaussian Distribution. <br><br>In a stationary Bandit problem, the probability distribution doesn't change at each timestep, which means that the true value of each action is constant across all timesteps. However, in a non-stationary bandit problem, the probability distribution changes at every timestep. <br><br>Therefore the true value of each action at each timestep is quite different from other timesteps. <br><br>However, we can solve both variations of such bandit problems by estimating the value of each action, continuing to re-estimate them by trying different actions (exploration), or picking the highest calculated value action you currently have (exploitation) </p>\n<h2 id=\"mcetoc_1fimqv2jdp6\">Solve by taking estimates </h2>\n<p>One way to solve the Multi-Armed Bandit problem is through estimating the true value of each action.</p>\n<p>Solving by estimating the true value of each action can either be applied to stationary and non-stationary bandit problems. </p>\n<p>Firstly we need a few mathematical notations to formalize the bandit problem before finding a way to solve it.</p>\n<ul>\n<li>\\( A_t \\)  The action the agent picks at time \\(t\\)</li>\n<li>\\( R_t \\)    The reward the agent earns at time \\(t\\)</li>\n<li>\\( q_*(a) \\)   The true value of action \\(a\\)</li>\n<li>\\( Q_t(a) \\)   The estimated value of action \\(a\\) at timestep \\(t\\) </li>\n</ul>\n<p>After exploration, we would want our RL agent to find closer estimates to the true value of action 'a'. </p>\n<p>\\[ Q_t(a) \\approx q_*(a) \\]</p>\n<p>To estimate the value of each action, we keep track of the total sum of rewards collected when we took a specific action 'a' and the number of times we took that action 'a'.</p>\n<p>By dividing as follows, we get an average estimate of how good that action is at the current timestep. </p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/3/captur1.PNG\" alt=\"\" width=\"818\" height=\"94\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/3/responsive/captur1-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/3/responsive/captur1-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/3/responsive/captur1-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/3/responsive/captur1-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/3/responsive/captur1-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/3/responsive/captur1-2xl.PNG 1600w\"></figure>\n<p>The below shows our definition for how the agent picks the suitable action at timestep t. <br><br>This equation is the greedy approach (exploitation) in which the agent selects the highest estimated action value as its next action. <br><br>The agent picks the action with the highest estimate, hoping that it would bring the highest possible reward for timestep 't'. </p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/3/captur2.PNG\" alt=\"\" width=\"258\" height=\"67\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/3/responsive/captur2-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/3/responsive/captur2-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/3/responsive/captur2-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/3/responsive/captur2-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/3/responsive/captur2-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/3/responsive/captur2-2xl.PNG 1600w\"></figure>\n<h3 id=\"mcetoc_1fimqv2jdp7\">Incremental Updates</h3>\n<p>To keep things simple let us assume the following change in notation, <br>  </p>\n<p>Since the above is computationally costly, we can incrementally compute the mean by re-arranging the mean calculation using a little bit of algebra. </p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/3/incremental-2.PNG\" alt=\"\" width=\"564\" height=\"474\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/3/responsive/incremental-2-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/3/responsive/incremental-2-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/3/responsive/incremental-2-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/3/responsive/incremental-2-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/3/responsive/incremental-2-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/3/responsive/incremental-2-2xl.PNG 1600w\"></figure>\n<p>This mean update has the following form used throughout in reinforcement learning for problems other than bandits. <br><br>The new estimate of the value of an action is the old estimate moved in the direction of the error between the current reward seen (i.e., 'target') and the old estimate. </p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/3/updaterule.PNG\" alt=\"\" width=\"822\" height=\"67\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/3/responsive/updaterule-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/3/responsive/updaterule-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/3/responsive/updaterule-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/3/responsive/updaterule-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/3/responsive/updaterule-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/3/responsive/updaterule-2xl.PNG 1600w\"></figure>\n<h3 id=\"mcetoc_1fimt3ol2qu\">Solving non-stationary bandit problems </h3>\n<p><span data-preserver-spaces=\"true\">When the optimal value changes from timestep to timestep, then you have a non-stationary bandit problem.</span></p>\n<p><span data-preserver-spaces=\"true\">The probability distribution in which rewards are sampled from the agent for the action it picks comes from a different probability distribution randomly at every timestep.</span></p>\n<p><span data-preserver-spaces=\"true\">Therefore the true value of each action is different at each timestep. </span></p>\n<p><span data-preserver-spaces=\"true\">Therefore we cannot rely on average sampling.</span></p>\n<p><span data-preserver-spaces=\"true\">We need a mechanism in which we give more weight to rewards that we see recently instead of values we saw in the past and move the estimates of our action values towards recently seen reward signals. </span></p>\n<p><span data-preserver-spaces=\"true\">This is done using a fixed step size parameter \\( \\alpha \\), a </span><strong><span data-preserver-spaces=\"true\">constant</span></strong><span data-preserver-spaces=\"true\"> </span><strong><span data-preserver-spaces=\"true\">parameter</span></strong><span data-preserver-spaces=\"true\"> that lies between</span><strong><span data-preserver-spaces=\"true\"> 0 and 1. </span></strong></p>\n<p>$$\\begin{aligned} Q_{n+1} &amp;= Q_n + \\alpha[R_n + Q_n] \\\\ &amp;= \\alpha R_n + (1 - \\alpha)Q_n \\\\ &amp;= \\alpha R_n + (1 - \\alpha)Q_n \\\\ &amp;= \\alpha R_n + (1 - \\alpha)[\\alpha R_{n-1} + (1-\\alpha)Q_{n-1}] \\\\ &amp;= \\alpha R_n + (1 - \\alpha)[\\alpha R_{n-1} + (1-\\alpha)Q_{n-1}] \\\\ &amp;= \\alpha R_n + (1 - \\alpha) \\alpha R_{n-1} + (1 - \\alpha)^2 \\alpha R_{n-2} + .... + (1- \\alpha)^nQ_1 \\\\ &amp;= (1- \\alpha)^nQ_1 + \\sum^{n}_{i=1}\\alpha(1 - \\alpha)^{n-i}R_i \\end{aligned}$$</p>\n<p>By having a constant step size, the action values are updated so that the estimates are moved in the direction of the recent reward, as they have a higher weight and the older reward signals have a lower influence in the direction of where the estimates would move. </p>\n<p>This makes the RL agents get more good action value estimates in non-stationary bandit problems. </p>\n<h2 id=\"mcetoc_1fivpg6e31e\"><span data-preserver-spaces=\"true\">Pseudocode, Code, and Results </span></h2>\n<p>The following code snippet shows the pseudocode implementation of solving the bandit problem by estimating the value of each action. </p>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/3/carbon7-2.png\" alt=\"\" width=\"1806\" height=\"1076\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/3/responsive/carbon7-2-xs.png 300w ,https://thenlpstudent.github.io/media/posts/3/responsive/carbon7-2-sm.png 480w ,https://thenlpstudent.github.io/media/posts/3/responsive/carbon7-2-md.png 768w ,https://thenlpstudent.github.io/media/posts/3/responsive/carbon7-2-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/3/responsive/carbon7-2-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/3/responsive/carbon7-2-2xl.png 1600w\"></figure>\n<p>The following code snippet is the complete implementation of solving a ten-armed bandit problem. At each timestep, the agent can pick any ten actions and get a reward based on a gaussian distribution. <br><br>This distribution doesn't change over timesteps. Therefore, this is a stationary bandit problem. </p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/3/maincode1-2.png\" alt=\"\" width=\"2354\" height=\"3068\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/3/responsive/maincode1-2-xs.png 300w ,https://thenlpstudent.github.io/media/posts/3/responsive/maincode1-2-sm.png 480w ,https://thenlpstudent.github.io/media/posts/3/responsive/maincode1-2-md.png 768w ,https://thenlpstudent.github.io/media/posts/3/responsive/maincode1-2-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/3/responsive/maincode1-2-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/3/responsive/maincode1-2-2xl.png 1600w\"></figure>\n<p>The following code snippet is modified slightly to show how we can solve a non-stationary bandit problem.<br><br>We can solve the problem using a step-size constant instead of counting how many times action 'a' was chosen before timestep t. </p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/3/stepsize.png\" alt=\"\" width=\"2354\" height=\"1278\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/3/responsive/stepsize-xs.png 300w ,https://thenlpstudent.github.io/media/posts/3/responsive/stepsize-sm.png 480w ,https://thenlpstudent.github.io/media/posts/3/responsive/stepsize-md.png 768w ,https://thenlpstudent.github.io/media/posts/3/responsive/stepsize-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/3/responsive/stepsize-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/3/responsive/stepsize-2xl.png 1600w\"></figure>\n<p>The average reward at each timestep is stored in a list for each timestep.</p>\n<p>For example, the following graph shows how the epsilon parameter influences the epsilon-greedy algorithm to obtain high amounts of rewards over 10,000 timesteps. </p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/3/f22.png\" alt=\"\" width=\"640\" height=\"480\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/3/responsive/f22-xs.png 300w ,https://thenlpstudent.github.io/media/posts/3/responsive/f22-sm.png 480w ,https://thenlpstudent.github.io/media/posts/3/responsive/f22-md.png 768w ,https://thenlpstudent.github.io/media/posts/3/responsive/f22-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/3/responsive/f22-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/3/responsive/f22-2xl.png 1600w\"></figure>\n<p>The graph below shows how average sampling methods with a variable step size perform compared to fixed step size sampling methods for non-stationary bandit problems. </p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/3/f2.png\" alt=\"\" width=\"640\" height=\"480\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/3/responsive/f2-xs.png 300w ,https://thenlpstudent.github.io/media/posts/3/responsive/f2-sm.png 480w ,https://thenlpstudent.github.io/media/posts/3/responsive/f2-md.png 768w ,https://thenlpstudent.github.io/media/posts/3/responsive/f2-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/3/responsive/f2-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/3/responsive/f2-2xl.png 1600w\"></figure>\n<p>Thanks for reading this article this far! </p>\n<p>Follow me on Twitter <a href=\"https://twitter.com/intent/user?screen_name=studentnlp\" target=\"_blank\" rel=\"noopener noreferrer\">@studentnlp</a> so you won't miss my latest blog post, and as always, happy hacking!</p>\n<h2 id=\"mcetoc_1fimqv2jdp9\">References </h2>\n<p><span class=\"ILfuVd\"><span class=\"hgKElc\">Sutton, R.S. &amp; Barto, A.G., 2018. Reinforcement learning: An introduction, MIT press.</span></span></p>",
            "author": {
                "name": "The NLP Student"
            },
            "tags": [
            ],
            "date_published": "2021-10-23T20:04:32+05:30",
            "date_modified": "2021-10-27T08:04:03+05:30"
        },
        {
            "id": "https://thenlpstudent.github.io/introduction-to-reinforcement-learning.html",
            "url": "https://thenlpstudent.github.io/introduction-to-reinforcement-learning.html",
            "title": "Basics of Reinforcement Learning: Part I",
            "summary": "Welcome! In this article, we will be going over the fundamentals of reinforcement learning. We will discuss what reinforcement learning is, how agents learn from rewards and experiences, what policies and value functions are, and how to model environments in a Markov decision process step-by-step.",
            "content_html": "<p>Welcome! In this article, we will be going over the fundamentals of reinforcement learning. We will discuss what reinforcement learning is, how agents learn from rewards and experiences, what policies and value functions are, and how to model environments in a Markov decision process step-by-step.</p>\n<div class=\"post__toc\">\n<h3>Table of Contents</h3>\n<ul>\n<li><a href=\"#mcetoc_1fickllfsg7\">What is Reinforcement Learning (RL)? </a>\n<ul>\n<li><a href=\"#mcetoc_1fickllfsg8\">What is a reward? </a></li>\n<li><a href=\"#mcetoc_1fickllfsg9\">The Reward Hypothesis </a></li>\n</ul>\n</li>\n<li><a href=\"#mcetoc_1fickllfsga\">Components in Reinfocement learning</a>\n<ul>\n<li><a href=\"#mcetoc_1fickllfsgb\">The Agent and the Enviroment  </a></li>\n<li><a href=\"#mcetoc_1fickllfsgc\">What is the Markov State?</a></li>\n<li><a href=\"#mcetoc_1ficm0ep4ij\">Returns, Epsoides and Continuous Tasks  </a></li>\n<li><a href=\"#mcetoc_1fie1hfh3oq\">What are value functions and policies?  </a></li>\n</ul>\n</li>\n<li><a href=\"#mcetoc_1fickqocai8\">The Markov Decision Process (MDP)</a>\n<ul>\n<li><a href=\"#mcetoc_1fickllfsgf\">The Value Function </a></li>\n<li><a href=\"#mcetoc_1fickllfsgg\">The Action-Value Function</a></li>\n</ul>\n</li>\n<li><a href=\"#mcetoc_1fickllfsgk\">Solving the MDP</a>\n<ul>\n<li><a href=\"#mcetoc_1fickqocaib\">The optimal value and action-value function</a></li>\n</ul>\n</li>\n<li><a href=\"#mcetoc_1fif6had5p3\">Grid world Example </a></li>\n<li><a href=\"#mcetoc_1fie5fapjov\">References </a></li>\n</ul>\n</div>\n<h2 id=\"mcetoc_1fickllfsg7\">What is Reinforcement Learning (RL)? </h2>\n<p>Reinforcement learning is a learning process in which agents learn through interacting with the environment and learn to take actions in a way that maximizes its reward signal. The reward signal is given by the environment depending on how well the agent performed in that environment. <br><br>Reinforcement learning is a computational approach that learns from interaction with an environment. The goal of a reinforcement learning agent is to maximize a numerical reward signal. </p>\n<h3 id=\"mcetoc_1fickllfsg8\">What is a reward? </h3>\n<p>A reward is a simple number given to the agent by the environment each time the agent interacts with the environment. The agent's goal is to maximize then the total amount of reward it receives, which means that the agent needs to maximize the cumulative reward in the long term, not the immediate reward in the short term. </p>\n<h3 id=\"mcetoc_1fickllfsg9\">The Reward Hypothesis </h3>\n<p>The reward hypothesis states that an RL agent's goal is to maximize the expected value of the received scalar signal's cumulative reward. <br><br>Therefore, reinforcement learning gets based on learning methods that maximize the total cumulative reward by taking actions that help reach this goal.</p>\n<h2 id=\"mcetoc_1fickllfsga\">Components in Reinfocement learning</h2>\n<p>The two main components in any reinforcement learning-based algorithm are known as the agent and the environment.</p>\n<h3 id=\"mcetoc_1fickllfsgb\">The Agent and the Enviroment  </h3>\n<p>An agent is a learner and the decision-maker inside an environment. The agent observes the environment and takes action, which is how the agent interacts with the environment. The environment then gives a reward to that agent based on the agent's action at that particular time and situation. <br><br>Everything that is outside of the agent is known as the environment.</p>\n<h3 id=\"mcetoc_1fickllfsgc\">What is the Markov State?</h3>\n<p>In RL, understanding the concept of a state is vital. The state is the information that we use to determine what might happen next. Simply put, the state is a function of history. <br><br>The agent has its state, in which case the agent's state is the function of the agent's history and experiences collected by interacting with the environment.</p>\n<p>The environment also has its state, known as the environment state. <br><br>The agent's or the environment's state is a <strong>Markov state</strong> if the current state is independent of the past. This is because the past information (history) gets derived from the current state. Therefore the state doesn't depend on history. <br><br><strong>\"The future is independent of the past given the present\"</strong> - and future states can be computed from the current state, and the history can be discarded. </p>\n<p>Suppose the agent can access the environment state. In that case, the mathematical model of the RL problem is known as a fully observable Markov decision process, whereas if the agent cannot access the state of the environment, then it's known as a partially observable Markov decision process. </p>\n<h3 id=\"mcetoc_1ficm0ep4ij\">Returns, Epsoides and Continuous Tasks  </h3>\n<p>The agent's goal is to maximize the total cumulative reward it receives from the environment over time. <br><br>To understand more about the return, we formalize it as follows. This is the basic form of the expected return, which is just the sum of all rewards gathered till the agent reaches the terminal state of the environment (last state of interaction with the environment) from some timestep t. Capital T denotes the final timestep.</p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/2/returneq.PNG\" alt=\"\" width=\"375\" height=\"61\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/2/responsive/returneq-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/returneq-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/returneq-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/returneq-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/returneq-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/returneq-2xl.PNG 1600w\"></figure>\n<p>Depending on the environment, the agent can work in episodic tasks or continuing tasks.</p>\n<p><br>An episodic task ends at a specific terminal state S+ with different rewards for different outcomes. (All nonterminal states denoted as S.)</p>\n<p>E.g., winning and losing a game of chess, the next episode or next game begins independently of how the previous game ended) The terminal state ends, and the environment resets back into a starting state. </p>\n<p><br>Continous tasks don't have a terminal state. Therefore they go on without a limit. Here taking the total cumulative reward is tricky because the total cumulative reward will be T = ∞.  <br><br>Now we introduce a notation known as the discount factor, a value between 0 and 1. This is so that in continuous environments, the total cumulative reward would not be ∞. <br>If we set the discount factor closer to 0, the agent tends to try and optimize to get higher immediate rewards. On the other hand, if the discount factor is closer to 1, the agent looks ahead to long-term rewards.</p>\n<p>Therefore the agent now tries to select actions that maximize the sum of the discounted rewards (discounted return) it receives from timestep t onwards. So at each state, the agent tries to pick the best action at timestep t that maximizes the expected discounted return:</p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/2/returnmodified.PNG\" alt=\"\" width=\"522\" height=\"66\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/2/responsive/returnmodified-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/returnmodified-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/returnmodified-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/returnmodified-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/returnmodified-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/returnmodified-2xl.PNG 1600w\"></figure>\n<p>In the following manner, the current expected discounted return at timestep t relates with the future discounted return at time step t+1. <br><br>Thus, the current expected discounted return is the immediate reward at timestep t plus the discounted expected return at time step t+1.</p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  style=\"outline: 3px solid rgba(var(--primary-color-rgb), 0.55)  !important;\" src=\"https://thenlpstudent.github.io/media/posts/2/returnwhole.PNG\" alt=\"\" width=\"408\" height=\"100\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/2/responsive/returnwhole-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/returnwhole-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/returnwhole-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/returnwhole-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/returnwhole-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/returnwhole-2xl.PNG 1600w\"></figure>\n<h3 id=\"mcetoc_1fie1hfh3oq\">What are value functions and policies?  </h3>\n<p>Two other essential concepts in Reinforcement Learning are the value function and the policy function of an RL agent. <br><br>The value function of an RL agent describes how 'good' a state is in the long run.<br><br>It's a function that maps a state to a value that describes how much total expected discounted return the agent would get if the agent takes actions starting from that given state.<br><br>The policy is a function that maps a state with a given action. <br><br>Next, it describes how the behavior of the agent would be in the environment. Finally, it defines a way in which the agent would interact with the environment.<br>The policy function can either be discrete or stochastic, in the sense that it can map to certain actions based on an argmax function, where it picks the action that has the highest return (a greedy policy), or picks an action based on a probabilistic distribution.</p>\n<h2 id=\"mcetoc_1fickqocai8\">The Markov Decision Process (MDP)</h2>\n<p>The Markov Decision Process is a classic formalization of sequential decision making, where actions influence the immediate reward at time step t and future states those actions and the rewards at those future states. <br><br>The MDP process formalizes and helps us derive equations on dealing with the trade-off between immediate and delayed rewards. </p>\n<p>There are two types of values that we estimate by using an MDP, so that we can better deal with the immediate and delayed reward trade-off. <br><br>We estimate the optimal value function that gives each state S's best possible value given optimal action selections. <br><br>We also estimate the optimal action-value function, which takes in as arguments the action and the state the agent is in and gives how good of an action that action is depending on the state. </p>\n<p>Estimating these state-dependent quantities is essential to accurately assign credit to the agent's actions in the environment to make decisions better to effectively trade-off between immediate and delayed rewards. <br><br>The agent would observe the current state at time step t and get the current reward from transitioning into that state. Then, the state would take action, which hopefully maximizes the discounted return for the next timestep. </p>\n<p>The environment would process this action if the environment would produce the next state and reward the agent for moving into the new state. </p>\n<p><br>This is how the agent and the environment interact with each other. The agent's goal is to behave optimally in this environment, which is done by maximizing the total discounted return over time. </p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/2/Group-5-2.png\" alt=\"\" width=\"556\" height=\"241\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/2/responsive/Group-5-2-xs.png 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/Group-5-2-sm.png 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/Group-5-2-md.png 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/Group-5-2-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/Group-5-2-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/Group-5-2-2xl.png 1600w\"></figure>\n<p>While interacting with the environment, this would be the sequential history the agent goes through.</p>\n<p>For example, the agent starts at timestep 0, observes the state, takes action, and obtains a reward, and this cycle repeats. </p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/2/bellman_1.PNG\" alt=\"\" width=\"433\" height=\"49\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/2/responsive/bellman_1-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_1-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_1-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_1-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_1-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_1-2xl.PNG 1600w\"></figure>\n<p>The MDP describes the probability in which the agent transitions from one state to the next.</p>\n<p>The likelihood that given the agent is in a given state 's' and takes action' a', the likelihood that that agent would end up in a new state \" s' \" and gets a reward 'r' is described as follows, </p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/2/bellman_2.PNG\" alt=\"\" width=\"588\" height=\"56\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/2/responsive/bellman_2-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_2-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_2-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_2-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_2-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_2-2xl.PNG 1600w\"></figure>\n<figure class=\"post__image post__image--center\">The probability in taking action 'a' from state 's' and transitioning to state \" s' \" is the summation for all rewards in set R (all possible reward values in the MDP), and their transition probability of starting at state 's', taking action 'a', transitioning to state \" s' \" and getting probability 'r' for all rewards in set R.<br><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/2/bellman_5.PNG\" alt=\"\" width=\"710\" height=\"103\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/2/responsive/bellman_5-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_5-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_5-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_5-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_5-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_5-2xl.PNG 1600w\"></figure>\n<p>The expected reward for choosing action a from state s can be described as the summation of all rewards in R weighted over all the state transition probabilities from state 's', taking action 'a', landing in the state \" s' \" and getting a reward 'r' afterward. </p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/2/bellman_6.PNG\" alt=\"\" width=\"670\" height=\"77\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/2/responsive/bellman_6-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_6-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_6-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_6-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_6-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_6-2xl.PNG 1600w\"></figure>\n<p>The expected reward the agent will receive at the current state \" s' \" given that the previous state and action was 's' and 'a', is the summation of all rewards R, weighted by the total probability of transitioning from state 's' to \" s' \" by taking action 'a' over the probability of transitioning from state 's' to \" s' \" by taking action 'a' and getting a specific reward 'r' in R.  </p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/2/bellman_7.PNG\" alt=\"\" width=\"737\" height=\"78\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/2/responsive/bellman_7-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_7-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_7-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_7-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_7-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellman_7-2xl.PNG 1600w\"></figure>\n<p>Here it shows that the probability of transitioning from all states \" s' \" and getting all rewards 'r' from those states given that the agent was in state 's' and takes action 'a' sums to 1. This shows that the function 'p' specifies a probability distribution for each choice of state 's' and action 'a'. </p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  style=\"font-size: 18.4px; outline: 3px solid rgba(var(--primary-color-rgb), 0.55)  !important;\" src=\"https://thenlpstudent.github.io/media/posts/2/bellamn_3.PNG\" alt=\"\" width=\"521\" height=\"81\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/2/responsive/bellamn_3-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellamn_3-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellamn_3-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellamn_3-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellamn_3-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/bellamn_3-2xl.PNG 1600w\"></figure>\n<h3 id=\"mcetoc_1fickllfsgf\">The Value Function </h3>\n<p><span data-preserver-spaces=\"true\">The value function defines how good it is to be in a particular state. </span></p>\n<p><span data-preserver-spaces=\"true\">This means it gives the total expected discounted reward that is possible to receive, given that the agent continues to take actions from that state onward. </span></p>\n<p><span data-preserver-spaces=\"true\">The value function depends upon the state, and the policy the agent follows since the selection of actions from state 's' onwards depends on the agent's policy or behavior. </span></p>\n<p><span data-preserver-spaces=\"true\">The below equation shows that the value function when the agent uses a given policy </span><strong><span data-preserver-spaces=\"true\">π </span></strong><span data-preserver-spaces=\"true\">and starts at state s, is equivalent by the MDP to the expectation of the total discounted return from timestep t onwards, given that the agent currently at timestep t is on the state 's'.</span></p>\n<p><span data-preserver-spaces=\"true\">Afterward, we can expand the definition of the total discounted return and show that this is true for all states in the MDP. </span></p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/2/v_policy_s.PNG\" alt=\"\" width=\"613\" height=\"68\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s-2xl.PNG 1600w\"></figure>\n<p>We can also evaluate this expectation as follows by the recursive defintion of the total discounted return.</p>\n<p>Here it shows that the value function is the summation of all actions that can be made by the agent, and the probability the agent will choose that action  is weighted by the sum the transitioning probabiltieis of all states and rewards from state 's' by taking action 'a' multiplied by the immediate reward of the given transitioning probabilities plus the discounted value of the value function under policy <strong>π </strong>for the next state \" s' \".</p>\n<p> </p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  style=\"font-size: 18.4px; outline: 3px solid rgba(var(--primary-color-rgb), 0.55)  !important;\" src=\"https://thenlpstudent.github.io/media/posts/2/v_policy_s_s.PNG\" alt=\"\" width=\"567\" height=\"173\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s_s-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s_s-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s_s-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s_s-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s_s-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/v_policy_s_s-2xl.PNG 1600w\"></figure>\n<div> </div>\n<h3 id=\"mcetoc_1fickllfsgg\">The Action-Value Function</h3>\n<p>The action-value function maps a state and an action to show how much expected total discounted return the agent will receive from the environment if the agent starts at state 's' and takes action 'a' from that time until the environment terminates. </p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/2/q_policy_s_a.PNG\" alt=\"\" width=\"630\" height=\"87\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/2/responsive/q_policy_s_a-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_policy_s_a-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_policy_s_a-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_policy_s_a-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_policy_s_a-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_policy_s_a-2xl.PNG 1600w\"></figure>\n<h2 id=\"mcetoc_1fickllfsgk\">Solving the MDP</h2>\n<p>Solving the MDP entails that we find the optimal value function or the optimal action-value function, which can be derived from the optimal value function in the MDP and allowing the agent to pick the best action that gives the highest possible action-value for the state that the agent is currently at timestep t. </p>\n<h3 id=\"mcetoc_1fickqocaib\">The optimal value and action-value function</h3>\n<p>The optimal value function is the maximum value that can be obtained from the state s given under the policy, which enables the agent to choose such actions from that state onwards which maximizes the expected total discounted return from that state onwards. </p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/2/optimal_val_policy.PNG\" alt=\"\" width=\"230\" height=\"47\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/2/responsive/optimal_val_policy-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/optimal_val_policy-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/optimal_val_policy-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/optimal_val_policy-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/optimal_val_policy-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/optimal_val_policy-2xl.PNG 1600w\"></figure>\n<p>The optimal action-value function is the maximum action value obtained from state 's' by taking action 'a' under the policy that enables the agent to choose such actions from that state and action onwards, which maximizes the expected total discounted return from that state, action pair onwards. </p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/2/optimal_q_value.PNG\" alt=\"\" width=\"245\" height=\"42\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/2/responsive/optimal_q_value-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/optimal_q_value-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/optimal_q_value-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/optimal_q_value-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/optimal_q_value-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/optimal_q_value-2xl.PNG 1600w\"></figure>\n<p>The optimal action-value function can be written in terms of the optimal value function as the expectation of the immediate reward plus the discounted optimal value of the next state given that the current state is 's' and the action taken from that state is action 'a'. </p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/2/q_optimal_s_a_expe-2.PNG\" alt=\"\" width=\"417\" height=\"35\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_s_a_expe-2-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_s_a_expe-2-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_s_a_expe-2-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_s_a_expe-2-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_s_a_expe-2-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_s_a_expe-2-2xl.PNG 1600w\"></figure>\n<p>The recurrence relation of the optimal value function is the value of the maximum action 'a' that can be taken which is a summation of all the states and rewards in the sets S and R of the MDP, weighted by the transition probability of moving to those states obtaining those rewards multiplied by the immediate reward plus the discounted optimal value of the next state the agent would move to. </p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/2/big_equ.PNG\" alt=\"\" width=\"447\" height=\"208\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/2/responsive/big_equ-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/big_equ-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/big_equ-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/big_equ-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/big_equ-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/big_equ-2xl.PNG 1600w\"></figure>\n<p>The recurrence relation of the optimal action-value function is the sum of all states and rewards in S and R of the MDP, weighted by the transition probability multiplied by the immediate reward 'r' plus the discounted best action-value function of the next state the agent would move into by taking action \" a' \" from that state. </p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://thenlpstudent.github.io/media/posts/2/q_optimal_stup.PNG\" alt=\"\" width=\"523\" height=\"109\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_stup-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_stup-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_stup-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_stup-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_stup-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/q_optimal_stup-2xl.PNG 1600w\"></figure>\n<h2 id=\"mcetoc_1fif6had5p3\">Grid world Example </h2>\n<p>In this example, the RL agent has to traverse the following grid world, and any action that doesn't involve traversing from A to A' or B to B' gives a reward of -1. Otherwise, a reward of +10 and +5 is given if the agent gets teleported from A to A' or B to B'.</p>\n<p> <br>The agent can only take one step in north, east, south, and west. </p>\n<p>While the agent updates its value function by interacting and moving in this grid world environment, the value function over time would update as shown in the below table.</p>\n<p>It shows how much discounted total return each cell would give the agent over the long run. </p>\n<figure class=\"post__image post__image--center\" ><img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/2/policyexample.PNG\" alt=\"Reinforcement Learning: 2nd Edition (Finite Markov Decision Processes)\" width=\"508\" height=\"182\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/2/responsive/policyexample-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/policyexample-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/policyexample-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/policyexample-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/policyexample-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/policyexample-2xl.PNG 1600w\">\n<figcaption >Reinforcement Learning: 2nd Edition (Finite Markov Decision Processes)</figcaption>\n</figure>\n<p>Once the agent finds the optimal policy that makes the agent optimize on getting the best possible discounted returns from this grid world, the value function would converge to the optimal value function. </p>\n<p><br>By acting greedily upon the optimal value function, we find the optimal policy function.</p>\n<p>If the agent chooses actions that get to better states (cells) with a higher value than the other states accessible from the current state the agent is in, the agent has successfully found the best policy to act optimally in this grid world in the below diagram. </p>\n<figure class=\"post__image post__image--center\" ><img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/2/gridworld2.PNG\" alt=\"\" width=\"779\" height=\"270\" sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/2/responsive/gridworld2-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/2/responsive/gridworld2-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/2/responsive/gridworld2-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/2/responsive/gridworld2-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/2/responsive/gridworld2-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/2/responsive/gridworld2-2xl.PNG 1600w\">\n<figcaption >Reinforcement Learning: 2nd Edition (Finite Markov Decision Processes)</figcaption>\n</figure>\n<p><span data-preserver-spaces=\"true\">That brings us to the end of the basics of reinforcement learning part 1.</span></p>\n<p><span data-preserver-spaces=\"true\">This would be a 3 part series. In the second post, we will be going over Multi-armed bandits, an interesting problem in Reinforcement Learning. In the 3rd and final post, we will be going over finding the optimal value function solving the MDP using Dynamic Programming. </span></p>\n<p><span data-preserver-spaces=\"true\">Stay tuned, and follow me on Twitter </span><a target=\"_blank\" href=\"https://twitter.com/intent/user?screen_name=studentnlp\" class=\"editor-rtfLink\" rel=\"noopener\"><span data-preserver-spaces=\"true\">@studentnlp</span></a><span data-preserver-spaces=\"true\">, so you don't miss any new posts. </span></p>\n<p><span data-preserver-spaces=\"true\">Cheers! </span></p>\n<h2 id=\"mcetoc_1fie5fapjov\">References </h2>\n<ul>\n<li><span class=\"ILfuVd\"><span class=\"hgKElc\">Sutton, R.S. &amp; Barto, A.G., 2018. Reinforcement learning: An introduction, MIT press.</span></span></li>\n<li><a href=\"https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ\" target=\"_blank\" rel=\"noopener noreferrer\">David Silver \"Introduction to Reinforcement Learning\" DeepMind 2015</a> </li>\n</ul>\n<p> </p>\n<p> </p>\n<p> </p>\n<p> </p>\n<p> </p>\n<p> </p>\n<p> </p>\n<p> </p>\n<p> </p>",
            "author": {
                "name": "The NLP Student"
            },
            "tags": [
            ],
            "date_published": "2021-10-16T12:45:52+05:30",
            "date_modified": "2021-10-21T07:51:32+05:30"
        },
        {
            "id": "https://thenlpstudent.github.io/transformers-and-attention.html",
            "url": "https://thenlpstudent.github.io/transformers-and-attention.html",
            "title": "The Transformer Explained",
            "summary": "Welcome! In this article, we will be going over what a Transformer is, the intuition and the inner workings behind the attention mechanism it employs to process sequential data, and how the Multi-Head Attention mechanism works as implemented by the paper 'Attention is all you&hellip;",
            "content_html": "\n  <p>\n    Welcome! In this article, we will be going over what a <b>Transformer </b>is, the intuition and the inner workings behind the <b>attention mechanism</b> it employs to process sequential data, and how the <b>Multi-Head Attention mechanism</b> works as implemented by the paper '<b>Attention is all you need</b>' <b>NeurIPS 2017.&nbsp;</b>\n  </p>\n\n  <p>\n    \n  </p>\n\n  <div class=\"post__toc\">\n    <h3>Quick Links&nbsp;</h3>\n    <ul>\n      <li><a href=\"#the-big-picture\">The Big Picture</a></li><li><a href=\"#how-the-transformer-works\">How the Transformer works</a><ul><li><a href=\"#step-1-word-and-positional-embeddingnbsp\">Step 1: Word and Positional Embedding&nbsp;</a></li><li><a href=\"#step-2-encoding-sequences-using-attention\">Step 2: Encoding Sequences using Attention</a><ul><li><a href=\"#what-is-attentionnbsp\">What is attention?&nbsp;</a></li><li><a href=\"#what-is-the-add-and-norm-layernbsp\">What is the 'add and norm' layer?&nbsp;</a></li><li><a href=\"#the-feedforward-layer\">The feed-forward layer</a></li></ul></li><li><a href=\"#step-3-decoding-the-input-sequencesnbsp\">Step 3: Decoding the input sequences&nbsp;</a></li><li><a href=\"#step-4-training-and-inferencenbsp\">Step 4: Training and Inference&nbsp;</a></li></ul></li><li><a href=\"#understanding-attentionnbsp\">Understanding Attention&nbsp;</a><ul><li><a href=\"#maskingnbsp\">Masking&nbsp;</a></li><li><a href=\"#attention-in-the-decoder-layersnbsp\">Attention in the decoder layers&nbsp;</a></li></ul></li><li><a href=\"#multihead-attention-cause-ngt1-heads-are-better-than-onenbsp\">Multi-Head Attention: cause n&gt;1 heads are better than one!&nbsp;</a></li><li><a href=\"#referencesnbsp\">References&nbsp;</a></li>\n    </ul>\n  </div>\n  \n\n  <p>\n    A <b>transformer </b>is a deep learning architecture that performs well for sequential data-related Machine learning tasks. It is based around an encoder-decoder architecture to handle and process sequential data in parallel. In addition, it uses a mechanism known as Attention to look back and forwards the sequential input and identify long-range patterns and relationships between each component in the sequence. <br><br>The transformer architecture solves the shortcomings of recurrent neural networks and convolutional neural networks when processing sequential data and making good predictions.&nbsp;\n  </p>\n\n  <p>\n    Recurrent neural networks aren't capable of processing sequential data in parallel. Therefore it increases training time since we can't utilize the full power of GPU processing units for parallel matrix multiplication if data is processed sequentially. <br><br>Convolutional neural networks can also process sequential data in parallel. But due to their window size, they can only look back and forward the input sequence, making them unable to identify good long-distance relationships between the entire sequence.&nbsp;\n  </p>\n\n  <p>\n    The attention mechanism can look at the whole input sequence at once and identify the long-distance relationships of every element (i.e. word) in the sequence with every other element of the sequence, which is why transformers, in general, perform better when it comes to sequential data processing.&nbsp;\n  </p>\n\n    <h2 id=\"the-big-picture\">\n      The Big Picture\n    </h2>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/transformers.png\" height=\"713\" width=\"818\" alt=\"Transformer architecture, bird's eye view diagram\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/transformers-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/transformers-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/transformers-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/transformers-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/transformers-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/transformers-2xl.png 1600w\">\n      <figcaption>High level overview of the transformer architecture and it's key components </figcaption>\n    </figure>\n\n  <p>\n    <br>In a nutshell, the transformer uses an encoder-decoder architecture, where N (N=5 in the&nbsp; different encoding layers encode the input sequences, and N different decoder layers decode the information encoded by the encoding layers to predict the next word in sequence, depending on the NLP task at hand.&nbsp;&nbsp;\n  </p>\n\n  <p>\n    Before sending the input sequences into the encoder layers or to the decoder layers of the transformer, word embeddings and positional embeddings preprocess the input sequences so that the transformer can process the data more intuitively and effectively.&nbsp;\n  </p>\n\n  <p>\n    &nbsp;We will discuss the concept of word embedding and positional embedding once we dive more deeply into the inner workings of the transformer. \n  </p>\n\n  <p>\n    These encoder layers are stacked on top of each other. The last encoder layer creates an encoded embedding matrix that contains a representation of the learned sequence.\n  </p>\n\n  <p>\n    Finally, the decoder layer utilizes this matrix for a certain natural language processing task such as text classification, text generation, or machine translation.&nbsp;\n  </p>\n\n  <p>\n    The original paper (Attention is all you need) utilized the transformer to convert English input sentences to German and French, proving that transformers are good at machine translation tasks.&nbsp;\n  </p>\n\n  <p>\n    So let's dive deep into the inner workings of the transformers' encoder and decoder layers, how Attention works, and how supervised learning can train the transformer to optimize to perform better at machine translation and similar text processing tasks.&nbsp;\n  </p>\n\n    <h2 id=\"how-the-transformer-works\">\n      How the Transformer works\n    </h2>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/1-do7YDFF2sads0p9BnjzrWA-2.png\" height=\"671\" width=\"500\" alt=\"Detailed diagram of the transformer\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-2xl.png 1600w\">\n      <figcaption>Detailed diagram of the transformer, from the paper 'Attention is all you need' NIPS 2017</figcaption>\n    </figure>\n\n  <p>\n    The paper that started it all by introducing the transformer at NIPS 2017 was called 'Attention is all you need', where Ashish Vaswani et al. proposed a deep learning model capable of processing sequences utilizing only Attention. Earlier variants of using Attention with sequence data involved connecting an attention layer to a recurrent neural network like a Long Short Term Memory (LSTM) network or a (Gated recurrent unit) GRU network.&nbsp;\n  </p>\n\n  <p>\n    However, transformers proved that relying solely on the attention mechanism yielded better results, especially in the NLP problem area of language translation.&nbsp;\n  </p>\n\n  <p>\n    Let's look at how the transformer processes information in a step by step manner and learns to make better predictions overtime.&nbsp;\n  </p>\n\n    <h3 id=\"step-1-word-and-positional-embeddingnbsp\">\n      Step 1: Word and Positional Embedding&nbsp;\n    </h3>\n\n  <p>\n    The first step involves creating embeddings for the input sequences by word and positional embedding.&nbsp;\n  </p>\n\n  <p>\n    Deep learning models can't process words the way humans do. They deal with vectors and matrices to make predictions. Therefore we must first convert words in the sequence into word vectors, representing a word in the vocabulary.&nbsp;\n  </p>\n\n  <p>\n    Before the input sequences enter the encoders, the inputs are preprocessed and converted to word vectors, representing a word's contextual meaning in numeric form, so transformers can carry out numerical calculations to make predictions.&nbsp;\n  </p>\n\n  <p>\n    When training a transformer, we have a set of vocabulary that we would like the transformer to learn. A vocabulary is a set of distinct words that you want the transformer to learn to do the NLP problem at hand effectively.&nbsp;\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/1-4UHP_q1_FdqMr1T5yvXEKg.jpeg\" height=\"650\" width=\"485\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-xs.jpeg 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-sm.jpeg 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-md.jpeg 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-lg.jpeg 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-xl.jpeg 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-2xl.jpeg 1600w\">\n      <figcaption>A vocabulary list with distinct words with their corresponding indices </figcaption>\n    </figure>\n\n  <p>\n    These word embeddings convert a given word to a word vector, which is a vector with a dimension of 'dᵐᵒᵈᵉˡ', which is a constant, or referred to as a hyperparameter of the network. The original paper had word vectors with the size of 512, which meant that to represent one word, the transformer converted each word into 512 numbers and stored them in vectors.&nbsp;\n  </p>\n\n  <p>\n    Word vectors can also be projected onto a 2-D plane, by shrinking their dimensionality from N dimensions to 2 dimensions. Once projected, the words that have similar context are grouped near each other.\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/3225.1569667846.png\" height=\"922\" width=\"1552\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-2xl.png 1600w\">\n      <figcaption>Word vectors projected onto a 2-D plane, words that share similar context are close by to each other</figcaption>\n    </figure>\n\n  <p>\n    Therefore to process the input sequence, we get the input word tokens from the sequence. We map each word to the location of the word in the vocabulary, assuming that the word is in the vocabulary.&nbsp;\n  </p>\n\n  <p>\n    For example, the word index in the vocabulary could be 21; 21 is the word ID. So for every word in the sequence, we map it to its word ID.&nbsp;\n  </p>\n\n  <p>\n    The below diagram shows multiple sentences with words being represented as a 2 dimensional matrix.&nbsp;\n  </p>\n\n    <figure class=\"post__image post__image--wide\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/Capture.PNG\" height=\"354\" width=\"929\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/Capture-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture-2xl.PNG 1600w\">\n      <figcaption>Think of the sample size (row) as a sentence, and Sequence Length as the number of words in each sentence, a cell is a word</figcaption>\n    </figure>\n\n  <p>\n    So our word tokens in the input sequences to the transformers are converted into word vectors, so the transformer knows the word's context when it processes it.\n  </p>\n\n    <figure class=\"post__image post__image--wide\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/Capture2.PNG\" height=\"423\" width=\"1351\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-2xl.PNG 1600w\">\n      \n    </figure>\n\n  <p>\n    Once the word embeddings are created in this manner, positional encoding is also done to the input word tokens in the sequences before the sequences are sent off to the encoder layers.\n  </p>\n\n  <p>\n    Positional encoding tells the transformer what position the word vectors are in relative to each other in the sequence. Since sequences are processed in parallel, the transformer doesn't know beforehand what order the word vectors come in. In the perspective of a transformer, all words are in a similar position which is untrue. Therefore positional encoding encodes the word's relative position information into the representation before being processed by the encoder and decoder layers.&nbsp;<br>\n  </p>\n\n  <p>\n    Positional encoding is done using a sine and cosine function, and computed independtly of the word embeddings.&nbsp;\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/Capture3.PNG\" height=\"144\" width=\"518\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-2xl.PNG 1600w\">\n      \n    </figure>\n\n  <ul>\n    <li><b style=\"font-style: italic;\">pos&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; : </b>Position of the word in the sequence (or sentence)&nbsp;</li><li><b style=\"font-style: italic;\">d_model : </b>Length of the word vector, also known as the embedding size&nbsp;</li><li><b style=\"font-style: italic;\">i&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; :&nbsp; </b>Index value of the word vector<b style=\"font-style: italic;\">&nbsp;&nbsp;</b></li>\n  </ul>\n\n  <p>\n    The sine function is applied to even indexes of <b style=\"font-style: italic;\">i </b>of the word vector, whereas the cosine function is applied to all odd indexes of the word vector.&nbsp;\n  </p>\n\n  <p>\n    This diagram shows the big picture of word and position embedding done in the transformer<br>\n  </p>\n\n    <figure class=\"post__image post__image--wide\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/capture4.PNG\" height=\"814\" width=\"1082\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/capture4-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture4-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture4-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture4-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture4-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture4-2xl.PNG 1600w\">\n      \n    </figure>\n\n    <h3 id=\"step-2-encoding-sequences-using-attention\">\n      Step 2: Encoding Sequences using Attention\n    </h3>\n\n  <p>\n    The N encoder layers encode the word vector sequences. Depending on the implementation the number of encoder/decoder layers can vary, the proposed transformer architecture in the paper has N = 6 encoder and decoder layers, stacked on top of one another.&nbsp;\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/Group-11-2.png\" height=\"640\" width=\"405\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-2xl.png 1600w\">\n      <figcaption>Components inside an encoder layer of a transformer</figcaption>\n    </figure>\n\n  <ol>\n    <li>The input to the layer gets processed by an attention layer.&nbsp;</li><li>An operation known as the 'Add &amp; Norm' is done afterward, then the output is passed down to a feed-forward network.<br></li><li>Another 'Add &amp; Norm' operation is done to the output and sent onto the next encoder layer.<br></li><li>The next encoder layer carries out the same operations, and the output is passed onto the next encoder on top of that afterward.&nbsp;<br></li><li>The decoder layers utilize the output of the last encoder layer to generate the final output.&nbsp;<br></li>\n  </ol>\n\n    <h4 id=\"what-is-attentionnbsp\">\n      What is attention?&nbsp;\n    </h4>\n\n  <p>\n    Before moving forward, it is essential to understand intuitively what the attention layer does in the encoder layer. <br><br>The attention layer tries to create a matrix that gives a <b>score on how much each word relates to every other word in that sequence. </b><br><br>The following visual depicts what the attention layer tries to do to the input sequences.&nbsp;\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/capture5.PNG\" height=\"537\" width=\"688\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/capture5-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture5-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture5-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture5-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture5-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/capture5-2xl.PNG 1600w\">\n      <figcaption>Softmax scores of an attention weight matrix </figcaption>\n    </figure>\n\n  <p>\n    The following visual depicts what the attention layer tries to do to the input sequences. The attention layer lets the transformer model know where to focus on each word and its relation to other words. These long-range patterns are vital for various natural language processing tasks. Attention allows the transformer to look at the data at once and identify patterns effectively, which is why transformers perform better than RNNs. &nbsp;\n  </p>\n\n    <h4 id=\"what-is-the-add-and-norm-layernbsp\">\n      What is the 'add and norm' layer?&nbsp;\n    </h4>\n\n  <p>\n    The add and norm layer takes the output generated by the attention layer and the input for the attention layer, adds them together, and passes them as input to the  Layer normalization function.&nbsp;\n  </p>\n\n  <p>\n    \"<b>Layer Normalization directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases.</b>\"\n  </p>\n\n  <p>\n    Here's the equation for the layer normalization function, where we first calculate the mean of the vector and using the mean, you calculate \\row, which becomes the new input to the feed-forward layer.&nbsp;\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/Capture6.PNG\" height=\"157\" width=\"252\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-2xl.PNG 1600w\">\n      \n    </figure>\n\n    <h4 id=\"the-feedforward-layer\">\n      The feed-forward layer\n    </h4>\n\n  <p>\n    The feed-forward layer in the encoder layer is a point-wise feed-forward network, a fully connected feed-forward network <b>consisting of two linear transformations W1, B1, and W2, B2 </b>with a <b>ReLU </b>activation function in between.&nbsp;\n  </p>\n\n  <p>\n    The inputs for the feed-forward layer are d_model which is the embedding size, and the inner layer has a dimensionality of d_ff. The dimensions of the d_model and d_ff in the proposed architecture are 512, and 2048 respectively.&nbsp;\n  </p>\n\n    <h3 id=\"step-3-decoding-the-input-sequencesnbsp\">\n      Step 3: Decoding the input sequences&nbsp;\n    </h3>\n\n  <p>\n    The decoder layers are more or less similar to each other, but their behavior slightly changes when we use the model to train and when we use it for inference.\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/Group-2.png\" height=\"680\" width=\"600\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-2xl.png 1600w\">\n      \n    </figure>\n\n  <p>\n    Like in the encoder layers, we embed the text input to the decoder layers, calculate their word vectors, and add positional encoding to it. We first input a 'START' token to the decoder layer to predict the next word in the sequence at the very end of the decoder layer.\n  </p>\n\n  <p>\n    The next word predicted by the decoder layers is ignored during training, only used for the loss calculation and backpropagation of the loss to optimize the weights of the transformer.\n  </p>\n\n  <p>\n    However, during inference, we append the predicted word as the next word token to our decoder layer and append it to the sequence to predict the next word at time step t+1.&nbsp;\n  </p>\n\n  <p>\n    Each decoder layer has two attention layers; one is responsible for finding the connections of the output sequence's words with words that come before it, not after it.&nbsp;\n  </p>\n\n  <p>\n    Then we send the output scores through an 'add &amp; norm' layer, implemented in the same manner as the encoder layers.&nbsp;\n  </p>\n\n  <p>\n    The next attention layer finds how relevant the words are in the output sequence compared with the input sequence from the encoder layer output.\n  </p>\n\n  <p>\n    The information from the second attention layer passes along onto another 'add and norm' layer, afterward a point-wise feed-forward layer, and finally through another 'add and norm' layer, and onto the next decoder layer above it.&nbsp;\n  </p>\n\n    <h3 id=\"step-4-training-and-inferencenbsp\">\n      Step 4: Training and Inference&nbsp;\n    </h3>\n\n  <p>\n    The final decoder layer has a linear layer, a single linear operation applied onto the output of the last decoder layer, and maps the output of the decoder layer into a vector with the dimensions of the vocabulary size. (E.g. 10000)\n  </p>\n\n  <p>\n    <b>A SoftMax function converts the values of that output vector into a probability distribution</b>, and the<b> index with the highest probability is chosen as the network's final output.</b>\n  </p>\n\n  <p>\n    This <b>chosen index position is mapped into the corresponding word by looking up the index from the vocabulary.&nbsp;</b>\n  </p>\n\n  <p>\n    In the training phase of the network, we calculate the next word and calculate the loss of the probability distribution of the SoftMax function with the target distribution. This loss is backpropagated across the network, and the weights of every decoder and encoder block are updated. \n  </p>\n\n  <p>\n    In the <b>training phase, we use a method known as 'teacher forcing' </b>where we manually input and append the correct word to the decoder layer's input sequence at each timestep to predict the next word.\n  </p>\n\n  <p>\n    During <b>inference, the predicted next word is appended to the current input sequence of the decoder, and until a 'END' token is generated, the next word is predicted and appended to the sequence.</b>\n  </p>\n\n    <h2 id=\"understanding-attentionnbsp\">\n      Understanding Attention&nbsp;\n    </h2>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/SCALDE.png\" height=\"535\" width=\"500\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-2xl.png 1600w\">\n      <figcaption>Scaled Dot Product Attention </figcaption>\n    </figure>\n\n  <p>\n    <b>Attention </b>is what makes the transformer understand long-range patterns in the sequence data that it processes and can make predictions considering the context of the input sequence and the relationships between each word vector in each sequence. \n  </p>\n\n  <p>\n    The <b>self-attention mechanism compares every word token in a given sequence with every other word token in the sequence and gauges how much they are important to each other.</b>&nbsp;\n  </p>\n\n  <p>\n    They model the relationship each word token has with each other in the sequence. Whether that relationship is strong or weak, based on these attention scores, the feedforward layers of the encoder/decoder layer can make better predictions by understanding the dependency of each word token with the rest of the word tokens.&nbsp;\n  </p>\n\n  <p>\n    The below matrices shows how the attention layer scores word vector pairs based on how strong the relationships is between the two word vectors in the sequence.&nbsp;\n  </p>\n\n  <p>\n    Note that each word's contextual meaning as well as positional information is taken into account when the attention layer computes the scores.&nbsp;\n  </p>\n\n  <div  class=\"gallery-wrapper\">\n    <div class=\"gallery\" data-columns=\"3\">\n      <figure class=\"gallery__item\">\n      <a href=\"https://thenlpstudent.github.io/media/posts/1/gallery/attention2-2.PNG\" data-size=\"329x339\">\n        <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/gallery/attention2-2-thumbnail.PNG\" height=\"339\" width=\"329\" alt=\"\" >\n      </a>\n      \n    </figure><figure class=\"gallery__item\">\n      <a href=\"https://thenlpstudent.github.io/media/posts/1/gallery/attention1-2.PNG\" data-size=\"346x345\">\n        <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/gallery/attention1-2-thumbnail.PNG\" height=\"345\" width=\"346\" alt=\"\" >\n      </a>\n      \n    </figure>\n    </div>\n  </div>\n\n  <p>\n    The attention model works by comparing every word token with every other word token using the dot-product operation. The more significant the magnitude of the dot-product, the higher likelihood that the pair of word vectors have a strong relationship and vice versa. Thus, the attention mechanism functions as a <b>lookup dictionary.</b>\n  </p>\n\n  <p>\n    A dictionary has <b>queries, keys, and values</b>. <br>We match up the queries with the keys and weigh them. The higher the dot product of the query and key pair, the higher the likelihood of them relating to one another. <br>\n  </p>\n\n  <p>\n    With the multiplication of the query and key pairs together with every Value vector in the Value Matrix, we find which word is most likely has a strong connection with which word in the sequence.&nbsp;\n  </p>\n\n  <p>\n    These query, key, and value are matrices created by multiplying the input sequence by weight matrices<b> Wq, Wk, </b>and <b>Wv</b>. These parameters are fine-tuned when the transformer trains, and over time the attention mechanism can learn the interrelationships between each word token for effective prediction over time.&nbsp;\n  </p>\n\n  <p>\n    Here are the formulas that generate matrices Q for Query, K for Key, and V for Value.&nbsp;<br>\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/CodeCogsEqn1.gif\" height=\"32\" width=\"145\" alt=\"\" >\n      \n    </figure>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/CodeCogsEqn2.gif\" height=\"27\" width=\"153\" alt=\"\" >\n      \n    </figure>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/CodeCogsEqn3.gif\" height=\"27\" width=\"147\" alt=\"\" >\n      \n    </figure>\n\n  <p>\n    The below diagram shows how the matrix multiplication occurs visually&nbsp;\n  </p>\n\n    <figure class=\"post__image post__image--wide\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/test1.PNG\" height=\"290\" width=\"500\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/test1-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/test1-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/test1-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/test1-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/test1-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/test1-2xl.PNG 1600w\">\n      \n    </figure>\n\n  <p>\n    Here are the formulae that calculate the attention score for a given input sequence, where M is the mask matrix, which is explained in the next section.&nbsp;\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/CodeCogsEqn5.gif\" height=\"77\" width=\"381\" alt=\"\" >\n      \n    </figure>\n\n    <h3 id=\"maskingnbsp\">\n      Masking&nbsp;\n    </h3>\n\n  <p>\n    A design feature of attention is masking.&nbsp;<b>When calculating attention, it's important that we only consider the attention scores of valid word vectors.</b>\n  </p>\n\n  <p>\n    Some sentences might not be of the length as the sequence size in the encoder layer, and sentences can have varying lengths. So we add a unique token known as the <b>'PAD' token</b> to make all sentences the same length.&nbsp;\n  </p>\n\n  <p>\n    However, the 'PAD' token is meaningless. Therefore we don't calculate the attention score of that token.&nbsp;\n  </p>\n\n  <p>\n    To cancel out the attention score of 'PAD' tokens, before we calculate the attention score using the SoftMax function, we add a mask matrix to the attention score.\n  </p>\n\n  <p>\n    A mask matrix in the encoder layer might look something like this.&nbsp;&nbsp;\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/mask1.PNG\" height=\"421\" width=\"507\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/mask1-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask1-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask1-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask1-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask1-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask1-2xl.PNG 1600w\">\n      \n    </figure>\n\n  <p>\n    In the decoder layers, we want to force the network to only make predictions by looking at the word before it. Thus, we force the network to guess the next word in the sequence by looking back at the words before it. This type of mask is known as a look-ahead mask.&nbsp;\n  </p>\n\n  <p>\n    Here's what a mask in the decoder layer might look like this.\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/mask2.PNG\" height=\"388\" width=\"447\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/mask2-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask2-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask2-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask2-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask2-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/mask2-2xl.PNG 1600w\">\n      \n    </figure>\n\n  <p>\n    Here's the complete visualization on how attention is calculated\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/softmax-fn.PNG\" height=\"313\" width=\"886\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-2xl.PNG 1600w\">\n      <figcaption>How the attention weight matrix is calculated using the Softmax function with a suitable mask</figcaption>\n    </figure>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/Group-38.png\" height=\"727\" width=\"930\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-2xl.png 1600w\">\n      <figcaption>How the final output of the self attention layer is calculated by using the V (values) matrix</figcaption>\n    </figure>\n\n    <h3 id=\"attention-in-the-decoder-layersnbsp\">\n      Attention in the decoder layers&nbsp;\n    </h3>\n\n  <p>\n    In the decoder layer, we have an attention layer that relies on the encoder's input representation.&nbsp;\n  </p>\n\n  <p>\n    The encoder's input representation takes the Query and Key values, and the value matrix is taken from the decoder's input. Intuitively this means that we find the relationship between the encoder's input word vectors and how it relates to the decoder's word vectors.&nbsp;\n  </p>\n\n  <p>\n    For example, in the context of machine translation, if you have English words as input and translating them into German, the encoder's input representation would be English word vectors, and the decoder's input representation would be German word vectors. We find the relationship between the encoder's input representation and the decoder's input representation, essentially finding out how strong or weak the relationship is between English and German word pairs.&nbsp;\n  </p>\n\n  <p>\n    The below matrix shows how the attention layer gauges and compares every German word in the output sequence with every English word in the input sequence, and models the relationship between both the languages.&nbsp;\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/scaledd-2.PNG\" height=\"402\" width=\"350\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-2xl.PNG 1600w\">\n      \n    </figure>\n\n    <h2 id=\"multihead-attention-cause-ngt1-heads-are-better-than-onenbsp\">\n      Multi-Head Attention: cause n&gt;1 heads are better than one!&nbsp;\n    </h2>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/multi-head-attention_l1A3G7a.png\" height=\"648\" width=\"500\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-xs.png 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-sm.png 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-md.png 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-lg.png 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-xl.png 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-2xl.png 1600w\">\n      <figcaption>Multi-Head Attention mechanism in the transformer model, found in both the encoder & decoder layers</figcaption>\n    </figure>\n\n  <p>\n    <b>Multi-head attention splits the input sequence into multiple parts and applies scaled dot product attention to each part individually</b>.&nbsp;\n  </p>\n\n  <p>\n    This has been shown to improve the quality of attention as more complex relationships between word tokens can be extracted.&nbsp;\n  </p>\n\n  <p>\n    The input sequence is split based on the number of heads, a hyperparameter set before training the transformer.&nbsp;\n  </p>\n\n  <p>\n    For example, if the head size is 8, the input sequence is split into eight equal parts by dividing the embedding size by the number of attention heads. This is known as the <b>query size</b>.\n  </p>\n\n  <p>\n    Query size is equal to the embedding size divided by the number of attention heads.&nbsp;\n  </p>\n\n  <p>\n    A linear layer is used to get the Q, K, and V matrices by multiplying the input matrix by the corresponding weight matrix. In this example, the <b>embedding size is 6</b>, and the <b>number of attention heads used is 2</b>. <b>Therefore the query size is 6/2, which is 3.&nbsp;</b>\n  </p>\n\n  <p>\n    As you can see in the above diagram, there is a red line separating the weights of the two heads, which is known as the logical split. When implementing, each attention head doesn't have its weight matrix, but the weights for every attention head are in one matrix. Thus, it's designed to easily update the weight matrices without taking up much memory, compute power, or time.&nbsp;\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/part1.PNG\" height=\"341\" width=\"605\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/part1-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/part1-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/part1-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/part1-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/part1-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/part1-2xl.PNG 1600w\">\n      <figcaption>Using a logical split to share weights for the attention heads in the same weight matrices </figcaption>\n    </figure>\n\n  <p>\n    Same as before, the K matrix transposes to multiply the Q and K matrices together.&nbsp;\n  </p>\n\n  <p>\n    However, before multiplying both these matrices, we first add an extra dimension known as the head size, which is 2 in this case.&nbsp;\n  </p>\n\n  <p>\n    This turns the 2D Q and K matrices to 3 dimensional. The V matrix is also reshaped to have a head dimension. Therefore all matrices are reshaped to the form Sequence Size x Head Size x Query Size.&nbsp;\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/part2.PNG\" height=\"533\" width=\"812\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/part2-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/part2-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/part2-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/part2-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/part2-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/part2-2xl.PNG 1600w\">\n      <figcaption>Introducing a new dimension known as 'head size' to explicitly separate calculations for different attention heads</figcaption>\n    </figure>\n\n  <p>\n    Afterward, we carry out the matrix multiplication for Q and K, \twhere we multiply the matrices for each head, Q_head1 with K_head1 and Q_head2 with K_head2.&nbsp;\n  </p>\n\n  <p>\n    Afterward, every matrix multiplication of Q and K for each head results in a matrix with the shape sequence size x sequence size.&nbsp;\n  </p>\n\n  <p>\n    Finally, we add masks for each head's Q and K product, and we scale it by the square root of the query size and then apply a SoftMax function over it, creating separate attention weight matrices for each head, as show below.\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/part3.PNG\" height=\"227\" width=\"600\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/part3-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/part3-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/part3-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/part3-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/part3-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/part3-2xl.PNG 1600w\">\n      \n    </figure>\n\n  <p>\n    Finally, we multiply the corresponding V matrix's heads with attention weight heads giving us attention scores for each head, as show below.&nbsp;\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/part4.PNG\" height=\"369\" width=\"600\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/part4-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/part4-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/part4-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/part4-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/part4-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/part4-2xl.PNG 1600w\">\n      \n    </figure>\n\n  <p>\n    We need to reshape it back into its original form and merge all the results from the different heads into one by dropping the head dimension altogether. <br><br>First, we reshape the matrix into the form Sequence Size x Query Size x Head Size by swapping the head size with the query size, which alters the shape of the matrix to the following form. <br><br>We can get rid of the head dimension, which makes the 3-dimensional matrix back into two dimensions.\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/part5.PNG\" height=\"269\" width=\"600\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/part5-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/part5-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/part5-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/part5-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/part5-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/part5-2xl.PNG 1600w\">\n      \n    </figure>\n\n  <p>\n    Afterward, we multiply the resulting merged matrix using a <b>weight matrix W0</b> with a shape of <b>embedding size x embedding size</b>, and we add a<b> bias vector to the resulting matrix.</b>\n  </p>\n\n    <figure class=\"post__image post__image--center\">\n      <img loading=\"lazy\" src=\"https://thenlpstudent.github.io/media/posts/1/part6.PNG\" height=\"206\" width=\"600\" alt=\"\"  sizes=\"(max-width: 48em) 100vw, 768px\" srcset=\"https://thenlpstudent.github.io/media/posts/1/responsive/part6-xs.PNG 300w ,https://thenlpstudent.github.io/media/posts/1/responsive/part6-sm.PNG 480w ,https://thenlpstudent.github.io/media/posts/1/responsive/part6-md.PNG 768w ,https://thenlpstudent.github.io/media/posts/1/responsive/part6-lg.PNG 1024w ,https://thenlpstudent.github.io/media/posts/1/responsive/part6-xl.PNG 1360w ,https://thenlpstudent.github.io/media/posts/1/responsive/part6-2xl.PNG 1600w\">\n      \n    </figure>\n\n  <p>\n    This is how multi-head attention is done to a single sample of our input.<b> Likewise, we carry out the following process simultaneously for all sentence samples</b>, thanks to the fact that <b>matrix multiplication is inherently parallelizable.&nbsp;</b>\n  </p>\n\n    <h2 id=\"referencesnbsp\">\n      References&nbsp;\n    </h2>\n\n  <ul>\n    <li><a href=\"https://arxiv.org/pdf/1706.03762.pdf\">Attention is all you need - Ashish Vaswani et al, NIPS 2017</a><br></li><li><a href=\"https://www.tensorflow.org/text/tutorials/transformer\">Tensor flow model for language understanding&nbsp;</a></li>\n  </ul>\n\n  <p>\n    \n  </p>\n\n  <p>\n    &nbsp;\n  </p>\n\n  <p>\n    &nbsp;\n  </p>",
            "author": {
                "name": "The NLP Student"
            },
            "tags": [
            ],
            "date_published": "2021-08-31T13:40:35+05:30",
            "date_modified": "2021-09-09T17:11:20+05:30"
        }
    ]
}
