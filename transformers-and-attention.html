<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>The Transformer Explained - The NLP Student&#x27;s Blog</title><meta name="description" content="Welcome! In this article, we will be going over what a Transformer is, the intuition and the inner workings behind the attention mechanism it employs to process sequential data, and how the Multi-Head Attention mechanism works as implemented by the paper 'Attention is all you&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://thenlpstudent.github.io/transformers-and-attention.html"><link rel="alternate" type="application/atom+xml" href="https://thenlpstudent.github.io/feed.xml"><link rel="alternate" type="application/json" href="https://thenlpstudent.github.io/feed.json"><meta property="og:title" content="The Transformer Explained"><meta property="og:image" content="https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400.jpg"><meta property="og:image:width" content="314"><meta property="og:image:height" content="314"><meta property="og:site_name" content="The NLP Student's Blog"><meta property="og:description" content="Welcome! In this article, we will be going over what a Transformer is, the intuition and the inner workings behind the attention mechanism it employs to process sequential data, and how the Multi-Head Attention mechanism works as implemented by the paper 'Attention is all you&hellip;"><meta property="og:url" content="https://thenlpstudent.github.io/transformers-and-attention.html"><meta property="og:type" content="article"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@studentnlp"><meta name="twitter:title" content="The Transformer Explained"><meta name="twitter:description" content="Welcome! In this article, we will be going over what a Transformer is, the intuition and the inner workings behind the attention mechanism it employs to process sequential data, and how the Multi-Head Attention mechanism works as implemented by the paper 'Attention is all you&hellip;"><meta name="twitter:image" content="https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400.jpg"><link rel="shortcut icon" href="https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400-2.jpg" type="image/png"><style>:root{--body-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--heading-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--logo-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--menu-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol"}</style><link rel="stylesheet" href="https://thenlpstudent.github.io/assets/css/style.css?v=8ac49514f3b5a54ab40b9772cb61e8d3"><link rel="stylesheet" href="https://thenlpstudent.github.io/assets/css/photoswipe.css?v=7331f6eb15671b2fc8e3ffd46e849f7b"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://thenlpstudent.github.io/transformers-and-attention.html"},"headline":"The Transformer Explained","datePublished":"2021-08-31T13:40","dateModified":"2021-09-09T17:11","image":{"@type":"ImageObject","url":"https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400.jpg","height":314,"width":314},"description":"Welcome! In this article, we will be going over what a Transformer is, the intuition and the inner workings behind the attention mechanism it employs to process sequential data, and how the Multi-Head Attention mechanism works as implemented by the paper 'Attention is all you&hellip;","author":{"@type":"Person","name":"The NLP Student","url":"https://thenlpstudent.github.io/authors/chirath-nissanka/"},"publisher":{"@type":"Organization","name":"The NLP Student","logo":{"@type":"ImageObject","url":"https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400.jpg","height":314,"width":314}}}</script><style>.pswp--svg .pswp__button,
				          .pswp--svg .pswp__button--arrow--left:before,
				          .pswp--svg .pswp__button--arrow--right:before {
					          background-image: url(https://thenlpstudent.github.io/assets/svg/gallery-icons-light.svg);
			              }</style><script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script></head><body><div class="site-container"><header class="top" id="js-header"><a class="logo" href="https://thenlpstudent.github.io/"><img src="https://thenlpstudent.github.io/media/website/jkHsBbG9_400x400.jpg" alt="The NLP Student&#x27;s Blog" width="314" height="314"></a></header><main><article class="post"><div class="hero"><header class="hero__content"><div class="wrapper"><div class="post__meta"><time datetime="2021-08-31T13:40">August 31, 2021</time></div><h1>The Transformer Explained</h1><div class="post__meta post__meta--author"><a href="https://thenlpstudent.github.io/authors/chirath-nissanka/" class="feed__author invert">The NLP Student</a></div></div></header></div><div class="wrapper post__entry"><p>Welcome! In this article, we will be going over what a <b>Transformer </b>is, the intuition and the inner workings behind the <b>attention mechanism</b> it employs to process sequential data, and how the <b>Multi-Head Attention mechanism</b> works as implemented by the paper '<b>Attention is all you need</b>' <b>NeurIPS 2017.&nbsp;</b></p><p></p><div class="post__toc"><h3>Quick Links&nbsp;</h3><ul><li><a href="#the-big-picture">The Big Picture</a></li><li><a href="#how-the-transformer-works">How the Transformer works</a><ul><li><a href="#step-1-word-and-positional-embeddingnbsp">Step 1: Word and Positional Embedding&nbsp;</a></li><li><a href="#step-2-encoding-sequences-using-attention">Step 2: Encoding Sequences using Attention</a><ul><li><a href="#what-is-attentionnbsp">What is attention?&nbsp;</a></li><li><a href="#what-is-the-add-and-norm-layernbsp">What is the 'add and norm' layer?&nbsp;</a></li><li><a href="#the-feedforward-layer">The feed-forward layer</a></li></ul></li><li><a href="#step-3-decoding-the-input-sequencesnbsp">Step 3: Decoding the input sequences&nbsp;</a></li><li><a href="#step-4-training-and-inferencenbsp">Step 4: Training and Inference&nbsp;</a></li></ul></li><li><a href="#understanding-attentionnbsp">Understanding Attention&nbsp;</a><ul><li><a href="#maskingnbsp">Masking&nbsp;</a></li><li><a href="#attention-in-the-decoder-layersnbsp">Attention in the decoder layers&nbsp;</a></li></ul></li><li><a href="#multihead-attention-cause-ngt1-heads-are-better-than-onenbsp">Multi-Head Attention: cause n&gt;1 heads are better than one!&nbsp;</a></li><li><a href="#referencesnbsp">References&nbsp;</a></li></ul></div><p>A <b>transformer </b>is a deep learning architecture that performs well for sequential data-related Machine learning tasks. It is based around an encoder-decoder architecture to handle and process sequential data in parallel. In addition, it uses a mechanism known as Attention to look back and forwards the sequential input and identify long-range patterns and relationships between each component in the sequence.<br><br>The transformer architecture solves the shortcomings of recurrent neural networks and convolutional neural networks when processing sequential data and making good predictions.&nbsp;</p><p>Recurrent neural networks aren't capable of processing sequential data in parallel. Therefore it increases training time since we can't utilize the full power of GPU processing units for parallel matrix multiplication if data is processed sequentially.<br><br>Convolutional neural networks can also process sequential data in parallel. But due to their window size, they can only look back and forward the input sequence, making them unable to identify good long-distance relationships between the entire sequence.&nbsp;</p><p>The attention mechanism can look at the whole input sequence at once and identify the long-distance relationships of every element (i.e. word) in the sequence with every other element of the sequence, which is why transformers, in general, perform better when it comes to sequential data processing.&nbsp;</p><h2 id="the-big-picture">The Big Picture</h2><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/transformers.png" height="713" width="818" alt="Transformer architecture, bird's eye view diagram" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/transformers-xs.png 300w, https://thenlpstudent.github.io/media/posts/1/responsive/transformers-sm.png 480w, https://thenlpstudent.github.io/media/posts/1/responsive/transformers-md.png 768w, https://thenlpstudent.github.io/media/posts/1/responsive/transformers-lg.png 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/transformers-xl.png 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/transformers-2xl.png 1600w"><figcaption>High level overview of the transformer architecture and it's key components</figcaption></figure><p><br>In a nutshell, the transformer uses an encoder-decoder architecture, where N (N=5 in the&nbsp; different encoding layers encode the input sequences, and N different decoder layers decode the information encoded by the encoding layers to predict the next word in sequence, depending on the NLP task at hand.&nbsp;&nbsp;</p><p>Before sending the input sequences into the encoder layers or to the decoder layers of the transformer, word embeddings and positional embeddings preprocess the input sequences so that the transformer can process the data more intuitively and effectively.&nbsp;</p><p>&nbsp;We will discuss the concept of word embedding and positional embedding once we dive more deeply into the inner workings of the transformer.</p><p>These encoder layers are stacked on top of each other. The last encoder layer creates an encoded embedding matrix that contains a representation of the learned sequence.</p><p>Finally, the decoder layer utilizes this matrix for a certain natural language processing task such as text classification, text generation, or machine translation.&nbsp;</p><p>The original paper (Attention is all you need) utilized the transformer to convert English input sentences to German and French, proving that transformers are good at machine translation tasks.&nbsp;</p><p>So let's dive deep into the inner workings of the transformers' encoder and decoder layers, how Attention works, and how supervised learning can train the transformer to optimize to perform better at machine translation and similar text processing tasks.&nbsp;</p><h2 id="how-the-transformer-works">How the Transformer works</h2><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/1-do7YDFF2sads0p9BnjzrWA-2.png" height="671" width="500" alt="Detailed diagram of the transformer" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-xs.png 300w, https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-sm.png 480w, https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-md.png 768w, https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-lg.png 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-xl.png 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/1-do7YDFF2sads0p9BnjzrWA-2-2xl.png 1600w"><figcaption>Detailed diagram of the transformer, from the paper 'Attention is all you need' NIPS 2017</figcaption></figure><p>The paper that started it all by introducing the transformer at NIPS 2017 was called 'Attention is all you need', where Ashish Vaswani et al. proposed a deep learning model capable of processing sequences utilizing only Attention. Earlier variants of using Attention with sequence data involved connecting an attention layer to a recurrent neural network like a Long Short Term Memory (LSTM) network or a (Gated recurrent unit) GRU network.&nbsp;</p><p>However, transformers proved that relying solely on the attention mechanism yielded better results, especially in the NLP problem area of language translation.&nbsp;</p><p>Let's look at how the transformer processes information in a step by step manner and learns to make better predictions overtime.&nbsp;</p><h3 id="step-1-word-and-positional-embeddingnbsp">Step 1: Word and Positional Embedding&nbsp;</h3><p>The first step involves creating embeddings for the input sequences by word and positional embedding.&nbsp;</p><p>Deep learning models can't process words the way humans do. They deal with vectors and matrices to make predictions. Therefore we must first convert words in the sequence into word vectors, representing a word in the vocabulary.&nbsp;</p><p>Before the input sequences enter the encoders, the inputs are preprocessed and converted to word vectors, representing a word's contextual meaning in numeric form, so transformers can carry out numerical calculations to make predictions.&nbsp;</p><p>When training a transformer, we have a set of vocabulary that we would like the transformer to learn. A vocabulary is a set of distinct words that you want the transformer to learn to do the NLP problem at hand effectively.&nbsp;</p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/1-4UHP_q1_FdqMr1T5yvXEKg.jpeg" height="650" width="485" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-xs.jpeg 300w, https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-sm.jpeg 480w, https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-md.jpeg 768w, https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-lg.jpeg 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-xl.jpeg 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/1-4UHP_q1_FdqMr1T5yvXEKg-2xl.jpeg 1600w"><figcaption>A vocabulary list with distinct words with their corresponding indices</figcaption></figure><p>These word embeddings convert a given word to a word vector, which is a vector with a dimension of 'dᵐᵒᵈᵉˡ', which is a constant, or referred to as a hyperparameter of the network. The original paper had word vectors with the size of 512, which meant that to represent one word, the transformer converted each word into 512 numbers and stored them in vectors.&nbsp;</p><p>Word vectors can also be projected onto a 2-D plane, by shrinking their dimensionality from N dimensions to 2 dimensions. Once projected, the words that have similar context are grouped near each other.</p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/3225.1569667846.png" height="922" width="1552" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-xs.png 300w, https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-sm.png 480w, https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-md.png 768w, https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-lg.png 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-xl.png 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/3225.1569667846-2xl.png 1600w"><figcaption>Word vectors projected onto a 2-D plane, words that share similar context are close by to each other</figcaption></figure><p>Therefore to process the input sequence, we get the input word tokens from the sequence. We map each word to the location of the word in the vocabulary, assuming that the word is in the vocabulary.&nbsp;</p><p>For example, the word index in the vocabulary could be 21; 21 is the word ID. So for every word in the sequence, we map it to its word ID.&nbsp;</p><p>The below diagram shows multiple sentences with words being represented as a 2 dimensional matrix.&nbsp;</p><figure class="post__image post__image--wide"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/Capture.PNG" height="354" width="929" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/Capture-xs.PNG 300w, https://thenlpstudent.github.io/media/posts/1/responsive/Capture-sm.PNG 480w, https://thenlpstudent.github.io/media/posts/1/responsive/Capture-md.PNG 768w, https://thenlpstudent.github.io/media/posts/1/responsive/Capture-lg.PNG 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/Capture-xl.PNG 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/Capture-2xl.PNG 1600w"><figcaption>Think of the sample size (row) as a sentence, and Sequence Length as the number of words in each sentence, a cell is a word</figcaption></figure><p>So our word tokens in the input sequences to the transformers are converted into word vectors, so the transformer knows the word's context when it processes it.</p><figure class="post__image post__image--wide"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/Capture2.PNG" height="423" width="1351" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-xs.PNG 300w, https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-sm.PNG 480w, https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-md.PNG 768w, https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-lg.PNG 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-xl.PNG 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/Capture2-2xl.PNG 1600w"></figure><p>Once the word embeddings are created in this manner, positional encoding is also done to the input word tokens in the sequences before the sequences are sent off to the encoder layers.</p><p>Positional encoding tells the transformer what position the word vectors are in relative to each other in the sequence. Since sequences are processed in parallel, the transformer doesn't know beforehand what order the word vectors come in. In the perspective of a transformer, all words are in a similar position which is untrue. Therefore positional encoding encodes the word's relative position information into the representation before being processed by the encoder and decoder layers.&nbsp;<br></p><p>Positional encoding is done using a sine and cosine function, and computed independtly of the word embeddings.&nbsp;</p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/Capture3.PNG" height="144" width="518" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-xs.PNG 300w, https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-sm.PNG 480w, https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-md.PNG 768w, https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-lg.PNG 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-xl.PNG 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/Capture3-2xl.PNG 1600w"></figure><ul><li><b style="font-style: italic;">pos&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; : </b>Position of the word in the sequence (or sentence)&nbsp;</li><li><b style="font-style: italic;">d_model : </b>Length of the word vector, also known as the embedding size&nbsp;</li><li><b style="font-style: italic;">i&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; :&nbsp; </b>Index value of the word vector<b style="font-style: italic;">&nbsp;&nbsp;</b></li></ul><p>The sine function is applied to even indexes of <b style="font-style: italic;">i </b>of the word vector, whereas the cosine function is applied to all odd indexes of the word vector.&nbsp;</p><p>This diagram shows the big picture of word and position embedding done in the transformer<br></p><figure class="post__image post__image--wide"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/capture4.PNG" height="814" width="1082" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/capture4-xs.PNG 300w, https://thenlpstudent.github.io/media/posts/1/responsive/capture4-sm.PNG 480w, https://thenlpstudent.github.io/media/posts/1/responsive/capture4-md.PNG 768w, https://thenlpstudent.github.io/media/posts/1/responsive/capture4-lg.PNG 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/capture4-xl.PNG 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/capture4-2xl.PNG 1600w"></figure><h3 id="step-2-encoding-sequences-using-attention">Step 2: Encoding Sequences using Attention</h3><p>The N encoder layers encode the word vector sequences. Depending on the implementation the number of encoder/decoder layers can vary, the proposed transformer architecture in the paper has N = 6 encoder and decoder layers, stacked on top of one another.&nbsp;</p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/Group-11-2.png" height="640" width="405" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-xs.png 300w, https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-sm.png 480w, https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-md.png 768w, https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-lg.png 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-xl.png 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/Group-11-2-2xl.png 1600w"><figcaption>Components inside an encoder layer of a transformer</figcaption></figure><ol><li>The input to the layer gets processed by an attention layer.&nbsp;</li><li>An operation known as the 'Add &amp; Norm' is done afterward, then the output is passed down to a feed-forward network.<br></li><li>Another 'Add &amp; Norm' operation is done to the output and sent onto the next encoder layer.<br></li><li>The next encoder layer carries out the same operations, and the output is passed onto the next encoder on top of that afterward.&nbsp;<br></li><li>The decoder layers utilize the output of the last encoder layer to generate the final output.&nbsp;<br></li></ol><h4 id="what-is-attentionnbsp">What is attention?&nbsp;</h4><p>Before moving forward, it is essential to understand intuitively what the attention layer does in the encoder layer.<br><br>The attention layer tries to create a matrix that gives a <b>score on how much each word relates to every other word in that sequence.</b><br><br>The following visual depicts what the attention layer tries to do to the input sequences.&nbsp;</p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/capture5.PNG" height="537" width="688" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/capture5-xs.PNG 300w, https://thenlpstudent.github.io/media/posts/1/responsive/capture5-sm.PNG 480w, https://thenlpstudent.github.io/media/posts/1/responsive/capture5-md.PNG 768w, https://thenlpstudent.github.io/media/posts/1/responsive/capture5-lg.PNG 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/capture5-xl.PNG 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/capture5-2xl.PNG 1600w"><figcaption>Softmax scores of an attention weight matrix</figcaption></figure><p>The following visual depicts what the attention layer tries to do to the input sequences. The attention layer lets the transformer model know where to focus on each word and its relation to other words. These long-range patterns are vital for various natural language processing tasks. Attention allows the transformer to look at the data at once and identify patterns effectively, which is why transformers perform better than RNNs. &nbsp;</p><h4 id="what-is-the-add-and-norm-layernbsp">What is the 'add and norm' layer?&nbsp;</h4><p>The add and norm layer takes the output generated by the attention layer and the input for the attention layer, adds them together, and passes them as input to the Layer normalization function.&nbsp;</p><p>"<b>Layer Normalization directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases.</b>"</p><p>Here's the equation for the layer normalization function, where we first calculate the mean of the vector and using the mean, you calculate \row, which becomes the new input to the feed-forward layer.&nbsp;</p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/Capture6.PNG" height="157" width="252" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-xs.PNG 300w, https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-sm.PNG 480w, https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-md.PNG 768w, https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-lg.PNG 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-xl.PNG 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/Capture6-2xl.PNG 1600w"></figure><h4 id="the-feedforward-layer">The feed-forward layer</h4><p>The feed-forward layer in the encoder layer is a point-wise feed-forward network, a fully connected feed-forward network <b>consisting of two linear transformations W1, B1, and W2, B2 </b>with a <b>ReLU </b>activation function in between.&nbsp;</p><p>The inputs for the feed-forward layer are d_model which is the embedding size, and the inner layer has a dimensionality of d_ff. The dimensions of the d_model and d_ff in the proposed architecture are 512, and 2048 respectively.&nbsp;</p><h3 id="step-3-decoding-the-input-sequencesnbsp">Step 3: Decoding the input sequences&nbsp;</h3><p>The decoder layers are more or less similar to each other, but their behavior slightly changes when we use the model to train and when we use it for inference.</p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/Group-2.png" height="680" width="600" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-xs.png 300w, https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-sm.png 480w, https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-md.png 768w, https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-lg.png 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-xl.png 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/Group-2-2xl.png 1600w"></figure><p>Like in the encoder layers, we embed the text input to the decoder layers, calculate their word vectors, and add positional encoding to it. We first input a 'START' token to the decoder layer to predict the next word in the sequence at the very end of the decoder layer.</p><p>The next word predicted by the decoder layers is ignored during training, only used for the loss calculation and backpropagation of the loss to optimize the weights of the transformer.</p><p>However, during inference, we append the predicted word as the next word token to our decoder layer and append it to the sequence to predict the next word at time step t+1.&nbsp;</p><p>Each decoder layer has two attention layers; one is responsible for finding the connections of the output sequence's words with words that come before it, not after it.&nbsp;</p><p>Then we send the output scores through an 'add &amp; norm' layer, implemented in the same manner as the encoder layers.&nbsp;</p><p>The next attention layer finds how relevant the words are in the output sequence compared with the input sequence from the encoder layer output.</p><p>The information from the second attention layer passes along onto another 'add and norm' layer, afterward a point-wise feed-forward layer, and finally through another 'add and norm' layer, and onto the next decoder layer above it.&nbsp;</p><h3 id="step-4-training-and-inferencenbsp">Step 4: Training and Inference&nbsp;</h3><p>The final decoder layer has a linear layer, a single linear operation applied onto the output of the last decoder layer, and maps the output of the decoder layer into a vector with the dimensions of the vocabulary size. (E.g. 10000)</p><p><b>A SoftMax function converts the values of that output vector into a probability distribution</b>, and the<b> index with the highest probability is chosen as the network's final output.</b></p><p>This <b>chosen index position is mapped into the corresponding word by looking up the index from the vocabulary.&nbsp;</b></p><p>In the training phase of the network, we calculate the next word and calculate the loss of the probability distribution of the SoftMax function with the target distribution. This loss is backpropagated across the network, and the weights of every decoder and encoder block are updated.</p><p>In the <b>training phase, we use a method known as 'teacher forcing' </b>where we manually input and append the correct word to the decoder layer's input sequence at each timestep to predict the next word.</p><p>During <b>inference, the predicted next word is appended to the current input sequence of the decoder, and until a 'END' token is generated, the next word is predicted and appended to the sequence.</b></p><h2 id="understanding-attentionnbsp">Understanding Attention&nbsp;</h2><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/SCALDE.png" height="535" width="500" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-xs.png 300w, https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-sm.png 480w, https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-md.png 768w, https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-lg.png 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-xl.png 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/SCALDE-2xl.png 1600w"><figcaption>Scaled Dot Product Attention</figcaption></figure><p><b>Attention </b>is what makes the transformer understand long-range patterns in the sequence data that it processes and can make predictions considering the context of the input sequence and the relationships between each word vector in each sequence.</p><p>The <b>self-attention mechanism compares every word token in a given sequence with every other word token in the sequence and gauges how much they are important to each other.</b>&nbsp;</p><p>They model the relationship each word token has with each other in the sequence. Whether that relationship is strong or weak, based on these attention scores, the feedforward layers of the encoder/decoder layer can make better predictions by understanding the dependency of each word token with the rest of the word tokens.&nbsp;</p><p>The below matrices shows how the attention layer scores word vector pairs based on how strong the relationships is between the two word vectors in the sequence.&nbsp;</p><p>Note that each word's contextual meaning as well as positional information is taken into account when the attention layer computes the scores.&nbsp;</p><div class="gallery-wrapper"><div class="gallery" data-columns="3"><figure class="gallery__item"><a href="https://thenlpstudent.github.io/media/posts/1/gallery/attention2-2.PNG" data-size="329x339"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/gallery/attention2-2-thumbnail.PNG" height="339" width="329" alt=""></a></figure><figure class="gallery__item"><a href="https://thenlpstudent.github.io/media/posts/1/gallery/attention1-2.PNG" data-size="346x345"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/gallery/attention1-2-thumbnail.PNG" height="345" width="346" alt=""></a></figure></div></div><p>The attention model works by comparing every word token with every other word token using the dot-product operation. The more significant the magnitude of the dot-product, the higher likelihood that the pair of word vectors have a strong relationship and vice versa. Thus, the attention mechanism functions as a <b>lookup dictionary.</b></p><p>A dictionary has <b>queries, keys, and values</b>.<br>We match up the queries with the keys and weigh them. The higher the dot product of the query and key pair, the higher the likelihood of them relating to one another.<br></p><p>With the multiplication of the query and key pairs together with every Value vector in the Value Matrix, we find which word is most likely has a strong connection with which word in the sequence.&nbsp;</p><p>These query, key, and value are matrices created by multiplying the input sequence by weight matrices<b> Wq, Wk, </b>and <b>Wv</b>. These parameters are fine-tuned when the transformer trains, and over time the attention mechanism can learn the interrelationships between each word token for effective prediction over time.&nbsp;</p><p>Here are the formulas that generate matrices Q for Query, K for Key, and V for Value.&nbsp;<br></p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/CodeCogsEqn1.gif" height="32" width="145" alt=""></figure><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/CodeCogsEqn2.gif" height="27" width="153" alt=""></figure><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/CodeCogsEqn3.gif" height="27" width="147" alt=""></figure><p>The below diagram shows how the matrix multiplication occurs visually&nbsp;</p><figure class="post__image post__image--wide"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/test1.PNG" height="290" width="500" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/test1-xs.PNG 300w, https://thenlpstudent.github.io/media/posts/1/responsive/test1-sm.PNG 480w, https://thenlpstudent.github.io/media/posts/1/responsive/test1-md.PNG 768w, https://thenlpstudent.github.io/media/posts/1/responsive/test1-lg.PNG 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/test1-xl.PNG 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/test1-2xl.PNG 1600w"></figure><p>Here are the formulae that calculate the attention score for a given input sequence, where M is the mask matrix, which is explained in the next section.&nbsp;</p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/CodeCogsEqn5.gif" height="77" width="381" alt=""></figure><h3 id="maskingnbsp">Masking&nbsp;</h3><p>A design feature of attention is masking.&nbsp;<b>When calculating attention, it's important that we only consider the attention scores of valid word vectors.</b></p><p>Some sentences might not be of the length as the sequence size in the encoder layer, and sentences can have varying lengths. So we add a unique token known as the <b>'PAD' token</b> to make all sentences the same length.&nbsp;</p><p>However, the 'PAD' token is meaningless. Therefore we don't calculate the attention score of that token.&nbsp;</p><p>To cancel out the attention score of 'PAD' tokens, before we calculate the attention score using the SoftMax function, we add a mask matrix to the attention score.</p><p>A mask matrix in the encoder layer might look something like this.&nbsp;&nbsp;</p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/mask1.PNG" height="421" width="507" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/mask1-xs.PNG 300w, https://thenlpstudent.github.io/media/posts/1/responsive/mask1-sm.PNG 480w, https://thenlpstudent.github.io/media/posts/1/responsive/mask1-md.PNG 768w, https://thenlpstudent.github.io/media/posts/1/responsive/mask1-lg.PNG 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/mask1-xl.PNG 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/mask1-2xl.PNG 1600w"></figure><p>In the decoder layers, we want to force the network to only make predictions by looking at the word before it. Thus, we force the network to guess the next word in the sequence by looking back at the words before it. This type of mask is known as a look-ahead mask.&nbsp;</p><p>Here's what a mask in the decoder layer might look like this.</p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/mask2.PNG" height="388" width="447" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/mask2-xs.PNG 300w, https://thenlpstudent.github.io/media/posts/1/responsive/mask2-sm.PNG 480w, https://thenlpstudent.github.io/media/posts/1/responsive/mask2-md.PNG 768w, https://thenlpstudent.github.io/media/posts/1/responsive/mask2-lg.PNG 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/mask2-xl.PNG 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/mask2-2xl.PNG 1600w"></figure><p>Here's the complete visualization on how attention is calculated</p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/softmax-fn.PNG" height="313" width="886" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-xs.PNG 300w, https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-sm.PNG 480w, https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-md.PNG 768w, https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-lg.PNG 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-xl.PNG 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/softmax-fn-2xl.PNG 1600w"><figcaption>How the attention weight matrix is calculated using the Softmax function with a suitable mask</figcaption></figure><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/Group-38.png" height="727" width="930" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-xs.png 300w, https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-sm.png 480w, https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-md.png 768w, https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-lg.png 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-xl.png 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/Group-38-2xl.png 1600w"><figcaption>How the final output of the self attention layer is calculated by using the V (values) matrix</figcaption></figure><h3 id="attention-in-the-decoder-layersnbsp">Attention in the decoder layers&nbsp;</h3><p>In the decoder layer, we have an attention layer that relies on the encoder's input representation.&nbsp;</p><p>The encoder's input representation takes the Query and Key values, and the value matrix is taken from the decoder's input. Intuitively this means that we find the relationship between the encoder's input word vectors and how it relates to the decoder's word vectors.&nbsp;</p><p>For example, in the context of machine translation, if you have English words as input and translating them into German, the encoder's input representation would be English word vectors, and the decoder's input representation would be German word vectors. We find the relationship between the encoder's input representation and the decoder's input representation, essentially finding out how strong or weak the relationship is between English and German word pairs.&nbsp;</p><p>The below matrix shows how the attention layer gauges and compares every German word in the output sequence with every English word in the input sequence, and models the relationship between both the languages.&nbsp;</p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/scaledd-2.PNG" height="402" width="350" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-xs.PNG 300w, https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-sm.PNG 480w, https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-md.PNG 768w, https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-lg.PNG 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-xl.PNG 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/scaledd-2-2xl.PNG 1600w"></figure><h2 id="multihead-attention-cause-ngt1-heads-are-better-than-onenbsp">Multi-Head Attention: cause n&gt;1 heads are better than one!&nbsp;</h2><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/multi-head-attention_l1A3G7a.png" height="648" width="500" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-xs.png 300w, https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-sm.png 480w, https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-md.png 768w, https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-lg.png 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-xl.png 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/multi-head-attention_l1A3G7a-2xl.png 1600w"><figcaption>Multi-Head Attention mechanism in the transformer model, found in both the encoder & decoder layers</figcaption></figure><p><b>Multi-head attention splits the input sequence into multiple parts and applies scaled dot product attention to each part individually</b>.&nbsp;</p><p>This has been shown to improve the quality of attention as more complex relationships between word tokens can be extracted.&nbsp;</p><p>The input sequence is split based on the number of heads, a hyperparameter set before training the transformer.&nbsp;</p><p>For example, if the head size is 8, the input sequence is split into eight equal parts by dividing the embedding size by the number of attention heads. This is known as the <b>query size</b>.</p><p>Query size is equal to the embedding size divided by the number of attention heads.&nbsp;</p><p>A linear layer is used to get the Q, K, and V matrices by multiplying the input matrix by the corresponding weight matrix. In this example, the <b>embedding size is 6</b>, and the <b>number of attention heads used is 2</b>. <b>Therefore the query size is 6/2, which is 3.&nbsp;</b></p><p>As you can see in the above diagram, there is a red line separating the weights of the two heads, which is known as the logical split. When implementing, each attention head doesn't have its weight matrix, but the weights for every attention head are in one matrix. Thus, it's designed to easily update the weight matrices without taking up much memory, compute power, or time.&nbsp;</p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/part1.PNG" height="341" width="605" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/part1-xs.PNG 300w, https://thenlpstudent.github.io/media/posts/1/responsive/part1-sm.PNG 480w, https://thenlpstudent.github.io/media/posts/1/responsive/part1-md.PNG 768w, https://thenlpstudent.github.io/media/posts/1/responsive/part1-lg.PNG 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/part1-xl.PNG 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/part1-2xl.PNG 1600w"><figcaption>Using a logical split to share weights for the attention heads in the same weight matrices</figcaption></figure><p>Same as before, the K matrix transposes to multiply the Q and K matrices together.&nbsp;</p><p>However, before multiplying both these matrices, we first add an extra dimension known as the head size, which is 2 in this case.&nbsp;</p><p>This turns the 2D Q and K matrices to 3 dimensional. The V matrix is also reshaped to have a head dimension. Therefore all matrices are reshaped to the form Sequence Size x Head Size x Query Size.&nbsp;</p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/part2.PNG" height="533" width="812" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/part2-xs.PNG 300w, https://thenlpstudent.github.io/media/posts/1/responsive/part2-sm.PNG 480w, https://thenlpstudent.github.io/media/posts/1/responsive/part2-md.PNG 768w, https://thenlpstudent.github.io/media/posts/1/responsive/part2-lg.PNG 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/part2-xl.PNG 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/part2-2xl.PNG 1600w"><figcaption>Introducing a new dimension known as 'head size' to explicitly separate calculations for different attention heads</figcaption></figure><p>Afterward, we carry out the matrix multiplication for Q and K, where we multiply the matrices for each head, Q_head1 with K_head1 and Q_head2 with K_head2.&nbsp;</p><p>Afterward, every matrix multiplication of Q and K for each head results in a matrix with the shape sequence size x sequence size.&nbsp;</p><p>Finally, we add masks for each head's Q and K product, and we scale it by the square root of the query size and then apply a SoftMax function over it, creating separate attention weight matrices for each head, as show below.</p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/part3.PNG" height="227" width="600" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/part3-xs.PNG 300w, https://thenlpstudent.github.io/media/posts/1/responsive/part3-sm.PNG 480w, https://thenlpstudent.github.io/media/posts/1/responsive/part3-md.PNG 768w, https://thenlpstudent.github.io/media/posts/1/responsive/part3-lg.PNG 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/part3-xl.PNG 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/part3-2xl.PNG 1600w"></figure><p>Finally, we multiply the corresponding V matrix's heads with attention weight heads giving us attention scores for each head, as show below.&nbsp;</p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/part4.PNG" height="369" width="600" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/part4-xs.PNG 300w, https://thenlpstudent.github.io/media/posts/1/responsive/part4-sm.PNG 480w, https://thenlpstudent.github.io/media/posts/1/responsive/part4-md.PNG 768w, https://thenlpstudent.github.io/media/posts/1/responsive/part4-lg.PNG 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/part4-xl.PNG 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/part4-2xl.PNG 1600w"></figure><p>We need to reshape it back into its original form and merge all the results from the different heads into one by dropping the head dimension altogether.<br><br>First, we reshape the matrix into the form Sequence Size x Query Size x Head Size by swapping the head size with the query size, which alters the shape of the matrix to the following form.<br><br>We can get rid of the head dimension, which makes the 3-dimensional matrix back into two dimensions.</p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/part5.PNG" height="269" width="600" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/part5-xs.PNG 300w, https://thenlpstudent.github.io/media/posts/1/responsive/part5-sm.PNG 480w, https://thenlpstudent.github.io/media/posts/1/responsive/part5-md.PNG 768w, https://thenlpstudent.github.io/media/posts/1/responsive/part5-lg.PNG 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/part5-xl.PNG 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/part5-2xl.PNG 1600w"></figure><p>Afterward, we multiply the resulting merged matrix using a <b>weight matrix W0</b> with a shape of <b>embedding size x embedding size</b>, and we add a<b> bias vector to the resulting matrix.</b></p><figure class="post__image post__image--center"><img loading="lazy" src="https://thenlpstudent.github.io/media/posts/1/part6.PNG" height="206" width="600" alt="" sizes="(max-width: 48em) 100vw, 768px" srcset="https://thenlpstudent.github.io/media/posts/1/responsive/part6-xs.PNG 300w, https://thenlpstudent.github.io/media/posts/1/responsive/part6-sm.PNG 480w, https://thenlpstudent.github.io/media/posts/1/responsive/part6-md.PNG 768w, https://thenlpstudent.github.io/media/posts/1/responsive/part6-lg.PNG 1024w, https://thenlpstudent.github.io/media/posts/1/responsive/part6-xl.PNG 1360w, https://thenlpstudent.github.io/media/posts/1/responsive/part6-2xl.PNG 1600w"></figure><p>This is how multi-head attention is done to a single sample of our input.<b> Likewise, we carry out the following process simultaneously for all sentence samples</b>, thanks to the fact that <b>matrix multiplication is inherently parallelizable.&nbsp;</b></p><h2 id="referencesnbsp">References&nbsp;</h2><ul><li><a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is all you need - Ashish Vaswani et al, NIPS 2017</a><br></li><li><a href="https://www.tensorflow.org/text/tutorials/transformer">Tensor flow model for language understanding&nbsp;</a></li></ul><p></p><p>&nbsp;</p><p>&nbsp;</p></div><footer class="wrapper post__footer"><p class="post__last-updated">This article was updated on September 9, 2021</p><div class="post__share"></div><div class="post__bio bio"><div class="bio__info"><h3 class="bio__name"><a href="https://thenlpstudent.github.io/authors/chirath-nissanka/" class="invert" rel="author">The NLP Student</a></h3></div></div></footer></article><nav class="post__nav"><div class="post__nav-inner"><div class="post__nav-next"><a href="https://thenlpstudent.github.io/introduction-to-reinforcement-learning.html" class="invert post__nav-link" rel="next"><span>Next</span> Basics of Reinforcement Learning: Part I </a><svg width="1.041em" height="0.416em" aria-hidden="true"><use xlink:href="https://thenlpstudent.github.io/assets/svg/svg-map.svg#arrow-next"/></svg></div></div></nav></main><footer class="footer"><div class="footer__copyright"><p>Powered by <a href="https://getpublii.com" target="_blank" rel="nofollow noopener">Publii</a></p></div><button class="footer__bttop js-footer__bttop" aria-label="Back to top"><svg><title>Back to top</title><use xlink:href="https://thenlpstudent.github.io/assets/svg/svg-map.svg#toparrow"/></svg></button></footer></div><script>window.publiiThemeMenuConfig = {    
        mobileMenuMode: 'sidebar',
        animationSpeed: 300,
        submenuWidth: 'auto',
        doubleClickTime: 500,
        mobileMenuExpandableSubmenus: true, 
        relatedContainerForOverlayMenuSelector: '.top',
   };</script><script defer="defer" src="https://thenlpstudent.github.io/assets/js/scripts.min.js?v=f4c4d35432d0e17d212f2fae4e0f8247"></script><script>var images = document.querySelectorAll('img[loading]');

        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script><script defer="defer" src="https://thenlpstudent.github.io/assets/js/photoswipe.min.js?v=b586a3bc288c989ecce7b81b48e35855"></script><script defer="defer" src="https://thenlpstudent.github.io/assets/js/photoswipe-ui-default.min.js?v=9952f77ca2b8fbc9449a268008ea8dee"></script><script>var initPhotoSwipeFromDOM=function(gallerySelector){var parseThumbnailElements=function(el){var thumbElements=el.childNodes,numNodes=thumbElements.length,items=[],figureEl,linkEl,size,item;for(var i=0;i<numNodes;i++){figureEl=thumbElements[i];if(figureEl.nodeType!==1){continue;}
          linkEl=figureEl.children[0];size=linkEl.getAttribute('data-size').split('x');item={src:linkEl.getAttribute('href'),w:parseInt(size[0],10),h:parseInt(size[1],10)};if(figureEl.children.length>1){item.title=figureEl.children[1].innerHTML;}
          if(linkEl.children.length>0){item.msrc=linkEl.children[0].getAttribute('src');}
          item.el=figureEl;items.push(item);}
          return items;};var closest=function closest(el,fn){return el&&(fn(el)?el:closest(el.parentNode,fn));};var onThumbnailsClick=function(e){e=e||window.event;e.preventDefault?e.preventDefault():e.returnValue=false;var eTarget=e.target||e.srcElement;var clickedListItem=closest(eTarget,function(el){return(el.tagName&&el.tagName.toUpperCase()==='FIGURE');});if(!clickedListItem){return;}
          var clickedGallery=clickedListItem.parentNode,childNodes=clickedListItem.parentNode.childNodes,numChildNodes=childNodes.length,nodeIndex=0,index;for(var i=0;i<numChildNodes;i++){if(childNodes[i].nodeType!==1){continue;}
          if(childNodes[i]===clickedListItem){index=nodeIndex;break;}
          nodeIndex++;}
          if(index>=0){openPhotoSwipe(index,clickedGallery);}
          return false;};var photoswipeParseHash=function(){var hash=window.location.hash.substring(1),params={};if(hash.length<5){return params;}
          var vars=hash.split('&');for(var i=0;i<vars.length;i++){if(!vars[i]){continue;}
          var pair=vars[i].split('=');if(pair.length<2){continue;}
          params[pair[0]]=pair[1];}
          if(params.gid){params.gid=parseInt(params.gid,10);}
          return params;};var openPhotoSwipe=function(index,galleryElement,disableAnimation,fromURL){var pswpElement=document.querySelectorAll('.pswp')[0],gallery,options,items;items=parseThumbnailElements(galleryElement);options={galleryUID:galleryElement.getAttribute('data-pswp-uid'),getThumbBoundsFn:function(index){var thumbnail=items[index].el.getElementsByTagName('img')[0],pageYScroll=window.pageYOffset||document.documentElement.scrollTop,rect=thumbnail.getBoundingClientRect();return{x:rect.left,y:rect.top+pageYScroll,w:rect.width};},
          mainClass:'pswp--dark',
          preload: [1,2],
          hideAnimationDuration:200,
          showAnimationDuration:0,
          bgOpacity: 0.7,
          showHideOpacity:true,
          closeOnScroll: true,
          arrowKeys: true,
          closeEl: true,
          captionEl: true,
          fullscreenEl: true,
          zoomEl: true,
          shareEl: true,
          counterEl: true,
          arrowEl: true,
          preloaderEl: true
          };if(fromURL){if(options.galleryPIDs){for(var j=0;j<items.length;j++){if(items[j].pid==index){options.index=j;break;}}}else{options.index=parseInt(index,10)-1;}}else{options.index=parseInt(index,10);}
          if(isNaN(options.index)){return;}
          if(disableAnimation){options.showAnimationDuration=0;}
          gallery=new PhotoSwipe(pswpElement,PhotoSwipeUI_Default,items,options);gallery.init();gallery.options.escKey=true;};var galleryElements=document.querySelectorAll(gallerySelector);for(var i=0,l=galleryElements.length;i<l;i++){galleryElements[i].setAttribute('data-pswp-uid',i+1);galleryElements[i].onclick=onThumbnailsClick;}
          var hashData=photoswipeParseHash();if(hashData.pid&&hashData.gid){openPhotoSwipe(hashData.pid,galleryElements[hashData.gid-1],true,true);}};window.addEventListener('load', function () {initPhotoSwipeFromDOM('.gallery');}, false);</script><div class="pswp" tabindex="-1" role="dialog" aria-hidden="true"><div class="pswp__bg"></div><div class="pswp__scroll-wrap"><div class="pswp__container"><div class="pswp__item"></div><div class="pswp__item"></div><div class="pswp__item"></div></div><div class="pswp__ui pswp__ui--hidden"><div class="pswp__top-bar"><div class="pswp__counter"></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button><button class="pswp__button pswp__button--share" title="Share"></button><button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button><button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class="pswp__preloader"><div class="pswp__preloader__icn"><div class="pswp__preloader__cut"><div class="pswp__preloader__donut"></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class="pswp__share-tooltip"></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button><button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class="pswp__caption"><div class="pswp__caption__center"></div></div></div></div></div></body></html>