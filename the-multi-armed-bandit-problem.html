<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>The Multi-Armed Bandit Problem  - The NLP Student</title><meta name="description" content=" In solving the problem of exploration vs. exploitation in reinforcement learning, we use bandit problems to understand and apply algorithms that balance RL agents' exploration and exploitation behaviors. What exactly is the multi-armed bandit problem, and how can you solve this type of problem&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://thenlpstudent.github.io/the-multi-armed-bandit-problem.html"><link rel="alternate" type="application/atom+xml" href="https://thenlpstudent.github.io/feed.xml"><link rel="alternate" type="application/json" href="https://thenlpstudent.github.io/feed.json"><meta property="og:title" content="The Multi-Armed Bandit Problem "><meta property="og:image" content="https://thenlpstudent.github.io/media/website/logo_nlp.jpg"><meta property="og:site_name" content="The NLP Student "><meta property="og:description" content=" In solving the problem of exploration vs. exploitation in reinforcement learning, we use bandit problems to understand and apply algorithms that balance RL agents' exploration and exploitation behaviors. What exactly is the multi-armed bandit problem, and how can you solve this type of problem&hellip;"><meta property="og:url" content="https://thenlpstudent.github.io/the-multi-armed-bandit-problem.html"><meta property="og:type" content="article"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@studentnlp"><meta name="twitter:title" content="The Multi-Armed Bandit Problem "><meta name="twitter:description" content=" In solving the problem of exploration vs. exploitation in reinforcement learning, we use bandit problems to understand and apply algorithms that balance RL agents' exploration and exploitation behaviors. What exactly is the multi-armed bandit problem, and how can you solve this type of problem&hellip;"><meta name="twitter:image" content="https://thenlpstudent.github.io/media/website/logo_nlp.jpg"><style>:root{--body-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--heading-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--logo-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--menu-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol"}</style><link rel="stylesheet" href="https://thenlpstudent.github.io/assets/css/style.css?v=c67e26389eb6b774845b5f0611dabaee"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://thenlpstudent.github.io/the-multi-armed-bandit-problem.html"},"headline":"The Multi-Armed Bandit Problem ","datePublished":"2022-01-26T21:45","dateModified":"2022-01-26T21:45","image":{"@type":"ImageObject","url":"https://thenlpstudent.github.io/media/website/logo_nlp.jpg","height":314,"width":314},"description":" In solving the problem of exploration vs. exploitation in reinforcement learning, we use bandit problems to understand and apply algorithms that balance RL agents' exploration and exploitation behaviors. What exactly is the multi-armed bandit problem, and how can you solve this type of problem&hellip;","author":{"@type":"Person","name":"The NLP Student"},"publisher":{"@type":"Organization","name":"The NLP Student","logo":{"@type":"ImageObject","url":"https://thenlpstudent.github.io/media/website/logo_nlp.jpg","height":314,"width":314}}}</script></head><body><div class="site-container"><header class="top" id="js-header"><a class="logo" href="https://thenlpstudent.github.io/"><img src="https://thenlpstudent.github.io/media/website/logo_nlp.jpg" alt="The NLP Student " width="314" height="314"></a></header><main><article class="post"><div class="hero"><header class="hero__content"><div class="wrapper"><div class="post__meta"><time datetime="2022-01-26T21:45">January 26, 2022</time></div><h1>The Multi-Armed Bandit Problem </h1><div class="post__meta post__meta--author"><a href="https://thenlpstudent.github.io/authors/test/" class="feed__author invert">The NLP Student</a></div></div></header></div><div class="wrapper post__entry"><p>In solving the problem of exploration vs. exploitation in reinforcement learning, we use bandit problems to understand and apply algorithms that balance RL agents' exploration and exploitation behaviors.<br><br>What exactly is the multi-armed bandit problem, and how can you solve this type of problem in RL?<br><br>This blog post focuses on doing just that.</p><h2 id="what-is-the-multi-armed-bandit-problemnbsp">What is the Multi Armed Bandit Problem?&nbsp;</h2><p>Consider the following situation that you find yourself in.<br>You are faced repeatedly with 'k' different actions to take each time.<br>After each action that you take at a given time, you receive a numerical reward.<br><br>This reward is picked out from a stationary probability distribution that depends on your selected action.<br><br>Your goal in this problem is to maximize the total reward you receive over a period of time by selecting the best possible action at each time so that you maximize your reward.<br><br>In this problem, each action has an average reward. This average reward is the value of the action, or how good that action is.<br><br>If you knew the value of each action, then solving this problem would be very simple. Just pick the action with the highest value to get the biggest reward possible at the end.<br><br>However, you don't know the value of each of these actions. Therefore, you would need to explore and come up with estimates on the value of each action by taking random actions in the first few timesteps.<br><br>Once you have good guesses after the first few timesteps, then you are presented with a choice.<br><br>Either you can exploit, which means pick the action with the largest value estimate or continue to explore and re-adjust your current estimates of the values of your actions.&nbsp;</p><h2 id="stationary-vs-non-stationary-bandit-problemnbsp">Stationary vs Non Stationary Bandit Problem&nbsp;</h2><p>The rewards for the actions you or the reinforcement learning (RL) agent takes at the current timestep are sampled from a probability distribution. E.g., Gaussian Distribution.<br><br>In a stationary Bandit problem, the probability distribution doesn't change at each timestep, which means that the true value of each action is constant across all timesteps. However, in a non-stationary bandit problem, the probability distribution changes at every timestep.<br><br>Therefore the true value of each action at each timestep is quite different from other timesteps.<br><br>However, we can solve both variations of such bandit problems by estimating the value of each action, continuing to re-estimate them by trying different actions (exploration), or picking the highest calculated value action you currently have (exploitation)&nbsp;</p><h2 id="solve-by-taking-estimatesnbsp">Solve by taking estimates&nbsp;</h2><p>One way to solve the Multi-Armed Bandit problem is through estimating the true value of each action.<br><br>Solving by estimating the true value of each action can either be applied to stationary and non-stationary bandit problems.<br><br>Firstly we need a few mathematical notations to formalize the bandit problem before finding a way to solve it.</p><ul><li></li></ul><p>&nbsp;</p></div><footer class="wrapper post__footer"><p class="post__last-updated">This article was updated on January 26, 2022</p><div class="post__share"></div><div class="post__bio bio"><div class="bio__info"><h3 class="bio__name"><a href="https://thenlpstudent.github.io/authors/test/" class="invert" rel="author">The NLP Student</a></h3></div></div></footer></article></main><footer class="footer"><div class="footer__copyright"><p>Powered by Publii</p></div><button class="footer__bttop js-footer__bttop" aria-label="Back to top"><svg><title>Back to top</title><use xlink:href="https://thenlpstudent.github.io/assets/svg/svg-map.svg#toparrow"/></svg></button></footer></div><script>window.publiiThemeMenuConfig = {    
        mobileMenuMode: 'sidebar',
        animationSpeed: 300,
        submenuWidth: 'auto',
        doubleClickTime: 500,
        mobileMenuExpandableSubmenus: true, 
        relatedContainerForOverlayMenuSelector: '.top',
   };</script><script defer="defer" src="https://thenlpstudent.github.io/assets/js/scripts.min.js?v=48e9576b9741cf2a93ab25c5689c9f5d"></script><script>var images = document.querySelectorAll('img[loading]');

        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>